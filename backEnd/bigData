----------
大数据数据仓库项目

大数据数据来源
1.业务数据 => mysql
2.用户行为数据 => 通过埋点，记录在日志服务器，文件形式
3.爬虫数据 => 爬取其他网站（违法）

数据仓库
为企业制定决策，提供数据支持

用户行为数据=>通过（flume框架，文件形式存储）=>hive(hdfs)
业务数据（mysql）=>sqoop=>hive(hdfs)

数据仓库分层（数据纬度建模）（atlas看数据分层的流向）
ODS数据原始备份
DWD数据清洗（比如去重、去空、脱敏）
DIM数据维度层，保存维度数据，对业务事实的描述信息，例如何时何人何地
DWS（数据汇总层，按天轻度汇总，比如一个用户一天下单次数）,DWT（数据主题层，累积型，例如一个用户从注册至今一共下了多少次单）数据聚合（合成大表，类似join池）
ADS统计报表层，存在mysql => [报表系统（图表展示），用户画像（给用户打标签），推荐系统]

数仓为什么要分层
1.复杂问题简单化
2.减少重复开发
3.隔离原始数据

用户画像：
统计类标签（hql）
规则类标签（hql）
机器学习（算法）

数据仓库分层由任务调度工具完成（Azkaban,Oozie）

数仓集群监控Zabbix,Prometheus
数仓权限管理Ranger

hdfs存储多目录
1.linux磁盘挂载
2.hdfs-site.xml
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3</value>
</property>

yarn配置
给开发测试环境增加ApplicationMaster资源比例，生产用默认值10%
默认每个资源队列上的ApplicationMaster最多可使用资源为总资源的10%，防止Map/Reduce Task无法执行，但开发环境集群资源少，一个job可能就超10%了，任务可达并行度很低，故可调大ApplicationMaster占比
vim /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml
改为80%
<property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.8</value>

可在http://bigdata101:8088/cluster/scheduler 中的
Configured Max Application Master Limit查看

关闭hive map端的小文件合并
因为hive map端的小文件合并，会把lzo文件的索引文件合并掉，导致lzo文件无法切片
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
----------
hadoop:
1.海量数据的存储，tb,pb,eb
2.海量数据的分析计算

4高，高可靠性，高扩展性，高效性，高容错性

hdfs 
NameNode（nn）元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等
DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。
Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份

yarn
ResourceManager：负责整个集群的资源管理与调度
NodeManager：管理Hadoop集群中单个计算节点

MapReduce
MapReduce 将计算过程分为两个阶段：Map 和 Reduce

Map 阶段并行处理输入数据（拆数据）
Reduce 阶段对 Map 结果进行汇总（合数据）

部署方式：
单机:
无hdfs，yarn，只能测试MapReduce
修改配置文件：
hadoop-env.sh添加上JAVA_HOME

伪分布式:
进程全，都在一台机器上
修改配置文件：
hadoop-env.sh
hdfs-site.xml
core-site.xml
mapred-site.xml	
yarn-site.xml

启动：
sbin/start-all.sh

jobhistory历史服务器默认是不启动的

分布式:
进程全，在多台机器上
修改配置文件：

核心配置文件
core-site.xml

hdfs配置文件
hdfs-site.xml

yarn配置文件，配置日志聚集功能
yarn-site.xml

MapReduce配置文件，配置历史服务器
mapred-site.xml	

workers配置文件

Hadoop三种访问方式：
1.命令行方式
2.java api 访问
3.web 浏览器
----------
atlas概念和架构
搭建atlas（元数据管理：组建大数据数据字典，查看大数据资产目录，数据质量监控，自动生成大数据表和字段的血缘关系图）

hive做元数据仓库
atlas将hive中的元数据做成数据字典，方便查询管理

atlas为组织提供开放式元数据管理和治理功能，构建资产目录，形成数据字典

数据字典：可以查到hive库的释义，表介绍，字段解释说明
表与表间血缘依赖
字段和字段间血缘依赖

atlas架构
核心：【导入/导出模块，分类系统模块，图引擎模块】

1.导入/导出模块:
可导入元数据源：【hive,sqoop,falcon,storm,hbase】

atlas能对于hive中表结构改变，而增量导入，通过kafka消息队列

atlas的 hive hook钩子，能把hive变动的元数据同步到kafka中，做消息缓冲，atlas导入导出会消费kafka	

atlas本身不存储元数据，还要导出数据，用hbase存储（metadata store元数据存储库）

2.分类系统模块：
atlas对元数据分类
库，表，字段，路径

每一张hive表都指向hdfs的一个存储路径

3.图引擎模块：
生成对应图表
表与表间血缘依赖图
字段间血缘依赖图

因为hbase底层kv结构，线性结构不能存储图结构

简单结构：线性结构
复杂：树结构
最复杂：图结构（点，线）

要存入图数据库（janusGraph），经过图数据库转变，再存入hbase里

atlas搭建数据字典，solr全文检索数据库，类似es，帮助搜索元数据

solr给所有元数据做索引（index store索引数据库）

atlas提供的api接口（http/rest协议）

上层对接
ranger tag
admin ui(查询atlas元数据库)
业务分类系统

流程总结：
1.metadata sources:atlas支持一下来源提取：hive,sqoop,falcon,storm,hbase,kafka,spark

2.messaging:与kafka对接的消息队列

3.导入/导出，类型系统分裂，图形引擎

4.hbase元数据库，solr索引数据库

5.http/rest的api

6.admin UI，ranger权限管理模块，业务分类模块

atlas2更新
支持hadoop3，hive3，hbase2，solr7.5,kafka2，janusGraph0.3.1
身份验证可信代理
指标模块收集通知
atlas增量导出元数据模式

hadoop三组件：
hdfs[nameNode,dataNode,secondaryNameNode],
yarn[resourceManager.nodeManager],
historyServer[jobHistoryServer]

安装顺序：
1.搭建前准备
2.jdk
3.hadoop
4.mysql
5.hive
6.zookeeper
7.kafka
8.hbase
9.solr
10.atlas
kafka, hbase, solr把自己的元数据（配置信息）存入zookeeper中
hive会把元数据存入mysql
----------
atlas搭建步骤
1.搭建前准备
2.jdk-8u212-linux-x64.tar.gz
3.hadoop-3.1.3.tar.gz集群

4.mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar
按照顺序安装
sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm 
如果linux最小化安装，额外装 sudo yum install -y libaio
sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm 

5.apache-hive-3.1.2-bin.tar.gz
6.apache-zookeeper-3.5.7-bin.tar.gz集群
7.kafka_2.11-2.4.1.tgz集群
8.hbase-2.0.5-bin.tar.gz集群
9.solr-7.7.3.tgz集群
10.apache-atlas-2.1.0-sources.tar.gz

1.搭建前准备
准备bigData100模板虚拟机（centos7，CentOS-7.5-x86-1804）
内存/处理器/硬盘：4 8 60

添加用户，修改用户密码
useradd shonqian
passwd shonqian

配置shonqian用户root权限
vim /etc/sudoers
大约91行，%wheel 这行下面加上
shonqian ALL=(ALL) NOPASSWD:ALL

安装必要环境
vim /etc/yum.repos.d/epel.repo
baseurl解开注释，metalink加上注释即可
yum install -y epel-release
yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git

关闭centos防火墙
systemctl stop firewalld
systemctl disable firewalld.service

在/opt目录下创建文件夹，module，software
software放安装包
module是安装目录

修改 module、software 文件夹的所有者和所属组均为 shonqian 用户（-R对子文件夹也起作用）
sudo chown -R shonqian:shonqian /opt/module
sudo chown -R shonqian:shonqian /opt/software

查看 module、software 文件夹的所有者和所属组，是否已为shonqian
cd /opt/
ll

卸载虚拟机自带的 JDK
查看是否有：
rpm -qa | grep -i java
卸载：
rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps

reboot看是否成功

配置/etc/hosts 文件，建立ip地址和主机名映射
vim /etc/hosts
后面加上
192.168.137.100 bigData100
192.168.137.101 bigData101
192.168.137.102 bigData102
192.168.137.103 bigData103
192.168.137.104 bigData104
192.168.137.105 bigData105
192.168.137.106 bigData106
192.168.137.107 bigData107
192.168.137.108 bigData108

把bigData100模板虚拟机克隆三台虚拟机
内存/处理器/硬盘
8 8 60
4 8 60
4 8 60

固定ip
cd /etc/sysconfig/network-scripts/
vim ifcfg-eno16777736

BOOTPROTO="static"
IPADDR=192.168.137.102
GATEWAY=192.168.137.2

改主机名
vim /etc/hostname
bigData102

reboot

其它两台参照配置

ping www.baidu.com 看能否连接外网

联不通可以右上角网络设置，设置下dns（223.5.5.5/223.6.6.6阿里的dns），再把飞行模式和有线连接开关下

xshell连接

配置windows hosts文件
C:\Windows\System32\drivers\etc\hosts

192.168.137.100 bigData100
192.168.137.101 bigData101
192.168.137.102 bigData102
192.168.137.103 bigData103
192.168.137.104 bigData104
192.168.137.105 bigData105
192.168.137.106 bigData106
192.168.137.107 bigData107
192.168.137.108 bigData108
192.168.137.109 bigData109

在102机器上把所有软件安装包通过xftp传到/opt/software中

配置ssh免密登录

ssh localhost

找到.ssh文件夹
find / -name .ssh -type d

cd /home/root/.ssh/

生成公钥
ssh-keygen -t rsa

将公钥复制到远程机器的authorized_keys文件中，也能有到远程机器的home, ~./ssh , 和 ~/.ssh/authorized_keys的权利
ssh-copy-id bigData102
ssh-copy-id bigData103
ssh-copy-id bigData104
ssh-copy-id 192.168.30.117(其它机器ip))
ssh-copy-id localhost(自身)
返回普通用户
exit

2.jdk8安装
在102上安装java1.8
在/opt/software中
tar -zxvf jdk-xxxx.tar.gz -C /opt/module/
cd /opt/module
mv jdk1.8.0_212/ jdk1.8

cd /etc/profile.d/

sudo vim my_env.sh
添加
# JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8
export PATH=$PATH:$JAVA_HOME/bin

source /etc/profile

集群分发脚本xsync

echo $PATH 查到现有的path路径，在path路径下添加脚本，可到处执行
cd /usr/local/bin

vim xsync
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
	echo Not Enough Arguement!
	exit;
fi
#2. 遍历集群所有机器
for host in bigData102 bigData103 bigData104
do
	echo ==================== $host ====================
	#3. 遍历所有目录，挨个发送
	for file in $@
	do
		#4. 判断文件是否存在
		if [ -e $file ]
		then
			#5. 获取父目录
			pdir=$(cd -P $(dirname $file); pwd)
			#6. 获取当前文件的名称
			fname=$(basename $file)
			ssh $host "mkdir -p $pdir"
			rsync -av $pdir/$fname $host:$pdir
		else
			echo $file does not exists!
			fi
	done
done

给脚本添加执行权限
chmod +x xsync

cd ..
xsync bin	

cd /opt/module
xsync java8
//普通用户没有权限发送etc目录。而sudo xsync也不行，因为xsync是配在普通用户的PATH里，root用户的家目录里没有，所以要将xsync向root的PATH发一份
sudo cp /usr/local/bin/xsync  /usr/bin/
sudo xsync /etc/profile.d/my_env.sh

xshell-工具-发送键输入到所有会话
source /etc/profile

3.安装hadoop3.1.3集群部署
在102机器
cd /opt/software/2_hadoop
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/

给hadoop配置环境变量
sudo vim /etc/profile.d/my_env.sh

#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

source下
hadoop version

配置hadoop完全分布式集群
cd /opt/module/hadoop-3.1.3/etc/hadoop
核心配置文件
core-site.xml
vim core-site.xml
<configuration>
	<!-- 指定 NameNode 的地址 -->
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://bigData102:8020</value>
	</property>
	<!-- 指定 hadoop 数据的存储目录 -->
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/module/hadoop-3.1.3/data</value>
	</property>
	<!-- 配置 HDFS 网页登录使用的静态用户为 shonqian -->
	<property>
		<name>hadoop.http.staticuser.user</name>
		<value>shonqian</value>
	</property>
	<!-- 配置 该shonqian（superUser）允许通过代理访问的主机节点 -->
	<property>
		<name>hadoop.proxyuser.shonqian.hosts</name>
		<value>*</value>
	</property>
	<!-- 配置 该shonqian（superUser）允许通过代理用户所属组 -->
	<property>
		<name>hadoop.proxyuser.shonqian.groups</name>
		<value>*</value>
	</property>
	<!-- 配置 该shonqian（superUser）允许通过代理的用户 -->
	<property>
		<name>hadoop.proxyuser.shonqian.groups</name>
		<value>*</value>
	</property>
</configuration>

hdfs配置文件
vim hdfs-site.xml
<configuration>
	<!-- namenode web 端访问地址-->
	<property>
		<name>dfs.namenode.http-address</name>
		<value>bigData102:9870</value>
	</property>
	<!-- 2nn(secondaryNameNode) web 端访问地址-->
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>bigData104:9868</value>
	</property>
</configuration>

yarn配置文件
vim yarn-site.xml
<configuration>
    <!-- 指定 MR 走 shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定 ResourceManager 的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigData103</value>
    </property>
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name> <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
	<!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
	<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
	<!-- yarn容器允许管理的物理内存大小 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>	
	<!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
	<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>

MapReduce配置文件
vim mapred-site.xml	
<configuration>
    <!-- 指定 MapReduce 程序运行在 Yarn 上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

workers配置文件
vim workers
删去localhost
bigData102
bigData103
bigData104
表示集群有3台机器

配置历史服务器
vim mapred-site.xml
<!-- 历史服务器端地址 -->
<property>
	<name>mapreduce.jobhistory.address</name>
	<value>bigData102:10020</value>
</property>
<!-- 历史服务器web端地址 -->
<property>
	<name>mapreduce.jobhistory.webapp.address</name>
	<value>bigData102:19888</value>
</property>

配置日志聚集功能
vim yarn-site.xml
<!-- 开启日志聚集功能 -->
<property>
	<name>yarn.log-aggregation-enabled</name>
	<value>true</value>
</property>
<!-- 设置日志聚集服务器地址 -->
<property>
	<name>yarn.log.server.url</name>
	<value>http://bigData102:19888/jobhistory/logs</value>
</property>
<!-- 设置日志保留时间为7天 -->
<property>
	<name>yarn.log-aggregation.retain-seconds</name>
	<value>604800</value>
</property>

hadoop文件分发
cd /opt/module
xsync hadoop-3.1.3/
sudo xsync /etc/profile.d/my_env.sh

工具-发送键输入到所有会话
source /etc/profile
hadoop version 检测下

cd hadoop-3.1.3/

第一次启动前，102机器格式化namenode
hdfs namenode -format

启动hadoop
群起
102上
#sbin/start-dfs.sh
103
#sbin/start-yarn.sh
102开启历史服务器
#mapred --daemon start historyserver

群起脚本
myhadoop.sh

cd /usr/local/bin

vim myhadoop.sh

#!/bin/bash
if [ $# -lt 1 ]
then
  echo "No Args Input..."
  exit ;
fi
case $1 in
"start")
  echo " =================== 启动 hadoop 集群 ==================="
  echo " --------------- 启动 hdfs ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
  echo " --------------- 启动 yarn ---------------"
  ssh bigData103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
  echo " --------------- 启动 historyserver ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
  echo " =================== 关闭 hadoop 集群 ==================="
  echo " --------------- 关闭 historyserver ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
  echo " --------------- 关闭 yarn ---------------"
  ssh bigData103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
  echo " --------------- 关闭 hdfs ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
  echo "Input Args Error..."
;;
esac

chmod +x myhadoop.sh

102上
myhadoop.sh start 开启hadoop集群
myhadoop.sh stop 关闭hadoop集群

jpsall查看3台机器java进程
cd /usr/local/bin

vim jpsall

#!/bin/bash
for host in bigData102 bigData103 bigData104
do
	echo =============== $host ===============
	ssh $host jps
done

chmod +x jpsall

如果未格式化启动了hadoop，去102，103，104删除data和logs目录

===============
正常启动jpsall显示：
bigData102
NodeManager
JobHistoryServer
DataNode
NameNode

bigData103
ResourceManager
DataNode
NodeManager

bigData104
SecondaryNameNode
NodeManager
DataNode

一个NameNode，三个DateNode
一个ResourceManager，三个NodeManager
一个Secondary	NameNode，一个JobHistoryServer

102，103，104 => 4，3，3架构（hadoop集群）
===============

浏览器访问hdfs
http://bigData102:9870/dfshealth.html#tab-overview

创建目录，上传文件
http://bigData102:9870/explorer.html#/
能上传就说明hdfs没问题

yarn
http://bigData103:8088/cluster

历史服务器
http://bigData102:19888/jobhistory

4.安装mysql
因为hive元数据是存入mysql中

102上装mysql
检查是否安装过mysql
rpm -qa | grep mariadb
如有需卸载
sudo rpm -e --nodeps mariadb-libs

cd /opt/software/3_mysql
里面有mysql安装包、连接驱动

tar -xvf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar

按照顺序安装
sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm 
如果linux最小化安装，额外装 sudo yum install -y libaio
sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm 

检查mysql指向目录，是否干净
cd /var/lib/mysql
sudo ls
如果有东西，sudo rm -rf ./*

初始化mysql
sudo mysqld --initialize --user=mysql
查看临时生成的root用户的密码
sudo cat /var/log/mysqld.log

启动mysql服务
sudo systemctl start mysqld
查看状态
sudo systemctl status mysqld

mysql -uroot -p
输入初始密码

重置密码
set password = password("root");

修改mysql库下user表中的root用户允许任意ip连接，'%'表示允许所有机器访问
update mysql.user set host = '%' where user = 'root';
刷新mysql表权限
flush privileges;
quit;

mysql -uroot -proot

5.hive3.1.2安装部署
102机器
cd /opt/software/4_hive
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/
cd /opt/module/
mv apache-hive-3.1.2-bin/ hive

添加hive环境变量
sudo vim /etc/profile.d/my_env.sh
#HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
source /etc/profile
hive --version

hive元数据配置到mysql

给hive拷贝mysql连接驱动
cp /opt/software/3_mysql/mysql-connector-java-5.1.37.jar /opt/module/hive/lib/

配置hive配置文件
cd /opt/module/hive/conf
vim hive-site.xml

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- jdbc连接的url -->
  <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://bigData102:3306/metastore?useSSL=false</value>
  </property>
<!-- jdbc连接的Driver -->
  <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
  </property>
<!-- jdbc连接的username -->
  <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
  </property>
<!-- jdbc连接的password -->
  <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>root</value>
  </property>
<!-- Hive默认在HDFS的工作目录 -->
  <property>
      <name>hive.metastore.warehoues.dir</name>
      <value>/user/hive/warehouse</value>
  </property>
<!-- 指定hiveserver2连接的端口号 -->
  <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
  </property>
<!-- 指定hiveserver2连接的host -->
  <property>
    <name>hive.server2.thrift.bind.host</name>
    <value>bigData102</value>
  </property>

<!-- 指定存储元数据要连接的地址 -->
<!-- <property>
  <name>hive.metastore.uris</name>
  <value>thrift://bigData102:9083</value>
</property>-->

<!-- 元数据存储授权 -->
 <property>
   <name>hive.metastore.event.db.notification.api.auth</name>
   <value>false</value>
 </property> 
<!-- Hive元数据存储版本的验证 -->
 <property>
   <name>hive.metastore.schema.verification</name>
   <value>false</value>
 </property>
<!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 -->
 <property>
   <name>hive.server2.active.passive.ha.enable</name>
   <value>true</value>
 </property>
</configuration>

hive调优：
mv hive-env.sh.template hive-env.sh
vim hive-env.sh
放开hadoop堆内存大小注释：
export HADOOP_HEAPSIZE=1024
默认256MB，运行大型sql会报错

mv hive-log4j2.properties.template hive-log4j2.properties
vim hive-log4j2.properties

修改hive运行日志目录
property.hive.log.dir = /opt/module/hive/logs
默认在/tmp/user/hive/log

初始化hive元数据库
mysql -uroot -proot
新建hive元数据库
create database metastore;
quit;
初始化hive元数据库，创建对应表（存储元数据所需要的表）
schematool -initSchema -dbType mysql -verbose

启动hive客户端
前提：hadoop是正常运行的

进到hive客户端
hive
show databases; (一个default库)

show tables;(没有表)

在hive创建用户表
create TABLE test_user (
	`id` STRING COMMENT '编号',
	`name` STRING COMMENT '姓名',
	`province_id` STRING COMMENT '省份ID',
	`province_name` STRING COMMENT '省份名称'
) COMMENT '用户表'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

既create TABLE test_user (`id` STRING COMMENT '编号',`name` STRING COMMENT '姓名',`province_id` STRING COMMENT '省份ID',`province_name` STRING COMMENT '省份名称') COMMENT '用户表' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

向用户表装载数据
insert into table test_user values('1','zhangsan','001','北京');
此时需要跑mapReduce，mapReduce跑在yarn上
可打开http://bigData103:8088/cluster，看任务是否进入yarn集群
state为FINISHED，表示插入成功
select * from test_user;
元数据存在mysql
数据存在hdfs上，http://bigData102:9870/explorer.html#/user/hive/warehouse/test_user

设置元数据支持中文显示
desc test_user;
中文显示??，atlas也会乱码
用dbe打开metastore-COLUMNS_V2表
打开metastore属性，字符集默认是latin1
只能修改对应字段的字符集，把需要存中文的列改为utf8

=====二次安装直接在这开始======
在metastore库下运行sql
# 修改字段注释字符集
alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
# 修改表注释字符集
alter table TABLE_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;
# 修改分区参数，支持分区建用中文表示
alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;
alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(20000) character set utf8;
#修改索引名注释，支持中文表示
alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
#修改视图，支持视图中文
ALTER TABLE TBLS modify COLUMN VIEW_EXPANDED_TEXT mediumtext CHARACTER SET utf8;
ALTER TABLE TBLS modify COLUMN VIEW_ORIGINAL_TEXT mediumtext CHARACTER SET utf8;

修改hive-site.xml中Hive读取元数据的编码
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://bigData102:3306/metastore?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
</property>

cd /opt/module/hive/conf
vim hive-site.xml
第一条添加&amp;useUnicode=true&amp;characterEncoding=UTF-8
修改后支持存读中文

hive
desc test_user;(还是乱码，因为是以latin1存入，已损坏)

create TABLE test_user2 (`id` STRING COMMENT '编号',`name` STRING COMMENT '姓名',`province_id` STRING COMMENT '省份ID',`province_name` STRING COMMENT '省份名称') COMMENT '用户表' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

desc test_user2;

6.安装zookeeper3.5.7
zookeeper集群
分布式安装部署
cd /opt/software/5_zookeeper
tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
cd /opt/module/
mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
cd zookeeper-3.5.7
mkdir zkData
cd zkData
vim myid(服务器编号)
2
cd /opt/module/zookeeper-3.5.7/conf
mv zoo_sample.cfg zoo.cfg 
vim zoo.cfg 
dataDir=/opt/module/zookeeper-3.5.7/zkData
#######cluster集群模式#########
server.2=bigData102:2888:3888
server.3=bigData103:2888:3888
server.4=bigData104:2888:3888

2888 是主从服务交换信息端口号
3888 如果leader挂了，选举leader的端口号

cd /opt/module/
xsync zookeeper-3.5.7/

去103，104机器，改myid
vim /opt/module/zookeeper-3.5.7/zkData/myid
分别
3
4

启动zookeeper
cd /opt/module/zookeeper-3.5.7/
#bin/zkServer.sh start

群起脚本：
在102机器
cd /usr/local/bin
sudo vim zk.sh
#!/bin/bash
if [ $# -lt 1 ]
then
  echo "No Args Input..."
  exit ;
fi

case $1 in
"start")
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- zookeeper $i 启动 ------------
                ssh $i "source /etc/profile && /opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
        done
;;
"stop")
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- zookeeper $i 停止 ------------    
                ssh $i "source /etc/profile && /opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
        done
;;
"status")
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- zookeeper $i 状态 ------------    
                ssh $i "source /etc/profile && /opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
        done
;;
*)
	echo "Input Args Error..."
;;
esac

sudo chmod +x zk.sh

jpaall
是否有3个 QuorumPeerMain

zk.sh status 一主两从

7.安装kafka2.4.1（单机安装方式同）
集群规划
hadoop 3台
zk 3台
kafka 3台

在102
cd /opt/software/6_kafka
tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/

cd /opt/module/
mv kafka_2.11-2.4.1/ kafka
cd kafka
mkdir logs

修改配置文件
cd config/
vim server.properties
不同的broker.id，0，1，2
broker.id=0
下行加上
#删除topic功能
delete.topic.enable=true

#修改kafka运行日志存放的路径
#kafka日志存放的路径（kafka存的topic的消息，既topic数据。错误信息存logs目录）
log.dirs=/opt/module/kafka/data

# 配置连接zookeeper集群地址（将kakfa的元数据放在zookeeper根节点下面的kafka目录下）
zookeeper.connect=bigData102:2181,bigData103:2181,bigData104:2181/kafka

配置环境变量
sudo vim /etc/profile.d/my_env.sh 

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin

分发
xsync /opt/module/kafka/
sudo xsync /etc/profile.d/my_env.sh

103机器
vim /opt/module/kafka/config/server.properties
broker.id=1
104机器
vim /opt/module/kafka/config/server.properties
broker.id=2

工具-发送所有会话
source /etc/profile

启动kafka
# bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties

群起脚本
cd /usr/local/bin/
sudo cp zk.sh kf.sh
sudo vim kf.sh

#!/bin/bash

case $1 in
"start"){
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- kafka $i 启动 ------------
                ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
        done
};;
"stop"){
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- kafka $i 停止 ------------    
                ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
        done
};;
esac

sudo chmod +x kf.sh

kf.sh start

kafka起来后，去zookeeper客户端看下
cd /opt/module/zookeeper-3.5.7/
bin/zkCli.sh
ls /  可以看见有kafka节点
ls /kafka  查看kafka元数据

kafka 命令行操作
cd /opt/module/zookeeper-3.5.7
查看kafka下topic
bin/kafka-topics.sh --zookeeper bigData102:2181/kafka --list
创建topic，3个副本，1个分区，名字叫first的topic。--topic 名字，--replication-factor 副本数，--partitions 分区数
kafka-topics.sh --zookeeper bigData102:2181/kafka --create --replication-factor 3 --partitions 1 --topic first

向topic发送消息，kafka-console-producer.sh kafka生产者脚本
kafka-console-producer.sh --broker-list bigData102:9092 --topic first
1
2
3
4
5

103机器上，消费者脚本，--from-beginning从头开始消费
cd /opt/module/kafka/
bin/kafka-console-consumer.sh --bootstrap-server bigData102:9092 --from-beginning --topic first

查看topic详情
kafka-topics.sh --zookeeper bigData102:2181/kafka --describe --topic first

修改分区数
kafka-topics.sh --zookeeper bigData102:2181/kafka --alter --topic first --partitions 6

8.安装Hbase2.0.5
atlas会把元数据存入hbase
前提：
1.zookeeper正常启动，因为hbase和kafka一样，要把元数据存入zookeeper中
2.hadoop正常启动，hbase本身是hdfs的高级应用

cd /opt/software/7_hbase
tar -zxvf hbase-2.0.5-bin.tar.gz -C /opt/module/
cd /opt/module/
mv hbase-2.0.5/ hbase

配置环境变量
sudo vim /etc/profile.d/my_env.sh

#HBASE_HOME
export HBASE_HOME=/opt/module/hbase
export PATH=$PATH:$HBASE_HOME/bin

修改配置文件
cd /opt/module/hbase/conf
vim hbase-env.sh
将 内部hbase管理zk 关闭，用的是外部的zk
export HBASE_MANAGES_ZK=false


vim hbase-site.xml

<configuration>
	<!--hbase在hdfs上的根路径-->
	<property>
		<name>hbase.rootdir</name>
		<value>hdfs://bigData102:8020/HBase</value>
	</property>
	<!--是否部署分布式hbase，如果为false，会去启动内置zk，如用外置zk，得设为true-->
	<property>
		<name>hbase.cluster.distributed</name>
		<value>true</value>
	</property>
	<!--指定外部zk集群，单机版hbase只需把zk集群改为一个就行-->
	<property>
		<name>hbase.zookeeper.quorum</name>
		<value>bigData102,bigData103,bigData104</value>
	</property>
</configuration>

hbase有两个角色组成：hmaster,hregionservers

vim regionservers
删去localhost
加上
bigData102
bigData103
bigData104

分发
cd /opt/module
xsync hbase/
sudo xsync /etc/profile.d/my_env.sh 
工具-发送键输入到所有会话
source /etc/profile

启动hbase
只启动HMaster：
#bin/hbase-daemon.sh start master

起HMaster和NodeManager：
bin/start-hbase.sh

关闭hbase
#bin/stop-hbase.sh

jpsall能看到三台分别
HMaster NodeManager / NodeManager / NodeManager

hBase集群正常启动
http://bigData102:16010/master-status

单台hbase启动
/opt/module/hbase/bin/start-hbase.sh

9.安装solr7.7.3（单机安装方式同）
atlas的索引数据库是solr

为solr创建系统用户solr
分别102，103，104上
sudo useradd solr

#--stdin读取echo输出，为solr用户的密码
分别102，103，104上
echo solr | sudo passwd --stdin solr

102上
cd /opt/software/8_solr
tar -zxvf solr-7.7.3.tgz -C /opt/module/
cd /opt/module
mv solr-7.7.3/ solr

把solr的所属主和所属组改为solr用户
sudo chown -R solr:solr /opt/module/solr/
ll

修改solr配置文件
cd /opt/module/solr/bin
//shonqian用户没有权限
sudo vim solr.in.sh
//solr也要将元数据存入zk，配置zk地址
ZK_HOST="bigData102:2181,bigData103:2181,bigData104:2181"
sudo xsync /opt/module/solr/

启动zk集群
zk.sh start
3台机器用solr用户启动solr
sudo -i -u solr /opt/module/solr/bin/solr start
//出现Happy searching!，表示启动成功

jps查看，会多一个jar的进程
===============
//如想按照推荐值修改
//修改打开文件数限制
vim /etc/security/limits.conf
增加
soft nofile 65000
hard nofile 65000
//修改进程数数限制
vim /etc/security/limits.d/20-nproc.conf
增加
soft nproc 65000
重启solr服务生效
===============

访问solr web页面
http://bigData102:8983/solr/#/
ui界面出现cloud菜单栏，说明solr集群部署成功

关闭所有solr
sudo -i -u solr /opt/module/solr/bin/solr stop -all


10.安装atlas2.1.0
10.1 atlas安装：
cd /opt/software/9_atlas
tar -zxvf apache-atlas-2.1.0-server.tar.gz -C /opt/module/
atlas的其它hook安装包是对接其它的钩子程序

cd /opt/module/
mv apache-atlas-2.1.0/ atlas
文件构成：
bin脚本
conf配置文件
models模块
server服务器
tools工具

atlas配置其它服务的集成
atlas集成Hbase
atlas要把元数据存入hbase

vim /opt/module/atlas/conf/atlas-application.properties
# 要存入hbase，配的是zk地址，因为hbase会把元数据放入zk中
atlas.graph.storage.hostname=bigData102:2181,bigData103:2181,bigData104:2181

vim /opt/module/atlas/conf/atlas-env.sh
# 让atlas能找到hbase的conf目录
最后行加上
export HBASE_CONF_DIR=/opt/module/hbase/conf

atlas集成solr
vim /opt/module/atlas/conf/atlas-application.properties
# 因为solr的元数据也放在的zk中，能找到zk就能找到solr
atlas.graph.index.search.solr.zookeeper-url=bigData102:2181,bigData103:2181,bigData104:2181

创建solr collection，用来存储索引文件，-c指定名字，-d指定配置文件，shards 3个分片，replicationFactor两个副本
# vertex_index 存的是点
sudo -i -u solr /opt/module/solr/bin/solr create -c vertex_index -d /opt/module/atlas/conf/solr/ -shards 3 -replicationFactor 2
# vertex_index 存的是线
sudo -i -u solr /opt/module/solr/bin/solr create -c edge_index -d /opt/module/atlas/conf/solr/ -shards 3 -replicationFactor 2
# vertex_index 存的是全文检索
sudo -i -u solr /opt/module/solr/bin/solr create -c fulltext_index -d /opt/module/atlas/conf/solr/ -shards 3 -replicationFactor 2
去 http://bigData102:8983/solr/#/~cloud 看，能看到刚建的索引

这样atlas的元数据的索引信息可以存到solr的collection中了，可以做全文检索了


atlas集成kafka
atlas需要kafka做数据缓冲，所以atlas需要kafka
vim /opt/module/atlas/conf/atlas-application.properties
atlas.notification.embedded=false
# kafka数据的存储位置
atlas.kafka.data=/opt/module/kafka/data
# kafka元数据在zk的位置
atlas.kafka.zookeeper.connect=bigData102:2181,bigData103:2181,bigData104:2181/kafka
# kafka本身broker连接地址
atlas.kafka.bootstrap.servers=bigData102:9092,bigData103:9092,bigData104:9092

atlas server配置
vim /opt/module/atlas/conf/atlas-application.properties
# 设置atlas主机名、端口号
atlas.rest.address=http://bigData102:21000
# 是否每次都初始化
atlas.server.run.setup.on.start=false
# 给atlas设置zk连接地址
atlas.audit.hbase.zookeeper.quorum=bigData102:2181,bigData103:2181,bigData104:2181

开启atlas日志中的记录性能指标
vim /opt/module/atlas/conf/atlas-log4j.xml
放开下面注释
<appender name="perf_appender" class="org.apache.log4j.DailyRollingFileAppender">
	<param name="file" value="${atlas.log.dir}/atlas_perf.log" />
	<param name="datePattern" value="'.'yyyy-MM-dd" />
	<param name="append" value="true" />
	<layout class="org.apache.log4j.PatternLayout">
		<param name="ConversionPattern" value="%d|%t|%m%n" />
	</layout>
</appender>

<logger name="org.apache.atlas.perf" additivity="false">
	<level value="debug" />
	<appender-ref ref="perf_appender" />
</logger>


atlas集成hive，钩取hive元数据
vim /opt/module/atlas/conf/atlas-application.properties
在最后加上
#########  Hive Hook Configs ########
# 关闭hive钩子自动同步，改为手动同步
atlas.hook.hive.synchronous=false
# 重试次数3
atlas.hook.hive.numRetries=3
# 队列大小10000
atlas.hook.hive.queueSize=10000
# 集群名称
atlas.cluster.name=primary

修改Hive配置文件，配置Hive Hook
vim /opt/module/hive/conf/hive-site.xml
在最后一个<property>后面加上
<!-- 让hive知道启用atlas的hook程序 -->	
<property>
   <name>hive.exec.post.hooks</name>
   <value>org.apache.atlas.hive.hook.HiveHook</value>
 </property>

安装hive hook程序
cd /opt/software/9_atlas
tar -zxvf apache-atlas-2.1.0-hive-hook.tar.gz
cp -r apache-atlas-hive-hook-2.1.0/* /opt/module/atlas/
修改hive-env.sh，把atlas/hook/hive给到hive第三方jar包路径，这样hive才能使用atlas的钩子程序，让atlas实时钩取hive元数据的变化
vim /opt/module/hive/conf/hive-env.sh
export HIVE_AUX_JARS_PATH=/opt/module/atlas/hook/hive

把atlas的配置文件拷贝到hive/conf目录
让hive能使用atlas的配置文件
cp /opt/module/atlas/conf/atlas-application.properties /opt/module/hive/conf/

10.2 atlas启动
前提：
启动hadoop集群
myhadoop.sh start
启动zk集群
zk.sh start
启动kafka集群
kf.sh start
启动hbase集群
/opt/module/hbase/bin/start-hbase.sh
启动solr集群，3台机器
sudo -i -u solr /opt/module/solr/bin/solr stop -all
sudo -i -u solr /opt/module/solr/bin/solr start

jpsall
包含jps的话，进程数9 7 7

启动atlas：
/opt/module/atlas/bin/atlas_start.py

错误信息路径：
/opt/module/atlas/logs/*.out
和
application.log

停止atlas
#bin/atlas_stop.py

atlas启动后只在102机器有一个进程

web页面
http://bigData102:21000/

如果jpsall，进程数为10 7 7
服务都起了，页面还是不出来，就是要等比较久，我等了七八分钟，页面就能正常访问了

只要application.log中没有error，就能耐心等待
第一次启动很慢，第二次启动就快了

初始账户密码，admin admin

左侧界面三tab:搜索（主用，查看数据字典，元数据管理和查询，血缘依赖的查看）、查看标签、查看商业分类，右侧是展示

10.3 atlas使用：
atlas使用
同步hive的元数据，构建元数据实体之间的关联关系，对所存储的元数据建立索引，最终为用户提供数据血缘查看和元数据检索

atlas刚装好后，需手动执行一次元数据的全量导入，后面atlas会利用hive hook增量同步hive的元数据

现在hive中有两张表test_user,test_user2

hive元数据初次导入
当公司数据仓库发展到一定阶段（hive中有很多表），才会需要元数据管理这个功能（导入atlas）
/opt/module/atlas/hook-bin/import-hive.sh
输入用户名、密码：admin admin
#Hive Meta Data imported successfully!!! 表示成功

点右上角statistics，查看元数据信息（数据资产目录）
hive_column(8) 8个列
hive_db(1) 1个库
hive_storagedesc(2) hive存储位置
hive_column(2) 2个表

查看血缘关系
点进表，点lineage，后面是关系图、分类、审计日志、元数据信息（数据字典）
跑了sql才有血缘关系

hive元数据增量同步
只要hive元数据发生变化，hive hook会把元数据变动通知atlas。atlas会根据dml获取数据之间的血缘关系

生成血缘依赖
进入hive客户端
hive

创建订单表
create table dwd_order_info (
    `id` string COMMENT '订单编号',
    `final_amount` decimal(16,2) COMMENT '订单最终金额',
    `order_status` string COMMENT '订单状态',
    `user_id` string COMMENT '用户id' ,
    `payment_way` string COMMENT '支付方式',
	`delivery_address` string COMMENT '送货地址',
    `out_trade_no` string COMMENT '支付流水号',
    `create_time` string COMMENT '创建时间',	
    `operate_time` string COMMENT '操作时间',
	`expire_time` string COMMENT '过期时间',
	`tracking_no` string COMMENT '物流单编号',
	`province_id` string COMMENT '省份id',
	`activity_reduce_amount` decimal(16,2) COMMENT '活动减免金额',
	`coupou_reduce_amount` decimal(16,2) COMMENT '优惠券减免金额',
	`original_amount` decimal(16,2) COMMENT '订单原价金额',
	`feight_fee` decimal(16,2) COMMENT '运费',
	`feight_fee_reduce` decimal(16,2) COMMENT '运费减免'    
) COMMENT '订单表'
row format delimited fields terminated by '\t';

既 create table dwd_order_info ( `id` string COMMENT '订单编号',`final_amount` decimal(16,2) COMMENT '订单最终金额',`order_status` string COMMENT '订单状态',`user_id` string COMMENT '用户id' ,`payment_way` string COMMENT '支付方式',`delivery_address` string COMMENT '送货地址',`out_trade_no` string COMMENT '支付流水号',`create_time` string COMMENT '创建时间',`operate_time` string COMMENT '操作时间',`expire_time` string COMMENT '过期时间',`tracking_no` string COMMENT '物流单编号',`province_id` string COMMENT '省份id',`activity_reduce_amount` decimal(16,2) COMMENT '活动减免金额',`coupou_reduce_amount` decimal(16,2) COMMENT '优惠券减免金额',`original_amount` decimal(16,2) COMMENT '订单原价金额',`feight_fee` decimal(16,2) COMMENT '运费',`feight_fee_reduce` decimal(16,2) COMMENT '运费减免' ) COMMENT '订单表' row format delimited fields terminated by '\t';

创建地区维度表
# DROP TABLE IF EXISTS `dim_base_province`;
CREATE TABLE `dim_base_province` (
`id` string COMMENT '编号',
`name` string COMMENT '省份名称',
`region_id` string COMMENT '地区 id',
`area_code` string COMMENT '地区编码',
`iso_code` string COMMENT 'ISO-3166编码，供可视化使用',
`iso_3166_2` string COMMENT 'ISO-3166-2编码，供可视化使用'
) COMMENT '省份表'
row format delimited fields terminated by '\t';

既CREATE TABLE `dim_base_province` (`id` string COMMENT '编号',`name` string COMMENT '省份名称',`region_id` string COMMENT '地区 id',`area_code` string COMMENT '地区编码',`iso_code` string COMMENT 'ISO-3166编码，供可视化使用',`iso_3166_2` string COMMENT 'ISO-3166-2编码，供可视化使用') COMMENT '省份表' row format delimited fields terminated by '\t';

这些建表语句就是ddl，数据的定义源

在atlas页面右上角Statistics点开，hive_table(4)，由2变为4，atlas已能实时获取hive元数据变化

给两张表填充数据，将base_province.txt和order_info.txt上传到hive表对应目录下

base_province.txt：
1	北京	1	110000	CN-11	CN-BJ
2	天津	1	120000	CN-12	CN-TJ
3	山西	1	140000	CN-14	CN-SX
4	内蒙古	1	150000	CN-15	CN-NM
5	河北	1	130000	CN-13	CN-HE
6	上海	2	310000	CN-31	CN-SH
7	江苏	2	320000	CN-32	CN-JS
8	浙江	2	330000	CN-33	CN-ZJ
9	安徽	2	340000	CN-34	CN-AH
10	福建	2	350000	CN-35	CN-FJ
11	江西	2	360000	CN-36	CN-JX
12	山东	2	370000	CN-37	CN-SD
14	台湾	2	710000	CN-71	CN-TW
15	黑龙江	3	230000	CN-23	CN-HL

order_info.txt:
4934	12033.00	1004	11	\N	第14大街第6号楼5单元647门	366893968325668	2020-06-10 22:39:20.0	2020-06-10 22:39:20.0	2020-06-10 22:54:20.0	\N	8	200.00	0.00	12218.00	15.00	\N
4935	12065.00	1005	22	\N	第6大街第11号楼2单元864门	845678229874954	2020-06-10 22:39:20.0	2020-06-12 22:43:57.0	2020-06-10 22:54:20.0	\N	11	0.00	70.00	12115.00	20.00	\N
4936	8308.00	1005	24	\N	第1大街第1号楼6单元562门	886898254767124	2020-06-10 22:39:20.0	2020-06-11 22:42:49.0	2020-06-10 22:54:20.0	\N	25	0.00	0.00	8298.00	10.00	\N
4937	3442.00	1005	37	\N	第17大街第23号楼7单元518门	333789257252777	2020-06-10 22:39:20.0	2020-06-10 22:39:20.0	2020-06-10 22:54:20.0	\N	17	0.00	30.00	3453.00	19.00	\N
4938	21014.00	1005	39	\N	第1大街第39号楼3单元294门	334997784963613	2020-06-10 22:39:20.0	2020-06-10 22:39:20.0	2020-06-10 22:54:20.0	\N	8	0.00	0.00	20997.00	17.00	\N
4939	2255.00	1005	47	\N	第11大街第7号楼7单元661门	456112632711976	2020-06-10 22:39:20.0	2020-06-13 22:44:29.0	2020-06-10 22:54:20.0	\N	21	0.00	70.00	2309.00	16.00	\N
4940	24904.00	1005	57	\N	第14大街第37号楼5单元536门	232551552927594	2020-06-10 22:39:20.0	2020-06-13 22:44:29.0	2020-06-10 22:54:20.0	\N	3	500.00	0.00	25390.00	14.00	\N
4941	8401.00	1005	58	\N	第12大街第18号楼4单元334门	711941717753122	2020-06-10 22:39:20.0	2020-06-12 22:43:57.0	2020-06-10 22:54:20.0	\N	12	0.00	0.00	8385.00	16.00	\N

http://bigData102:9870/explorer.html#/user/hive/warehouse
路径/user/hive/warehouse下面，上传到对应目录

去hive看一下
hive
select * from dwd_order_info;
select * from dim_base_province;

假设需求指标：
根据订单事实表，地区维度表，求出每个省份的订单次数和订单金额

建表：
create table `ads_order_by_province` (
	`dt` string comment '统计日期',
	`province_id` string comment '省份id',
	`province_name` string comment '省份名称',
	`area_code` string comment '地区编码',
	`iso_code` string comment '国际标准地区编码',
	`iso_code_3166_2` string comment '国际标准地区编码',
	`order_count` bigint comment '订单数',
	`order_amount` decimal(16,2) comment '订单金额'
) comment '各省份订单统计' 
row format delimited fields terminated by '\t';

既create table `ads_order_by_province` (`dt` string comment '统计日期',`province_id` string comment '省份id',`province_name` string comment '省份名称',`area_code` string comment '地区编码',`iso_code` string comment '国际标准地区编码',`iso_code_3166_2` string comment '国际标准地区编码',`order_count` bigint comment '订单数',`order_amount` decimal(16,2) comment '订单金额') comment '各省份订单统计' row format delimited fields terminated by '\t';

数据装载
insert into table ads_order_by_province
select 
	'2021-08-30' dt,
	bp.id,
	bp.name,
	bp.area_code,
	bp.iso_code,
	bp.iso_3166_2,
	count(*) order_count,
	sum(oi.final_amount) order_amount
from dwd_order_info oi 
left join dim_base_province bp 
on oi.province_id=bp.id 
group by bp.id,bp.name,bp.area_code,bp.iso_code,bp.iso_3166_2;

既insert into table ads_order_by_province select '2021-08-30' dt,bp.id,bp.name,bp.area_code,bp.iso_code,bp.iso_3166_2,count(*) order_count,sum(oi.final_amount) order_amount from dwd_order_info oi left join dim_base_province bp on oi.province_id=bp.id group by bp.id,bp.name,bp.area_code,bp.iso_code,bp.iso_3166_2;

hive里执行sql，atlas中可以实时获取到血缘关系

select * from ads_order_by_province;

表的血缘关系，数据来自哪些表
http://bigData102:21000/index.html#!/detailPage/c41f2548-39f6-4033-aaef-b17bfad8f544?tabActive=lineage

数据质量监控
字段的血缘关系
这一字段分别来源于哪些字段，源头字段来自哪张表
http://bigData102:21000/index.html#!/detailPage/87939055-4108-4230-a8af-682f13ad146a?tabActive=lineage

atlas主要功能：
1.右上角查看数据资产目录（数据字典）
2.左边搜索框（数据字典）
3.表或字段的血缘关系图

atlas源码编译（将atlas源码包编译成安装包，官网只有源码包）
将maven3.6.1下到/opt/software/下
解压到/opt/module/中
改名为maven
添加环境变量到/etc/profile
#MAVEN_HOME
export MAVEN_HOME=/opt/module/maven
export PATH=$PATH:$MAVEN_HOME/bin
刷新环境变量
修改maven setting.xml
vim /opt/module/maven/conf/settings.xml
<!--阿里云镜像-->
<mirror>
 <id>nexus-aliyun</id>
 <mirrorOf>central</mirrorOf>
 <name>Nexus aliyun</name>
<url>http://maven.aliyun.com/nexus/content/groups/public</url>
</mirror>
<mirror>
 <id>UK</id>
 <name>UK Central</name>
 <url>http://uk.maven.org/maven2</url>
 <mirrorOf>central</mirrorOf>
</mirror>
<mirror>
 <id>repo1</id>
 <mirrorOf>central</mirrorOf>
 <name>Human Readable Name for this Mirror.</name>
 <url>http://repo1.maven.org/maven2/</url>
</mirror>
<mirror>
 <id>repo2</id>
 <mirrorOf>central</mirrorOf>
 <name>Human Readable Name for this Mirror.</name>
 <url>http://repo2.maven.org/maven2/</url>
</mirror>

把 apache-atlas-2.1.0-sources.tar.gz 上传到 bigData102 的/opt/software 目录下
解压 apache-atlas-2.1.0-sources.tar.gz 到/opt/module/目录下面
下载 Atlas 依赖
export MAVEN_OPTS="-Xms2g -Xmx2g"
cd /opt/module/apache-atlas-sources2.1.0/
mvn clean -DskipTests install
mvn clean -DskipTests package -Pdis
#一定要在${atlas_home}执行
cd distro/target/
mv apache-atlas-2.1.0-server.tar.gz /opt/software/
mv apache-atlas-2.1.0-hive-hook.tar.gz /opt/software/

提示：执行过程比较长，会下载很多依赖，大约需要半个小时，期间如果报错很有可能是因为 TimeOut 造成的网络中断，重试即可。

atlas内存配置

atlas存储数万个元数据对象，建议调整参数值获得最佳的 JVM GC（垃圾回收器） 性能。以下是常见的服务器端选项
1）修改配置文件/opt/module/atlas/conf/atlas-env.sh

#设置 Atlas 内存
export ATLAS_SERVER_OPTS="-server -XX:SoftRefLRUPolicyMSPerMB=0
-XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC -
XX:+CMSParallelRemarkEnabled -XX:+PrintTenuringDistribution -
XX:+HeapDumpOnOutOfMemoryError -
XX:HeapDumpPath=dumps/atlas_server.hprof -Xloggc:logs/gcworker.log -verbose:gc -XX:+UseGCLogFileRotation -
XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1m -
XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps"

#建议 JDK1.7 使用以下配置
export ATLAS_SERVER_HEAP="-Xms15360m -Xmx15360m -
XX:MaxNewSize=3072m -XX:PermSize=100M -XX:MaxPermSize=512m"
#建议 JDK1.8 使用以下配置
export ATLAS_SERVER_HEAP="-Xms15360m -Xmx15360m -
XX:MaxNewSize=5120m -XX:MetaspaceSize=100M -
XX:MaxMetaspaceSize=512m"
#如果是 Mac OS 用户需要配置
export ATLAS_SERVER_OPTS="-Djava.awt.headless=true -
Djava.security.krb5.realm= -Djava.security.krb5.kdc="

参数说明：-XX:SoftRefLRUPolicyMSPerMB 此参数对管理具有许多并发用户的查询繁重工作负载的 GC 性能特别有用。

4.3 配置新用户名密码

#Atlas 支持以下身份验证方法：File、Kerberos 协议、LDAP 协议
#通过修改配置文件 atlas-application.properties 文件开启或关闭三种验证方法
#atlas.authentication.method.kerberos=true|false
#atlas.authentication.method.ldap=true|false
#atlas.authentication.method.file=true|false
#如果两个或多个身份证验证方法设置为 true，如果较早的方法失败，则身份验证将回退到后一种方法。
#例如，如果 Kerberos 身份验证设置为 true 并且 ldap 身份验证也设置为 true，那么，如果对于没有 kerberos principal 和 keytab 的请求，LDAP #身份验证将作为后备方案。

本文主要讲解采用文件方式修改用户名和密码设置。其他方式可以参见官网配置即可。
1）打开/opt/module/atlas/conf/users-credentials.properties 文件
vim users-credentials.properties

admin=ADMIN::8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918
#用户名=组名::密码（sha256加密后）
linux中得到sha256密文方式：echo -n "shonqian"|sha256sum

重启atlas生效

修改用户名和密码
vim users-credentials.properties
#username=group::sha256-password
shonqian=ADMIN::2628be627712c3555d65e0e5f9101dbdd403626e6646b72fdf728a20c5261dc2

rangertagsync=RANGER_TAG_SYNC::e3f67240f5117d1753c940dae9eea772d36ed5fe9bd9c94a300e40413f1afb9d
--------------
大数据项目理解：
1.数据采集：
数据不像业务系统那样一条条产生，通过文件或者数据库形式导入
1.1建立hive数据仓库
在hive中建立表结构，把数据导入hdfs对应路径

2.数据处理
2.1根据需求，通过hive语句把现有数据进行联合查询、合并到一张新表
2.2atlas通过hive hook，可以自动获取到hive的表结构和数据，包括后续更新
--------------
hive用法：

数据采集阶段：
1.入mysql库
2.入hdfs
3.在hive中建表（外部表）
4.hdfs数据导入hive表
load data local inpath '/opt/module/datas/dept.txt' into table default.dept;

数据查询阶段：
1.操作hive
根据hive查询结果建表

hive根据数据建表结构
1hive中create建表
2本地文件到hdfs
hadoop fs -put xxx.txt /
3hdfs到hive，会将/下文件移动到hive目录下
load data inpath "/xxx.txt" into table a;

命令：
hdfs dfs -ls /;
hive中dfs -ls /;

hive中数组：
a_b_c   意思[a,b,c]
friend array<string>

hive中map：
tom:18_jerry:19   意思{tom:18,jerry:19}
children map<string,int>

hive中struct：
hui long guan_beijing   意思{street:hui long guan,city:beijing}
address struct<string:string,city:string>

if not exists

查询表结构
desc a;
添加列
alter table a add columns (bbb string);
更新列：
alter table a change bbb ccc string;
删除表：
drop table a;
hive中不能删除列？
只能删除最后一列，

dfs -mkdir -p /a/b/c;

内外部表
create table if not exists a ()
create external table if not exists a ()

建内外部表时，hdfs上文件都会移到hive目录下，但是外部表删后，hive目录下还是有文件

建表时指定文件存放路径location '/xx/ss';

指定location后就会自动执行load操作
指定location后建外部表好，删除hive，原目录不删除

重命名表名
alter table a rename to b;

查询插入数据：
insert into table a select s,c from k; 
插入覆盖原数据
insert overwrite table a select s,c from k; 
同时插入两张表
from k insert into table a select s into table b select c;
直接插入
insert into table xx values ('1','ss'),('2','ssv'),('3','ssw');

hive导出数据，数据备份，将查询结构存放到本地
insert into/overwrite local directory '/root/xx/data' select * from a;
将查询结构存放到hdfs上：
hdfs dfs -mkdir -p /xxx/aaa
insert into/overwrite directory '/root/xx/data' select * from a;
hdfs备份hive目录数据
hdfs dfs -cp /hive/warehouse/xxx/* /sds/sss
表结构和数据同时备份
export tabke x to '/ssds/sss';
恢复表结构和数据(表从无到有)
import from '/ssds/sss';

详细信息
desc formatted a;

hive分区表（在目录的表现是多文件夹）
把数据按每天或者每小时分文件，或按部门、业务等
静态分区sp
create table...
partitioned by(grade int)//分区字段不能和表字段相同
载入数据
load data inpath '/as/sss.txt' into table a partition(grade=1);
多分区
partitioned by(grade int,clazz int)//前后为父子关系
分区表查询，可以把分区当字段用
select * .... where grade=1 and clazz=2;
查看分区
show partitions table_a;
添加分区
alter table a add partition (day='121212');
删除分区
alter table a drop partition (day='121212');
动态分区dp
静态：insert overwrite table a partition(gender=1) select id,name from b;
动态：
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table a partition(gender) select id,name from b;

分桶表
按照数据按照字段，划分到多个文件中
set hive.enforce.bucheting=true;
mapreduce数量和桶数量要一致
set mapreduce.job.reduce=3;

指定xx为桶值，aa降序排，16个桶
create table a...clustered by(xx) sorted by (aa desc) into 16 buckets
数据块抽样
随机抽取10%数据
create table xx as select * from xxx tablesample(10 percent);
根据桶的数量抽样
select * from xx tablesample(bucket 1 out of 16 on aa);

排序
全局排序order by
局部排序sort by，进入reduce前排序，只保证每个reduce有序，不保证全局有序
分区排序distribute by，reduce间有序
分区并排序cluster by=distribute by + sort by，只能升序，不能倒序

hive自带函数
show functions;
显示用法
desc function upper;
详细显示用法
desc function extended upper;

udtf，一分多
explode将一组数组的数据变成多行表记录
select explode(split(types,"-")) from a;
lateral view，将explode的数据生成一个列表

utaf，多合一
concat_ws(':',collect_set(type))

hive窗口函数
排序开窗函数
当分数为：81，73，73，66时
rank() // 1224
dense_rank() // 1223
row_number() // 1234 

自定义函数
udf一进一出
udaf多进一出
udtf一进多出
udf
1.pom依赖
hadoop-client
hadoop-hdfs
hive-exec
2.继承UDF，重写evaluate
3.将jar包传到hdfs上指定jar包目录
udaf
继承udaf

hive参数设置
hiveconf,system,hivevar
1.配置文件方式：重启生效
在${HIVE_HOME}/conf/hive-site.xml配置
2.命令行：当前一次生效
3.set设置

hive数据倾斜
单个节点处理的任务量，远大于其他
原因，相同key，或者不支持分割的大文件

hive内外部表转换
alter table a set tblproperties('external'='true/false');

tez代替mapreduce，提升速度

内部表
create table
内部表数据由Hive自身管理
存储的位置
hive.metastore.warehouse.dir（默认：/user/hive/warehouse）
删除内部表会直接删除元数据（metadata）及存储数据；
分建表和数据载入

外部表
create external table
外部表数据由HDFS管理
存储位置由自己制定
删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除；
同时完成创建和数据载入

hiveserver2提供了一个新的命令行工具beeline，增加了权限控制
要使用beeline需要先启动hiverserver2，再使用beeline连接

启动hiveserver2服务
hiveserver2 &
或
$HIVE_HOME/bin/hiveserver2 &
或
nohup hive --service metastore & ?
nohup hive --service hiveserver2 &

启动beeline，连接hiveserver2
jdbc方式连接hive
beeline
!connect jdbc:hive2://hadoop101:10000
或
beeline -u jdbc:hive2://bigData101:10000 -n shonqian

hive UI
http://192.168.30.99:10002/

向表中装载数据（Load）
hive> load data [local] inpath '/opt/module/datas/student.txt' overwrite | into table student [partition (partcol1=val1,…)];
（1）load data:表示加载数据
（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表
（3）inpath:表示加载数据的路径
（4）overwrite:表示覆盖表中已有数据，否则表示追加
（5）into table:表示加载到哪张表
（6）student:表示具体的表
（7）partition:表示上传到指定分区

创建一张表
hive (default)> create table student(id string, name string) row format delimited fields terminated by '\t';
加载HDFS文件到hive中
上传文件到HDFS
hive (default)> dfs -put /opt/module/datas/student.txt /user/atguigu/hive;
加载HDFS上数据
hive (default)> load data inpath '/user/atguigu/hive/student.txt' into table default.student;

创建表时通过Location指定加载数据路径
1．创建表，并指定在hdfs上的位置
hive (default)> create table if not exists student5(
              id int, name string
              )
              row format delimited fields terminated by '\t'
              location '/user/hive/warehouse/student5';
2．上传数据到hdfs上
hive (default)> dfs -put /opt/module/datas/student.txt
/user/hive/warehouse/student5;
3．查询数据
hive (default)> select * from student5;

hive建表三种方式
1.create命令

2.查询用as接收（不能把原hive表的列注释带过去）
create table test 
        as   
       select...

3.like复制表结构（能把原hive表的列注释带过去）
create table test like default.a;
--------------
把hive的mapreduce替换成spark：

hive on spark生态好，语法hql
spark on hive速度快，语法spark sql

hive3.1.2与spark3.0.0兼容问题
官网hive3.1.2支持的spark是2.4.5的，所以我们要手动修改下
官网下载hive3.1.2源码，修改pom文件引用的spark改为3.0.0，重新编译jar包

检测lib下spark是否为3.0.0
1.进入hive lib目录
cd /opt/module/hive/lib
2.查询spark的相关依赖
ls -al | grep spark

部署spark
cd /opt/software/spark
将
spark-3.0.0-bin-hadoop3.2.tgz
spark-3.0.0-bin-without-hadoop.tgz
传入
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/

cd /opt/module/
mv spark-3.0.0-bin-hadoop3.2/ spark

root账户
vim /etc/profile.d/my_env.sh
#spark
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin
source /etc/profile

在hive中创建spark配置文件
vim /opt/module/hive/conf/spark-defaults.conf
spark.master    yarn #spark计算模式
spark.eventLog.enabled  true #开启spark的事件日志
spark.eventLog.dir  hdfs://bigData101:8020/spark-history #指定日志路径
spark.executor.memory   1g #配置executor和driver内存大小
spark.driver.memory 1g

hadoop fs -mkdir /spark-history

将spark纯净版jar包上传到hdfs，这样每次提交任务就不用再重复上传了，纯净版避免冲突
hadoop fs -mkdir /spark-jars
cd /opt/software/spark/
tar -zxvf spark-3.0.0-bin-without-hadoop.tgz
cd spark-3.0.0-bin-without-hadoop/jars/
hadoop fs -put ./* /spark-jars

修改hive-site.xml文件
vim /opt/module/hive/conf/hive-site.xml
<!--Spark依赖位置（端口号8020必须和namenode端口号一致）-->
<property>
    <name>spark.yarn.jars</name>
    <value>hdfs://bigData101:8020/spark-jars/*</value>
</property>
<!--hive执行引擎-->
<property>
    <name>hive.execution.engine</name>
    <value>spark</value>
</property>

测试spark引擎是否成功
hive
create table temp_student(id int,name string);
insert into table temp_student values (1,'abc');

出现
STAGES: 02/02    [==========================>>] 100%  ELAPSED TIME: 3.05 s
代表成功

如果报错：
FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session 7fc2665b-5b7e-496f-a8bf-4a87430d8a00
或者：
yarn http://bigdata101:8088/cluster
上任务报错，Timed out waiting for RPC server connection

重试下命令
或者
中止RunJar进程？
或者
重启hadoop相关服务？
或者
set hive.spark.client.server.connect.timeout=300000;？
或者vim /opt/module/hive/conf/hive-site.xml中增加
<property>
    <name>hive.spark.client.server.connect.timeout</name>
    <value>2000000ms</value>
</property>?
<property>
    <name>hive.spark.client.connect.timeout</name>
    <value>200000ms</value>
</property>？
或者
cd /opt/module/spark/conf
mv spark-env.sh.template spark-env.sh
最后增加export SPARK_DIST_CLASSPATH=${hadoop classpath}？
可能是hive连接数只能1个？断开sql编辑器看看？
看看报错日志在哪，hive或者spark？
或
vim yarn-site.xml ?
<property>
   <name>yarn.scheduler.minimum-allocation-mb</name>
   <value>2048</value>
   <description>default value is 512</description>
</property>
--------------
Azkaban（轻）与oozie（重）对比
市面上最流行的两种工作流调度系统

简单调度：
Linux的Crontab

一个完整的数据分析系统通常都是由大量任务单元组成：
Shell脚本程序，Java程序，MapReduce程序、Hive脚本等

大数据工作流调度流程：
某业务系统每天产生20G原始数据，我们每天对其进行处理
1.先将原始数据同步到HDFS上
HDFS
2.对原始数据进行计算
MapReduce
3.生成的数据以分区表形式存储到多张Hive表中
[Hive表，Hive表，Hive表]
4.对Hive中多个表的数据进行JOIN处理，得到一张明细数据Hive大表
明细数据Hive大表
5.将明细数据进行复杂的统计分析，得到结果报表信息
结果报表
6.将统计分析得到的结果数据同步到业务系统中
业务系统

Azkaban集群部署
web-server,exec-server,mysql
将
azkaban-db-3.84.4.tar.gz
azkaban-exec-server-3.84.4.tar.gz
azkaban-web-server-3.84.4.tar.gz
上传至/opt/software/azkaban/
解压至/opt/module/azkaban/下

mv azkaban-exec-server-3.84.4 azkaban-exec
mv azkaban-web-server-3.84.4 azkaban-web

mysql -uroot -proot
create database azkaban;
use azkaban;

source /opt/module/azkaban/azkaban-db-3.84.4/create-all-sql-3.84.4.sql;

show tables;

quit;

修改mysql允许包大小，防止azkaban连接mysql阻塞
sudo vim /etc/my.cnf
在[mysqld]下面加上
max_allowed_packet=1024M

重启mysql
sudo systemctl restart mysqld
完成数据库初始化

配置ExecutorServer
cd /opt/module/azkaban/azkaban-exec/conf
vim azkaban.properties
default.timezone.id=Asia/Shanghai
azkaban.webserver.url=http://bigData101:8081
mysql.host=bigData101
mysql.user=root
mysql.password=root
最后添加executor.port=12321//不指定的话，每次都会是随机值，不方便管理

启动executor
cd /opt/module/azkaban/azkaban-exec
bin/start-exec.sh
// 停止命令bin/shutdown-exec.sh
// 把azkaban.properties里的相对路径改为绝对路径后，就能不用进目录再执行了，如conf/...

ll 查看当前目录下是否有executor.port文件，有说明启动成功，没有就查看日志

激活executor
向服务发送一个get请求即可
curl -G "bigData101:12321/executor?action=activate" && echo

配置Web Server
cd /opt/module/azkaban/azkaban-web/
vim conf/azkaban.properties
default.timezone.id=Asia/Shanghai
mysql.host=bigData101
mysql.user=root
mysql.password=root
azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus // 多executor时设置，az选择器挑选机制：StaticRemainingFlowSize目前排队任务数最少，MinimumFreeMemory最小内存限制6G(会导致任务无法执行)，CpuStatus cpu使用率最低
vim conf/azkaban-users.xml用户、权限管理文件
添加
<user password="root" roles="admin" username="shonqian"/>
启动
bin/start-web.sh
// 停止 bin/shutdown-web.sh

访问
http://bigdata101:8081/
登录
shonqian
root

在windows上
新建first.project
azkaban-flow-version: 2.0

新建first.flow
nodes:
  - name: jobA
    type: command
    config:
      command: echo "Hello World"

两个打包在一起，zip格式

右上角，Create Project

upload上传那个zip压缩包

点Execute Flow

左边Schedule定时调度
右边Execute立即执行

job每一个工作单元
flow所有工作单元组成的工作流

日志排查在Job List里的Log里

first.project文件内容固定，确定flow版本
first.flow文件（可properties和yaml两种格式）

yaml：
1.大小写敏感
2.缩进表示层级
3.不能用tab，用空格
4.缩进空格数不重要，相同层级元素要左侧对齐
5.#表示注释

yaml语法：
对象：{name:zhangsan,age:18}
name: zhangsan
age: 18
数组：[a,b,c]
- a
- b
- c
对象数组：[{name:zhangsan,age:18},{name:zhangsan,age:18},{name:zhangsan,age:18}]
- name: zhangsan
  age: 18
- name: zhangsan
  age: 18
- name: zhangsan
  age: 18
--------------
sqoop数据传递工具
(mysql,oracle) <=> (hdfs,hive,hbase)互导工具

sqoop版本
sqoop1（1.9之前）
sqoop2（1.9之后）
1与2不兼容

sqoop的导入/导出就是对mapreduce的inputformat和outputformat进行定制

安装：
0.安装前提，已有java和hadoop环境
1.下载安装包，一般使用sqoop1.4.6
2.解压
3.配置环境变量
4.修改配置文件
sqoop/conf/下
mv sqoop-env-template.sh sqoop-env.sh
配置入hadoop和hive等需要的地址
export HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3
export HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3
export HIVE_HOME=/opt/module/hive
5.拷贝jdbc驱动到sqoop/lib/下
6.查看sqoop是否配置正确
sqoop help

用法：
1.查出所有库
sqoop list-databases --connect jdbc:mysql://192.168.30.95:3306/ --username root --password root
#list-tables查出所有表

2.导入import（一般指关系型数据库 => 大数据仓库hdfs,hive,hbase）

2.1单表到hdfs
sqoop import \
--connect jdbc:mysql://192.168.40.113:3306/pra \
--username root \
--password root \
--table emp \
--target-dir /test/6/ \ #--target-dir指定目录，已有会报错
--fields-terminated-by "\t" #每字段结束符是tab键

2.2全库到hdfs
sqoop import-all-tables --connect "jdbc:mysql://192.168.40.113:3306/pra" --username root --password root --warehouse-dir "/test/4" -m 1 --fields-terminated-by "\t"
#--warehouse-dir指定父目录
#-m 指定mapreduce进程数

2.3单表到hive（包括数据），（sqoop把mysql=>hive，不能把mysql注释带过去）
sqoop-import --connect 'jdbc:mysql://192.168.40.113:3306/pra' --username root --password root --table emp --delete-target-dir --hive-import --hive-overwrite  --hive-table default.qqq --null-string '\\N' --null-non-string '\\N' -m 1
#--table指定关系型数据库的表名
#--hive-table指定hive的表名
#--hive-import 数据导入hdfs后，再载入hive目录

单表到hive（仅表结构）
sqoop create-hive-table --connect 'jdbc:mysql://192.168.40.113:3306/pra' --username root --password root --table emp --hive-table default.qqq2

java api调sqoop，单表到hive（仅表结构），表已有的话不会同步，也不报错
"create-hive-table",
"--connect", "jdbc:mysql://192.168.40.113:3306/pra?serverTimezone=Asia/Shanghai",
"-username", "root",
"-password", "root",
"--table", "emp",
"--hive-table","xxx",
"--fields-terminated-by","\t"

2.4全库到hive
sqoop import-all-tables \
-Dorg.apache.sqoop.splitter.allow_text_splitter=true \
--connect jdbc:mysql://192.168.40.113:3306/pra \
--username root \
--password root \
--hive-import \
-m 1

3.其它用法

3.1查询
--query 'select name,sex from staff where id <=1 and $CONDITIONS;'
# $CONDITIONS为固定用法

3.2导入指定列
--columns id,sex

3.3常用参数、命令 => 百度

增量导入
1.按数值类型增长（比如id）
append追加模式
append --check-column id --last-value 1201
更新数据追加到新文件，会造成重复数据

2.按时间（比如xxxTime）
2.1Lastmodified上次更新模式
时间+1秒，会取>=的记录
更新数据追加到新文件，会造成重复数据

2.2merge-key形式
把增量数据合并到原文件中
导入增量数据和更新变化的数据，且数据不会重复，
底层相当于做了一次完整的mapreduce

sqoop当中job作业
用其它调度软件工作调度这个job

同步策略
1.全量同步（每日全量表=>hive表中的一个分区），数据量小/码表
2.增量同步，数据量大+不变化
3.新增及变化同步，数据量大+变化
4.特殊情况（只需要存储一次，比如地区表、省份表）

业务表同步
第一次全量同步
sqoop import \
--connect jdbc:mysql://192.168.30.95:3306/gmall \
--username root \
--password root \
--query 'select * from order_info where $CONDITIONS' \
--target-dir /order_info/2020-06-14 \ #--target-dir指定目录，已有会报错
--delete-target-dir \ #已存在会删除
--fields-terminated-by "\t" #每字段结束符是tab键
--num-mappers 2 \ #指定mapreduce数
--split-by id #分割基准字段
后续新增及变化同步
sqoop import \
--connect jdbc:mysql://192.168.30.95:3306/gmall \
--username root \
--password root \
--query 'select * from order_info where (date_format(create_time,'%Y-%m-%d')='2020-06-15' or date_format(update_time,'%Y-%m-%d')='2020-06-15') and $CONDITIONS' \
--target-dir /order_info/2020-06-15 \ #--target-dir指定目录，已有会报错
--delete-target-dir \ #已存在会删除
--fields-terminated-by "\t" #每字段结束符是tab键
--num-mappers 2 \ #指定mapreduce数
--split-by id #分割基准字段

编写首日同步shell脚本，日期用变量传入
mysql_to_hdfs_init.sh
#!/bin/bash

APP=gmall#数据库库名
sqoop=/opt/module/sqoop/bin/sqoop#sqoop绝对路径

if [ -n "$2" ] ;then#-n非空判断，返回布尔
   do_date=$2
else 
   echo "请传入日期参数"
   exit
fi 

import_data(){#公用函数
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \#$1值此函数的第一个参数
--delete-target-dir \
--query "$2 where \$CONDITIONS" \#$2值此函数的第二个参数
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \#输出文件进行压缩
--compression-codec lzop \#压缩格式lzop
--null-string '\\N' \#将mysql中null值转换，hive中null值为\N
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}#为上传的lzop文件创建索引
import_order_info(){
  import_data order_info #调用import_data函数，order_info为$1既传入参数
  "select#为import_data函数的$2参数
        id, 
        total_amount, 
        order_status, 
        user_id, 
        payment_way,
        delivery_address,
        out_trade_no, 
        create_time, 
        operate_time,
        expire_time,
        tracking_no,
        province_id,
        activity_reduce_amount,
        coupon_reduce_amount,                            
        original_total_amount,
        feight_fee,
        feight_fee_reduce      
    from order_info"
}

import_coupon_use(){
  import_data coupon_use 
  "select
        id,
        coupon_id,
        user_id,
        order_id,
        coupon_status,
        get_time,
        using_time,
        used_time,
        expire_time
    from coupon_use"
}

case $1 in #脚本的第一个参数
  "order_info")
     import_order_info #调import_order_info方法，同步order_info数据
;;
  "coupon_use")
      import_coupon_use
;;
  "all") #传all就同步所有表数据
   import_order_info
   import_coupon_use
;;
esac

编写每日同步shell脚本
mysql_to_hdfs.sh
#!/bin/bash

APP=gmall#数据库库名
sqoop=/opt/module/sqoop/bin/sqoop#sqoop绝对路径

if [ -n "$2" ] ;then#-n非空判断，返回布尔
   do_date=$2
else 
   do_date=`date -d '-1 day'+%F` #为空就取前一天
fi 

import_data(){#公用函数
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \#$1值此函数的第一个参数
--delete-target-dir \
--query "$2 where \$CONDITIONS" \#$2值此函数的第二个参数
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \#输出文件进行压缩
--compression-codec lzop \#压缩格式lzop
--null-string '\\N' \#将mysql中null值转换，hive中null值为\N
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}#为上传的lzop文件创建索引
import_order_info(){
  import_data order_info #调用import_data函数，order_info为$1既传入参数
  "select#为import_data函数的$2参数
        id, 
        total_amount, 
        order_status, 
        user_id, 
        payment_way,
        delivery_address,
        out_trade_no, 
        create_time, 
        operate_time,
        expire_time,
        tracking_no,
        province_id,
        activity_reduce_amount,
        coupon_reduce_amount,                            
        original_total_amount,
        feight_fee,
        feight_fee_reduce      
    from order_info
    #新增及变化同步
    where (date_format(create_time,'%Y-%m-%d')='$do_date' 
    or date_format(update_time,'%Y-%m-%d')='$do_date')"
}

import_comment_info(){
  import_data comment_info 
  "select
        id,
        user_id,
        sku_id,
        spu_id,
        order_id,
        appraise,
        create_time
    from comment_info
    #新增同步
    where (date_format(create_time,'%Y-%m-%d')='$do_date')"
}
#商品表
import_sku_info(){
  import_data sku_info 
  "select
        id,
        spu_id,
        price,
        sku_name,
        sku_desc,
        weight,
        tm_id,
        category3_id,
        is_sale,
        create_time
    from sku_info
    #全量同步
    where (1=1)"
}

case $1 in #脚本的第一个参数
  "order_info")
     import_order_info #调import_order_info方法，同步order_info数据
;;
  "comment_info")
      import_comment_info
;;
  "sku_info")
      import_sku_info
;;
  "all") #同步每一张表数据
   import_order_info
   import_comment_info
   import_sku_info
;;
esac

第一个参数传表名或者all，第二个参数传日期
-------------
java操作sqoop1，将mysql数据导入hdfs
1.pom
<!-- sqoop1同步hdfs所需依赖 -->
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
</dependency>
<!-- https://repo.maven.apache.org/maven2/org/apache/sqoop/sqoop/1.4.6/ -->
<dependency>
    <groupId>org.apache.sqoop</groupId>
    <artifactId>sqoop</artifactId>
    <version>1.4.6</version>
    <classifier>hadoop200</classifier>
</dependency>
<!-- hadoop -->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-core</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-common</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
    <version>2.8.4</version>
    <scope>test</scope>
</dependency>
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.8.1</version>
</dependency>
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-mapred</artifactId>
    <version>1.8.1</version>
</dependency>

2.service
public void mysql2hdfs() {
    try {
        String[] args = new String[]{
                "import",
                "--connect", "jdbc:mysql://192.168.40.113:3306/pra?serverTimezone=Asia/Shanghai",
                "--driver", "com.mysql.cj.jdbc.Driver",
                "-username", "root",
                "-password", "root",
                "--table", "emp",
                "--null-string", "na",
                "--null-non-string", "na",
                "-m", String.valueOf(1),
                "--outdir", "/temp/xx/",
                "--delete-target-dir",
                "--target-dir", /test/5,
                "--fields-terminated-by","\t",
                "--hadoop-mapred-home", "/opt/module/hadoop-3.1.3"
        };
        String[] expandArguments = OptionsFileUtil.expandArguments(args);
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS", "hdfs://bigData101:8020");
        Sqoop.runTool(expandArguments, conf);
    } catch (Exception e) {
        e.printStackTrace();
    }
}

3.问题
3.1windows下需要hadoop运行所需文件和环境变量：
安装winutils，看看对应版本有没有hadoop.dll和winutils.exe这两个文件
将winutils下对应hadoop版本添加到系统变量
HADOOP_HOME C:\qxn\myPro\winutils\hadoop-3.0.0
添加Path变量，%HADOOP_HOME%\bin
将hadoop.dll复制到C:\Window\System32下
重启idea
还不行的话，试试重启电脑

在Windows下给hadoop指定用户，解决权限问题
System.setProperty("HADOOP_USER_NAME","shonqian");

3.2打成jar包后运行：
在sqoop运行过程中，会在/tmp/下生成一个mysql表名.java文件，文件头部有
import org.apache.hadoop.io.BytesWritable;
import com.cloudera.sqoop.lib.JdbcWritableBridge;等

org.apache.hadoop和com.cloudera.sqoop两个包会找不到，报错，
sqoop参数设定了"--hadoop-mapred-home", "/opt/module/hadoop-3.1.3"后，就能找到hadoop包了
将sqoop-1.4.6 jar包复制改名为hadoop-common-sqoop-1.4.6.jar，
并放入linux中hadoop目录/share/hadoop/common/下，就能找到sqoop包了
至此jar包能够正常运行
-------------
hive-jdbc和hadoop jar包有冲突
得把hive-jdbc去掉一些jar包，服务才能起起来
<!-- hive -->
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-jdbc</artifactId>
    <version>2.3.0</version>
    <exclusions>
        <exclusion>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-slf4j-impl</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.eclipse.jetty.orbit</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.eclipse.jetty.aggregate</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
            <groupId>tomcat</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>*</artifactId>
        </exclusion>
    </exclusions>
<!--     未生效       <exclusions>-->
<!--                <exclusion>-->
<!--                    <groupId>*</groupId>-->
<!--                    <artifactId>*</artifactId>-->
<!--                </exclusion>-->
<!--            </exclusions>-->
</dependency>
-------------
hadoop、hive是离线数据分析，不是实时的，固定周期从日志文件或者mysql同步数据到hdfs，不适合做实时在线交易系统，离线数据分析是它的特长
hdfs是半只读，只能插入追加数据,不能修改数据
-------------
数据仓库分层

数仓分层意义：
1.复杂任务分解成多层，每一层只处理简单任务，方便定位问题
2.分层减少重复计算，增加一次计算的复用性
3.使真实数据和统计数据解耦开

数据集市是微型数据仓库，更少的数据，更少的主题
数据仓库为企业服务
数据集市为部门服务

表字段类型：
数量类型bigint
金额类型decimal(16,2) 16位有效数字，小数部分2位
字符串string
主键外键string
时间戳bigint

数据仓库分层：
ods(operation data store) 原始数据层（业务数据、日志数据）

ods => 清洗（去除空值，脏数据，超过极限范围的数据），脱敏（用户手机号、身份证号） => dwd
dwd(data warehouse detail) 明细数据层（保存业务明细，每一个业务操作，一次下单，一次支付，一次添加购物车，通过埋点日志形式记录）

dim数据维度层，对业务事实的描述信息，何人、何时、何地

dwd => 按天进行轻度汇总每一个业务操作，一行信息代表一个主题对象一天的汇总行为，如：一个用户一天下单次数 => dws
dws(data warehouse summary) 数据汇总层

dws => 对数据进行累计汇总，一行信息代表一个主题对象的累计行为，如：一个用户从最近7天、最近30天、注册到如今一共下了多少订单 => dwt
dwt(data warehouse topic) 数据主题层

ads(application data store) 数据应用层，为数仓后续后续应用提供结果，如：各种统计报表

举例：
金融大数据需求举例：
一.推荐客户合适的理财产品
1.原始数据层：保留原始数据
用户基本原始表，用户资产原始表，用户信用信息原始表，用户理财投资原始表，用户账单明细原始表等

2.数据维度层：谁，何处，何时（如银行用户）
用户维度表（查自用户基本原始表，关联用户资产原始表、用户信用信息原始表）

3.数据明细层：定义单次行为（如一次投资理财行为）
投资理财明细表（查自用户理财投资原始表）

4.数据汇总层：某一维度下一天的所有明细行为（用户一天的所有行为）
每日用户行为表（查自投资理财明细表）

5.数据主题层：某一维度下多天的所有明细行为（用户从最近7天、最近30天、注册到如今的所有行为）
用户主题宽表（查自用户维度表，每日用户行为表）

6.具体需求层：具体需求（推荐客户合适的理财产品，查询用户投资理财情况）
用户理财投资统计表（查自用户主题宽表）


一般都是建外部表，再指定数据加载路径

数据仓库建模：
ods同mysql表

dim、dwd维度模型
选择业务过程，声明粒度，确认维度，确认事实
1.选择业务过程：
挑选我们的业务线，如下单业务、支付业务、退款业务、物流业务，一条业务线对应一张事实表

2.声明粒度
尽量选择最小粒度

3.确认维度
谁，何处，何时

4.确认事实
确认度量值（次数、个数、件数、金额，可以进行累加），例如订单金额、下单次数等

业务总线矩阵：
事实表\维度表	时间	用户	地区	商品	优惠券	活动	度量值
订单					✓		✓		✓									运费/优惠金额/原始金额/最终金额
订单详情			✓		✓		✓		✓		✓			✓		件数/优惠金额/原始金额/最终金额
支付					✓		✓		✓									支付金额
加购					✓		✓				✓							件数/金额
收藏					✓		✓				✓							次数
评价					✓		✓				✓							次数
退单					✓		✓		✓		✓							件数/金额
退款					✓		✓		✓		✓							件数/金额
优惠券领用		✓		✓						✓					次数

dws，dwt宽表层

例子需求：
统计每省份订单个数，每省份订单总金额
处理办法：两表join和group by，同样数据重复计算了两次
为了避免重复
设计一张地区宽表，主键为地区id，字段为下单次数，下单金额，支付次数，支付金额等

ads层根据后续需求决定

具体表字段：
ods层：
ods日志表：
ods_log:
line string //一条日志
dt string //按时间创建分区，一天一个分区

活动表，一级二级三级品类表，购物车表，评论表，品牌表，优惠券表，收藏表，支付表，退款表，商品平台属性表，商品表，用户表

dim维度表，dwd事实表

商品维度表（每日全量商品数据）
dim_sku_info(
id string 商品id
price decimal(19,2) 商品价格
sku_name string 商品名称
sku_desc string 商品描述
weight decimal(19,2) 重量
is_sale Boolean 是否在售
spu_id string spu编号
spu_name string spu名称
category3_id string 三级分类id
category3_name string 三级分类名称
category2_id string 二级分类id
category2_name string 二级分类名称
category1_id string 一级分类id
category1_name string 一级分类名称
tm_id string 品牌id
tm_name string 品牌名称
sku_attr_values array<struct<
	attr_id:string,
	value_id:string,
	attr_name:string,
	valure_name:string>> 平台属性
sale_attr_values array<struct<
	sale_attr_id:string,
	sale_attr_value_id:string,
	sale_attr_name:string,
	sale_attr_valure_name:string>> 销售属性	
create_time 创建时间

封装struct结构体
select 
named_struct('attr_id',attr_id,'value_id',value_id)
from ods_sku_attr_value
where dt='2020-06-14'
结果{"attr_id":"111","value_id":"11"}

结构体数组
select 
collect_set(named_struct('attr_id',attr_id,'value_id',value_id))
from ods_sku_attr_value
where dt='2020-06-14'
group by sku_id
结果[{"attr_id":"111","value_id":"11"},{"attr_id":"111","value_id":"11"}]

优惠券维度表（每日全量）
dim_coupon_info(
id string 购物券编号
coupon_name string 购物券名称
coupon_type string 购物券类型1现金券 2折扣券 3满减券 4 满减打折券
condition_amount decimal(16,2) 满额数
condition_num bigint 满减数
activity_id string 活动编号
`benefit_amount` DECIMAL(16,2) COMMENT '减金额',
`benefit_discount` DECIMAL(16,2) COMMENT '折扣',
`create_time` STRING COMMENT '创建时间',
`range_type` STRING COMMENT '范围类型 1、商品 2、品类 3、品牌',
`limit_num` BIGINT COMMENT '最多领取次数',
`taken_count` BIGINT COMMENT '已领取次数',
`start_time` STRING COMMENT '可以领取的开始日期',
`end_time` STRING COMMENT '可以领取的结束日期',
`operate_time` STRING COMMENT '修改时间',
`expire_time` STRING COMMENT '过期时间'

活动信息维度表，一个活动规则（每日全量）
dim_activity_rule_info(
`activity_rule_id` STRING COMMENT '活动规则ID',
`activity_id` STRING COMMENT '活动ID',
`activity_name` STRING  COMMENT '活动名称',
`activity_type` STRING  COMMENT '活动类型',
`start_time` STRING  COMMENT '开始时间',
`end_time` STRING  COMMENT '结束时间',
`create_time` STRING  COMMENT '创建时间',
`condition_amount` DECIMAL(16,2) COMMENT '满减金额',
`condition_num` BIGINT COMMENT '满减件数',
`benefit_amount` DECIMAL(16,2) COMMENT '优惠金额',
`benefit_discount` DECIMAL(16,2) COMMENT '优惠折扣',
`benefit_level` STRING COMMENT '优惠级别'

地区维度表（数据不太会变的）
dim_base_province(
`id` STRING COMMENT 'id',
`province_name` STRING COMMENT '省市名称',
`area_code` STRING COMMENT '地区编码',
`iso_code` STRING COMMENT 'ISO-3166编码，供可视化使用',
`iso_3166_2` STRING COMMENT 'IOS-3166-2编码，供可视化使用',
`region_id` STRING COMMENT '地区id',
`region_name` STRING COMMENT '地区名称'

时间维度表，每行数据是一个日期，数据不来源于业务，手动导入，无需每日导入，一次可导入一年数据（数据不太会变的）
dim_date_info(
`date_id` STRING COMMENT '日',
`week_id` STRING COMMENT '周ID',
`week_day` STRING COMMENT '周几',
`day` STRING COMMENT '每月的第几天',
`month` STRING COMMENT '第几月',
`quarter` STRING COMMENT '第几季度',
`year` STRING COMMENT '年',
`is_workday` STRING COMMENT '是否是工作日',
`holiday_id` STRING COMMENT '节假日'

创建时间临时表，该表不用lzo压缩，时间文本直接load入，再insert 时间表 select 临时表

用户维度表（拉链表，数据变得很缓慢，第一天全量，后续新增及变化）
dim_user_info(
`id` STRING COMMENT '用户id',
`login_name` STRING COMMENT '用户名称',
`nick_name` STRING COMMENT '用户昵称',
`name` STRING COMMENT '用户姓名',
`phone_num` STRING COMMENT '手机号码',
`email` STRING COMMENT '邮箱',
`user_level` STRING COMMENT '用户等级',
`birthday` STRING COMMENT '生日',
`gender` STRING COMMENT '性别',
`create_time` STRING COMMENT '创建时间',
`operate_time` STRING COMMENT '操作时间',
`start_date` STRING COMMENT '开始日期',
`end_date` STRING COMMENT '结束日期'

敏感数据脱敏操作
md5(name),md5(phone_num),md5(email)

dwd层，明细数据层
两类：用户行为日志、业务数据
get_json_object解析json字符串，line当前行数据
select get_json_object(jsonArray, "$[0].age")

启动日志表，一次启动
dwd_start_log(
`area_code` STRING COMMENT '地区编码',
`brand` STRING COMMENT '手机品牌',
`channel` STRING COMMENT '渠道',
`is_new` STRING COMMENT '是否首次启动',
`model` STRING COMMENT '手机型号',
`mid_id` STRING COMMENT '设备id',
`os` STRING COMMENT '操作系统',
`user_id` STRING COMMENT '会员id',
`version_code` STRING COMMENT 'app版本号',
`entry` STRING COMMENT 'icon手机图标 notice 通知 install 安装后启',
`loading_time` BIGINT COMMENT '启动加载时间',
`open_ad_id` STRING COMMENT '广告页ID ',
`open_ad_ms` BIGINT COMMENT '广告总共播放时间',
`open_ad_skip_ms` BIGINT COMMENT '用户跳过广告时点',
`ts` BIGINT COMMENT '时间'

页面日志表，一个页面访问记录
dwd_page_log(
`area_code` STRING COMMENT '地区编码',
`brand` STRING COMMENT '手机品牌',
`channel` STRING COMMENT '渠道',
`is_new` STRING COMMENT '是否首次启动',
`model` STRING COMMENT '手机型号',
`mid_id` STRING COMMENT '设备id',
`os` STRING COMMENT '操作系统',
`user_id` STRING COMMENT '会员id',
`version_code` STRING COMMENT 'app版本号',
`during_time` BIGINT COMMENT '持续时间毫秒',
`page_item` STRING COMMENT '目标id ',
`page_item_type` STRING COMMENT '目标类型',
`last_page_id` STRING COMMENT '上页类型',
`page_id` STRING COMMENT '页面ID ',
`source_type` STRING COMMENT '来源类型',
`ts` bigint

动作日志表，一个动作记录
dwd_action_log(
`area_code` STRING COMMENT '地区编码',
`brand` STRING COMMENT '手机品牌',
`channel` STRING COMMENT '渠道',
`is_new` STRING COMMENT '是否首次启动',
`model` STRING COMMENT '手机型号',
`mid_id` STRING COMMENT '设备id',
`os` STRING COMMENT '操作系统',
`user_id` STRING COMMENT '会员id',
`version_code` STRING COMMENT 'app版本号',
`during_time` BIGINT COMMENT '持续时间毫秒',
`page_item` STRING COMMENT '目标id ',
`page_item_type` STRING COMMENT '目标类型',
`last_page_id` STRING COMMENT '上页类型',
`page_id` STRING COMMENT '页面id ',
`source_type` STRING COMMENT '来源类型',
`action_id` STRING COMMENT '动作id',
`item` STRING COMMENT '目标id ',
`item_type` STRING COMMENT '目标类型',
`ts` BIGINT COMMENT '时间'

曝光日志表，一个曝光记录
dwd_display_log(
`area_code` STRING COMMENT '地区编码',
`brand` STRING COMMENT '手机品牌',
`channel` STRING COMMENT '渠道',
`is_new` STRING COMMENT '是否首次启动',
`model` STRING COMMENT '手机型号',
`mid_id` STRING COMMENT '设备id',
`os` STRING COMMENT '操作系统',
`user_id` STRING COMMENT '会员id',
`version_code` STRING COMMENT 'app版本号',
`during_time` BIGINT COMMENT 'app版本号',
`page_item` STRING COMMENT '目标id ',
`page_item_type` STRING COMMENT '目标类型',
`last_page_id` STRING COMMENT '上页类型',
`page_id` STRING COMMENT '页面ID ',
`source_type` STRING COMMENT '来源类型',
`ts` BIGINT COMMENT 'app版本号',
`display_type` STRING COMMENT '曝光类型',
`item` STRING COMMENT '曝光对象id ',
`item_type` STRING COMMENT 'app版本号',
`order` BIGINT COMMENT '曝光顺序',
`pos_id` BIGINT COMMENT '曝光位置'

一进多出explode炸裂操作

错误日志表，一个错误记录
dwd_error_log(
`area_code` STRING COMMENT '地区编码',
`brand` STRING COMMENT '手机品牌',
`channel` STRING COMMENT '渠道',
`is_new` STRING COMMENT '是否首次启动',
`model` STRING COMMENT '手机型号',
`mid_id` STRING COMMENT '设备id',
`os` STRING COMMENT '操作系统',
`user_id` STRING COMMENT '会员id',
`version_code` STRING COMMENT 'app版本号',
`page_item` STRING COMMENT '目标id ',
`page_item_type` STRING COMMENT '目标类型',
`last_page_id` STRING COMMENT '上页类型',
`page_id` STRING COMMENT '页面ID ',
`source_type` STRING COMMENT '来源类型',
`entry` STRING COMMENT ' icon手机图标  notice 通知 install 安装后启动',
`loading_time` STRING COMMENT '启动加载时间',
`open_ad_id` STRING COMMENT '广告页ID ',
`open_ad_ms` STRING COMMENT '广告总共播放时间',
`open_ad_skip_ms` STRING COMMENT '用户跳过广告时点',
`actions` STRING COMMENT '动作',
`displays` STRING COMMENT '曝光',
`ts` STRING COMMENT '时间',
`error_code` STRING COMMENT '错误码',
`msg` STRING COMMENT '错误信息'    

dwd业务表
1.事务型事实表
不会发生变化的业务，数据写入就不会变化了，以每个事务或者事件为单位，比如一个销售订单记录，一笔支付记录，后续增量同步
2.周期型快照事实表
不会保留所有数据，只保留固定时间间隔的数据，比如每月的销售额，每月账户余额，全量同步
3.累计型快照事实表
适用于跟踪业务事实的变化，比如订单的进展情况，新增及变化同步

事实表字段分为两类：
1.维度外键
2.度量值

评价事实表，（不可变，事务型事实表），一次评价记录
dwd_comment_info(
`id` STRING COMMENT '编号',
`user_id` STRING COMMENT '用户ID',
`sku_id` STRING COMMENT '商品sku',
`spu_id` STRING COMMENT '商品spu',
`order_id` STRING COMMENT '订单ID',
`appraise` STRING COMMENT '评价(好评、中评、差评、默认评价)',
`create_time` STRING COMMENT '评价时间'

度量值（隐含，一条数据代表一次）
appraise维度退化，字段很少，就不必单独创建维度表了，直接写入事实表中

订单明细事实表，一条订单的一个商品项（事务型事实表）
dwd_order_detail(
`id` STRING COMMENT '订单编号',
`order_id` STRING COMMENT '订单号',
`user_id` STRING COMMENT '用户id',
`sku_id` STRING COMMENT 'sku商品id',
`province_id` STRING COMMENT '省份ID',
`activity_id` STRING COMMENT '活动ID',
`activity_rule_id` STRING COMMENT '活动规则ID',
`coupon_id` STRING COMMENT '优惠券ID',
`create_time` STRING COMMENT '创建时间',
`source_type` STRING COMMENT '来源类型',
`source_id` STRING COMMENT '来源编号',
`sku_num` BIGINT COMMENT '商品数量',
`original_amount` DECIMAL(16,2) COMMENT '原始价格',
`split_activity_amount` DECIMAL(16,2) COMMENT '活动优惠分摊',
`split_coupon_amount` DECIMAL(16,2) COMMENT '优惠券优惠分摊',
`split_final_amount` DECIMAL(16,2) COMMENT '最终价格分摊'

维度退化create_time，source_type

退单事实表（事务型事实表）
dwd_order_refund_info(
`id` STRING COMMENT '编号',
`user_id` STRING COMMENT '用户ID',
`order_id` STRING COMMENT '订单ID',
`sku_id` STRING COMMENT '商品ID',
`province_id` STRING COMMENT '地区ID',
`refund_type` STRING COMMENT '退单类型',
`refund_num` BIGINT COMMENT '退单件数',
`refund_amount` DECIMAL(16,2) COMMENT '退单金额',
`refund_reason_type` STRING COMMENT '退单原因类型',
`create_time` STRING COMMENT '退单时间'

加购事实表（周期型快照事实表，每日快照）
dwd_cart_info(
`id` STRING COMMENT '编号',
`user_id` STRING COMMENT '用户ID',
`sku_id` STRING COMMENT '商品ID',
`source_type` STRING COMMENT '来源类型',
`source_id` STRING COMMENT '来源编号',
`cart_price` DECIMAL(16,2) COMMENT '加入购物车时的价格',
`is_ordered` STRING COMMENT '是否已下单',
`create_time` STRING COMMENT '创建时间',
`operate_time` STRING COMMENT '修改时间',
`order_time` STRING COMMENT '下单时间',
`sku_num` BIGINT COMMENT '加购数量'

cart_price * sku_num 为度量值

收藏事实表（周期型快照事实表，每日快照）
dwd_favor_info(
`id` STRING COMMENT '编号',
`user_id` STRING  COMMENT '用户id',
`sku_id` STRING  COMMENT 'skuid',
`spu_id` STRING  COMMENT 'spuid',
`is_cancel` STRING  COMMENT '是否取消',
`create_time` STRING  COMMENT '收藏时间',
`cancel_time` STRING  COMMENT '取消时间'

优惠券领用事实表（累积型快照事实表）
dwd_coupon_use(
`id` STRING COMMENT '编号',
`coupon_id` STRING  COMMENT '优惠券ID',
`user_id` STRING  COMMENT 'userid',
`order_id` STRING  COMMENT '订单id',
`coupon_status` STRING  COMMENT '优惠券状态',
`get_time` STRING  COMMENT '领取时间',
`using_time` STRING  COMMENT '使用时间(下单)',
`used_time` STRING  COMMENT '使用时间(支付)',
`expire_time` STRING COMMENT '过期时间'

支付事实表（累积型快照事实表）
dwd_payment_info(
`id` STRING COMMENT '编号',
`order_id` STRING COMMENT '订单编号',
`user_id` STRING COMMENT '用户编号',
`province_id` STRING COMMENT '地区ID',
`trade_no` STRING COMMENT '交易编号',
`out_trade_no` STRING COMMENT '对外交易编号',
`payment_type` STRING COMMENT '支付类型',
`payment_amount` DECIMAL(16,2) COMMENT '支付金额',
`payment_status` STRING COMMENT '支付状态',
`create_time` STRING COMMENT '创建时间',--调用第三方支付接口的时间
`callback_time` STRING COMMENT '完成时间'--支付完成时间，即支付成功回调时间

退款事实表（累积型快照事实表）
dwd_refund_payment(
`id` STRING COMMENT '编号',
`user_id` STRING COMMENT '用户ID',
`order_id` STRING COMMENT '订单编号',
`sku_id` STRING COMMENT 'SKU编号',
`province_id` STRING COMMENT '地区ID',
`trade_no` STRING COMMENT '交易编号',
`out_trade_no` STRING COMMENT '对外交易编号',
`payment_type` STRING COMMENT '支付类型',
`refund_amount` DECIMAL(16,2) COMMENT '退款金额',
`refund_status` STRING COMMENT '退款状态',
`create_time` STRING COMMENT '创建时间',--调用第三方支付接口的时间
`callback_time` STRING COMMENT '回调时间'--支付接口回调时间，即支付成功时间

订单事实表（累积型快照事实表）
dwd_order_info(
`id` STRING COMMENT '编号',
`order_status` STRING COMMENT '订单状态',
`user_id` STRING COMMENT '用户ID',
`province_id` STRING COMMENT '地区ID',
`payment_way` STRING COMMENT '支付方式',
`delivery_address` STRING COMMENT '邮寄地址',
`out_trade_no` STRING COMMENT '对外交易编号',
`tracking_no` STRING COMMENT '物流单号',
`create_time` STRING COMMENT '创建时间(未支付状态)',
`payment_time` STRING COMMENT '支付时间(已支付状态)',
`cancel_time` STRING COMMENT '取消时间(已取消状态)',
`finish_time` STRING COMMENT '完成时间(已完成状态)',
`refund_time` STRING COMMENT '退款时间(退款中状态)',
`refund_finish_time` STRING COMMENT '退款完成时间(退款完成状态)',
`expire_time` STRING COMMENT '过期时间',
`feight_fee` DECIMAL(16,2) COMMENT '运费',
`feight_fee_reduce` DECIMAL(16,2) COMMENT '运费减免',
`activity_reduce_amount` DECIMAL(16,2) COMMENT '活动减免',
`coupon_reduce_amount` DECIMAL(16,2) COMMENT '优惠券减免',
`original_amount` DECIMAL(16,2) COMMENT '订单原始价格',
`final_amount` DECIMAL(16,2) COMMENT '订单最终价格'

dws层（宽表），主键为维度id，其余字段为与该维度相关的事实表的度量值的聚合值，聚合到一天
意义：
避免重复计算

与dim维度表对应

nvl(p1,p2)

hive复杂数据类型:
map,array,struct

访客主题表
dws_visitor_action_daycount(
`mid_id` STRING COMMENT '设备id',
`brand` STRING COMMENT '设备品牌',
`model` STRING COMMENT '设备型号',
`is_new` STRING COMMENT '是否首次访问',
`channel` ARRAY<STRING> COMMENT '渠道',
`os` ARRAY<STRING> COMMENT '操作系统',
`area_code` ARRAY<STRING> COMMENT '地区ID',
`version_code` ARRAY<STRING> COMMENT '应用版本',
`visit_count` BIGINT COMMENT '访问次数',
`page_stats` ARRAY<STRUCT<page_id:STRING,page_count:BIGINT,during_time:BIGINT>> COMMENT '页面访问统计'

dim维度表
dwd事实表
dws汇总表，维度表（用户表dim） * 事实表
主键用户表dim的id，其余字段为，与该维度相关的事实表的度量值的聚合值，聚合到天

用户主题表，一个用户在某天的汇总行为
dws_user_action_daycount(
`user_id` STRING COMMENT '用户id',
`login_count` BIGINT COMMENT '登录次数',
`cart_count` BIGINT COMMENT '加入购物车次数',
`favor_count` BIGINT COMMENT '收藏次数',
`order_count` BIGINT COMMENT '下单次数',
`order_activity_count` BIGINT COMMENT '订单参与活动次数',
`order_activity_reduce_amount` DECIMAL(16,2) COMMENT '订单(活动)',
`order_coupon_count` BIGINT COMMENT '订单用券次数',
`order_coupon_reduce_amount` DECIMAL(16,2) COMMENT '订单减免惠券)',
`order_original_amount` DECIMAL(16,2)  COMMENT '订单单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '订单总金额',
`payment_count` BIGINT COMMENT '支付次数',
`payment_amount` DECIMAL(16,2) COMMENT '支付金额',
`refund_order_count` BIGINT COMMENT '退单次数',
`refund_order_num` BIGINT COMMENT '退单件数',
`refund_order_amount` DECIMAL(16,2) COMMENT '退单金额',
`refund_payment_count` BIGINT COMMENT '退款次数',
`refund_payment_num` BIGINT COMMENT '退款件数',
`refund_payment_amount` DECIMAL(16,2) COMMENT '退款金额',
`coupon_get_count` BIGINT COMMENT '优惠券领取次数',
`coupon_using_count` BIGINT COMMENT '优惠券使用(下单)次数',
`coupon_used_count` BIGINT COMMENT '优惠券使用(支付)次数',
`appraise_good_count` BIGINT COMMENT '好评数',
`appraise_mid_count` BIGINT COMMENT '中评数',
`appraise_bad_count` BIGINT COMMENT '差评数',
`appraise_default_count` BIGINT COMMENT '默认评价数',
`order_detail_stats` array<struct<sku_id:string,sku_num:bigint,order_count:bigint,activity_reduce_amount:decimal(16,2),coupon_reduce_amount:decimal(16,2),original_amount:decimal(16,2),final_amount:decimal(16,2)>> COMMENT '下单明细统计'

按天分区，当天活跃用户的汇总行为

当天分区和9999分区，完成和未完成分区

商品主题表
dws_sku_action_daycount(
`sku_id` STRING COMMENT 'sku_id',
`order_count` BIGINT COMMENT '被下单次数',
`order_num` BIGINT COMMENT '被下单件数',
`order_activity_count` BIGINT COMMENT '参与活动被下单次数',
`order_coupon_count` BIGINT COMMENT '使用优惠券被下单次数',
`order_activity_reduce_amount` DECIMAL(16,2) COMMENT '优惠金额(活动)',
`order_coupon_reduce_amount` DECIMAL(16,2) COMMENT '优惠金额(优惠券)',
`order_original_amount` DECIMAL(16,2) COMMENT '被下单原价金额',
`order_final_amount` DECIMAL(16,2) COMMENT '被下单最终金额',
`payment_count` BIGINT COMMENT '被支付次数',
`payment_num` BIGINT COMMENT '被支付件数',
`payment_amount` DECIMAL(16,2) COMMENT '被支付金额',
`refund_order_count` BIGINT  COMMENT '被退单次数',
`refund_order_num` BIGINT COMMENT '被退单件数',
`refund_order_amount` DECIMAL(16,2) COMMENT '被退单金额',
`refund_payment_count` BIGINT  COMMENT '被退款次数',
`refund_payment_num` BIGINT COMMENT '被退款件数',
`refund_payment_amount` DECIMAL(16,2) COMMENT '被退款金额',
`cart_count` BIGINT COMMENT '被加入购物车次数',
`favor_count` BIGINT COMMENT '被收藏次数',
`appraise_good_count` BIGINT COMMENT '好评数',
`appraise_mid_count` BIGINT COMMENT '中评数',
`appraise_bad_count` BIGINT COMMENT '差评数',
`appraise_default_count` BIGINT COMMENT '默认评价数'

优惠券主题
dws_coupon_info_daycount(
`coupon_id` STRING COMMENT '优惠券ID',
`get_count` BIGINT COMMENT '被领取次数',
`order_count` BIGINT COMMENT '被使用(下单)次数', 
`order_reduce_amount` DECIMAL(16,2) COMMENT '用券下单优惠金额',
`order_original_amount` DECIMAL(16,2) COMMENT '用券订单原价金额',
`order_final_amount` DECIMAL(16,2) COMMENT '用券下单最终金额',
`payment_count` BIGINT COMMENT '被使用(支付)次数',
`payment_reduce_amount` DECIMAL(16,2) COMMENT '用券支付优惠金额',
`payment_amount` DECIMAL(16,2) COMMENT '用券支付总金额',
`expire_count` BIGINT COMMENT '过期次数'

活动主题
dws_activity_info_daycount(
`activity_rule_id` STRING COMMENT '活动规则ID',
`activity_id` STRING COMMENT '活动ID',
`order_count` BIGINT COMMENT '参与某活动某规则下单次数',    
`order_reduce_amount` DECIMAL(16,2) COMMENT '参与某活动某规则下单减免金额',
`order_original_amount` DECIMAL(16,2) COMMENT '参与某活动某规则下单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '参与某活动某规则下单最终金额',
`payment_count` BIGINT COMMENT '参与某活动某规则支付次数',
`payment_reduce_amount` DECIMAL(16,2) COMMENT '参与某活动某规则支付减免金额',
`payment_amount` DECIMAL(16,2) COMMENT '参与某活动某规则支付金额'

地区主题
dws_area_stats_daycount(
`province_id` STRING COMMENT '地区编号',
`visit_count` BIGINT COMMENT '访问次数',
`login_count` BIGINT COMMENT '登录次数',
`visitor_count` BIGINT COMMENT '访客人数',
`user_count` BIGINT COMMENT '用户人数',
`order_count` BIGINT COMMENT '下单次数',
`order_original_amount` DECIMAL(16,2) COMMENT '下单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '下单最终金额',
`payment_count` BIGINT COMMENT '支付次数',
`payment_amount` DECIMAL(16,2) COMMENT '支付金额',
`refund_order_count` BIGINT COMMENT '退单次数',
`refund_order_amount` DECIMAL(16,2) COMMENT '退单金额',
`refund_payment_count` BIGINT COMMENT '退款次数',
`refund_payment_amount` DECIMAL(16,2) COMMENT '退款金额'

首日装载脚本，每日装载脚本

dwt层
也是以维度为基准，每张表对应一个维度
dws一个维度对象，在一天中的汇总行为
dwt一个维度对象，的累计汇总行为

dwt列：
主键用户表dim的id，其余字段为，与该维度相关的事实表的度量值的聚合值，聚合到多天

dws分区保存的：当日活跃对象的汇总行为
dwt分区保存的：截止到当日的全量的维度对象的累计汇总行为（包括最近1天，7天，30天，总共累计）

用户主题
dwt_user_topic(
`user_id` STRING  COMMENT '用户id',
`login_date_first` STRING COMMENT '首次活跃日期',
`login_date_last` STRING COMMENT '末次活跃日期',
`login_date_1d_count` STRING COMMENT '最近1日登录次数',
`login_last_1d_day_count` BIGINT COMMENT '最近1日登录天数',
`login_last_7d_count` BIGINT COMMENT '最近7日登录次数',
`login_last_7d_day_count` BIGINT COMMENT '最近7日登录天数',
`login_last_30d_count` BIGINT COMMENT '最近30日登录次数',
`login_last_30d_day_count` BIGINT COMMENT '最近30日登录天数',
`login_count` BIGINT COMMENT '累积登录次数',
`login_day_count` BIGINT COMMENT '累积登录天数',
`order_date_first` STRING COMMENT '首次下单时间',
`order_date_last` STRING COMMENT '末次下单时间',
`order_last_1d_count` BIGINT COMMENT '最近1日下单次数',
`order_activity_last_1d_count` BIGINT COMMENT '最近1日订单参与活动次数',
`order_activity_reduce_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日订单减免金额(活动)',
`order_coupon_last_1d_count` BIGINT COMMENT '最近1日下单用券次数',
`order_coupon_reduce_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日订单减免金额(优惠券)',
`order_last_1d_original_amount` DECIMAL(16,2) COMMENT '最近1日原始下单金额',
`order_last_1d_final_amount` DECIMAL(16,2) COMMENT '最近1日最终下单金额',
`order_last_7d_count` BIGINT COMMENT '最近7日下单次数',
`order_activity_last_7d_count` BIGINT COMMENT '最近7日订单参与活动次数',
`order_activity_reduce_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日订单减免金额(活动)',
`order_coupon_last_7d_count` BIGINT COMMENT '最近7日下单用券次数',
`order_coupon_reduce_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日订单减免金额(优惠券)',
`order_last_7d_original_amount` DECIMAL(16,2) COMMENT '最近7日原始下单金额',
`order_last_7d_final_amount` DECIMAL(16,2) COMMENT '最近7日最终下单金额',
`order_last_30d_count` BIGINT COMMENT '最近30日下单次数',
`order_activity_last_30d_count` BIGINT COMMENT '最近30日订单参与活动次数',
`order_activity_reduce_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日订单减免金额(活动)',
`order_coupon_last_30d_count` BIGINT COMMENT '最近30日下单用券次数',
`order_coupon_reduce_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日订单减免金额(优惠券)',
`order_last_30d_original_amount` DECIMAL(16,2) COMMENT '最近30日原始下单金额',
`order_last_30d_final_amount` DECIMAL(16,2) COMMENT '最近30日最终下单金额',
`order_count` BIGINT COMMENT '累积下单次数',
`order_activity_count` BIGINT COMMENT '累积订单参与活动次数',
`order_activity_reduce_amount` DECIMAL(16,2) COMMENT '累积订单减免金额(活动)',
`order_coupon_count` BIGINT COMMENT '累积下单用券次数',
`order_coupon_reduce_amount` DECIMAL(16,2) COMMENT '累积订单减免金额(优惠券)',
`order_original_amount` DECIMAL(16,2) COMMENT '累积原始下单金额',
`order_final_amount` DECIMAL(16,2) COMMENT '累积最终下单金额',
`payment_date_first` STRING COMMENT '首次支付时间',
`payment_date_last` STRING COMMENT '末次支付时间',
`payment_last_1d_count` BIGINT COMMENT '最近1日支付次数',
`payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日支付金额',
`payment_last_7d_count` BIGINT COMMENT '最近7日支付次数',
`payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日支付金额',
`payment_last_30d_count` BIGINT COMMENT '最近30日支付次数',
`payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日支付金额',
`payment_count` BIGINT COMMENT '累积支付次数',
`payment_amount` DECIMAL(16,2) COMMENT '累积支付金额',
`refund_order_last_1d_count` BIGINT COMMENT '最近1日退单次数',
`refund_order_last_1d_num` BIGINT COMMENT '最近1日退单件数',
`refund_order_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日退单金额',
`refund_order_last_7d_count` BIGINT COMMENT '最近7日退单次数',
`refund_order_last_7d_num` BIGINT COMMENT '最近7日退单件数',
`refund_order_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日退单金额',
`refund_order_last_30d_count` BIGINT COMMENT '最近30日退单次数',
`refund_order_last_30d_num` BIGINT COMMENT '最近30日退单件数',
`refund_order_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日退单金额',
`refund_order_count` BIGINT COMMENT '累积退单次数',
`refund_order_num` BIGINT COMMENT '累积退单件数',
`refund_order_amount` DECIMAL(16,2) COMMENT '累积退单金额',
`refund_payment_last_1d_count` BIGINT COMMENT '最近1日退款次数',
`refund_payment_last_1d_num` BIGINT COMMENT '最近1日退款件数',
`refund_payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日退款金额',
`refund_payment_last_7d_count` BIGINT COMMENT '最近7日退款次数',
`refund_payment_last_7d_num` BIGINT COMMENT '最近7日退款件数',
`refund_payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日退款金额',
`refund_payment_last_30d_count` BIGINT COMMENT '最近30日退款次数',
`refund_payment_last_30d_num` BIGINT COMMENT '最近30日退款件数',
`refund_payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日退款金额',
`refund_payment_count` BIGINT COMMENT '累积退款次数',
`refund_payment_num` BIGINT COMMENT '累积退款件数',
`refund_payment_amount` DECIMAL(16,2) COMMENT '累积退款金额',
`cart_last_1d_count` BIGINT COMMENT '最近1日加入购物车次数',
`cart_last_7d_count` BIGINT COMMENT '最近7日加入购物车次数',
`cart_last_30d_count` BIGINT COMMENT '最近30日加入购物车次数',
`cart_count` BIGINT COMMENT '累积加入购物车次数',
`favor_last_1d_count` BIGINT COMMENT '最近1日收藏次数',
`favor_last_7d_count` BIGINT COMMENT '最近7日收藏次数',
`favor_last_30d_count` BIGINT COMMENT '最近30日收藏次数',
`favor_count` BIGINT COMMENT '累积收藏次数',
`coupon_last_1d_get_count` BIGINT COMMENT '最近1日领券次数',
`coupon_last_1d_using_count` BIGINT COMMENT '最近1日用券(下单)次数',
`coupon_last_1d_used_count` BIGINT COMMENT '最近1日用券(支付)次数',
`coupon_last_7d_get_count` BIGINT COMMENT '最近7日领券次数',
`coupon_last_7d_using_count` BIGINT COMMENT '最近7日用券(下单)次数',
`coupon_last_7d_used_count` BIGINT COMMENT '最近7日用券(支付)次数',
`coupon_last_30d_get_count` BIGINT COMMENT '最近30日领券次数',
`coupon_last_30d_using_count` BIGINT COMMENT '最近30日用券(下单)次数',
`coupon_last_30d_used_count` BIGINT COMMENT '最近30日用券(支付)次数',
`coupon_get_count` BIGINT COMMENT '累积领券次数',
`coupon_using_count` BIGINT COMMENT '累积用券(下单)次数',
`coupon_used_count` BIGINT COMMENT '累积用券(支付)次数',
`appraise_last_1d_good_count` BIGINT COMMENT '最近1日好评次数',
`appraise_last_1d_mid_count` BIGINT COMMENT '最近1日中评次数',
`appraise_last_1d_bad_count` BIGINT COMMENT '最近1日差评次数',
`appraise_last_1d_default_count` BIGINT COMMENT '最近1日默认评价次数',
`appraise_last_7d_good_count` BIGINT COMMENT '最近7日好评次数',
`appraise_last_7d_mid_count` BIGINT COMMENT '最近7日中评次数',
`appraise_last_7d_bad_count` BIGINT COMMENT '最近7日差评次数',
`appraise_last_7d_default_count` BIGINT COMMENT '最近7日默认评价次数',
`appraise_last_30d_good_count` BIGINT COMMENT '最近30日好评次数',
`appraise_last_30d_mid_count` BIGINT COMMENT '最近30日中评次数',
`appraise_last_30d_bad_count` BIGINT COMMENT '最近30日差评次数',
`appraise_last_30d_default_count` BIGINT COMMENT '最近30日默认评价次数',
`appraise_good_count` BIGINT COMMENT '累积好评次数',
`appraise_mid_count` BIGINT COMMENT '累积中评次数',
`appraise_bad_count` BIGINT COMMENT '累积差评次数',
`appraise_default_count` BIGINT COMMENT '累积默认评价次数'

访客主题
dwt_visitor_topic(
`mid_id` STRING COMMENT '设备id',
`brand` STRING COMMENT '手机品牌',
`model` STRING COMMENT '手机型号',
`channel` ARRAY<STRING> COMMENT '渠道',
`os` ARRAY<STRING> COMMENT '操作系统',
`area_code` ARRAY<STRING> COMMENT '地区ID',
`version_code` ARRAY<STRING> COMMENT '应用版本',
`visit_date_first` STRING  COMMENT '首次访问时间',
`visit_date_last` STRING  COMMENT '末次访问时间',
`visit_last_1d_count` BIGINT COMMENT '最近1日访问次数',
`visit_last_1d_day_count` BIGINT COMMENT '最近1日访问天数',
`visit_last_7d_count` BIGINT COMMENT '最近7日访问次数',
`visit_last_7d_day_count` BIGINT COMMENT '最近7日访问天数',
`visit_last_30d_count` BIGINT COMMENT '最近30日访问次数',
`visit_last_30d_day_count` BIGINT COMMENT '最近30日访问天数',
`visit_count` BIGINT COMMENT '累积访问次数',
`visit_day_count` BIGINT COMMENT '累积访问天数'

商品主题
dwt_sku_topic(
`sku_id` STRING COMMENT 'sku_id',
`order_last_1d_count` BIGINT COMMENT '最近1日被下单次数',
`order_last_1d_num` BIGINT COMMENT '最近1日被下单件数',
`order_activity_last_1d_count` BIGINT COMMENT '最近1日参与活动被下单次数',
`order_coupon_last_1d_count` BIGINT COMMENT '最近1日使用优惠券被下单次数',
`order_activity_reduce_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日优惠金额(活动)',
`order_coupon_reduce_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日优惠金额(优惠券)',
`order_last_1d_original_amount` DECIMAL(16,2) COMMENT '最近1日被下单原始金额',
`order_last_1d_final_amount` DECIMAL(16,2) COMMENT '最近1日被下单最终金额',
`order_last_7d_count` BIGINT COMMENT '最近7日被下单次数',
`order_last_7d_num` BIGINT COMMENT '最近7日被下单件数',
`order_activity_last_7d_count` BIGINT COMMENT '最近7日参与活动被下单次数',
`order_coupon_last_7d_count` BIGINT COMMENT '最近7日使用优惠券被下单次数',
`order_activity_reduce_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日优惠金额(活动)',
`order_coupon_reduce_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日优惠金额(优惠券)',
`order_last_7d_original_amount` DECIMAL(16,2) COMMENT '最近7日被下单原始金额',
`order_last_7d_final_amount` DECIMAL(16,2) COMMENT '最近7日被下单最终金额',
`order_last_30d_count` BIGINT COMMENT '最近30日被下单次数',
`order_last_30d_num` BIGINT COMMENT '最近30日被下单件数',
`order_activity_last_30d_count` BIGINT COMMENT '最近30日参与活动被下单次数',
`order_coupon_last_30d_count` BIGINT COMMENT '最近30日使用优惠券被下单次数',
`order_activity_reduce_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日优惠金额(活动)',
`order_coupon_reduce_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日优惠金额(优惠券)',
`order_last_30d_original_amount` DECIMAL(16,2) COMMENT '最近30日被下单原始金额',
`order_last_30d_final_amount` DECIMAL(16,2) COMMENT '最近30日被下单最终金额',
`order_count` BIGINT COMMENT '累积被下单次数',
`order_num` BIGINT COMMENT '累积被下单件数',
`order_activity_count` BIGINT COMMENT '累积参与活动被下单次数',
`order_coupon_count` BIGINT COMMENT '累积使用优惠券被下单次数',
`order_activity_reduce_amount` DECIMAL(16,2) COMMENT '累积优惠金额(活动)',
`order_coupon_reduce_amount` DECIMAL(16,2) COMMENT '累积优惠金额(优惠券)',
`order_original_amount` DECIMAL(16,2) COMMENT '累积被下单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '累积被下单最终金额',
`payment_last_1d_count` BIGINT COMMENT '最近1日被支付次数',
`payment_last_1d_num` BIGINT COMMENT '最近1日被支付件数',
`payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日被支付金额',
`payment_last_7d_count` BIGINT COMMENT '最近7日被支付次数',
`payment_last_7d_num` BIGINT COMMENT '最近7日被支付件数',
`payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日被支付金额',
`payment_last_30d_count` BIGINT COMMENT '最近30日被支付次数',
`payment_last_30d_num` BIGINT COMMENT '最近30日被支付件数',
`payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日被支付金额',
`payment_count` BIGINT COMMENT '累积被支付次数',
`payment_num` BIGINT COMMENT '累积被支付件数',
`payment_amount` DECIMAL(16,2) COMMENT '累积被支付金额',
`refund_order_last_1d_count` BIGINT COMMENT '最近1日退单次数',
`refund_order_last_1d_num` BIGINT COMMENT '最近1日退单件数',
`refund_order_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日退单金额',
`refund_order_last_7d_count` BIGINT COMMENT '最近7日退单次数',
`refund_order_last_7d_num` BIGINT COMMENT '最近7日退单件数',
`refund_order_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日退单金额',
`refund_order_last_30d_count` BIGINT COMMENT '最近30日退单次数',
`refund_order_last_30d_num` BIGINT COMMENT '最近30日退单件数',
`refund_order_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日退单金额',
`refund_order_count` BIGINT COMMENT '累积退单次数',
`refund_order_num` BIGINT COMMENT '累积退单件数',
`refund_order_amount` DECIMAL(16,2) COMMENT '累积退单金额',
`refund_payment_last_1d_count` BIGINT COMMENT '最近1日退款次数',
`refund_payment_last_1d_num` BIGINT COMMENT '最近1日退款件数',
`refund_payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日退款金额',
`refund_payment_last_7d_count` BIGINT COMMENT '最近7日退款次数',
`refund_payment_last_7d_num` BIGINT COMMENT '最近7日退款件数',
`refund_payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日退款金额',
`refund_payment_last_30d_count` BIGINT COMMENT '最近30日退款次数',
`refund_payment_last_30d_num` BIGINT COMMENT '最近30日退款件数',
`refund_payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日退款金额',
`refund_payment_count` BIGINT COMMENT '累积退款次数',
`refund_payment_num` BIGINT COMMENT '累积退款件数',
`refund_payment_amount` DECIMAL(16,2) COMMENT '累积退款金额',
`cart_last_1d_count` BIGINT COMMENT '最近1日被加入购物车次数',
`cart_last_7d_count` BIGINT COMMENT '最近7日被加入购物车次数',
`cart_last_30d_count` BIGINT COMMENT '最近30日被加入购物车次数',
`cart_count` BIGINT COMMENT '累积被加入购物车次数',
`favor_last_1d_count` BIGINT COMMENT '最近1日被收藏次数',
`favor_last_7d_count` BIGINT COMMENT '最近7日被收藏次数',
`favor_last_30d_count` BIGINT COMMENT '最近30日被收藏次数',
`favor_count` BIGINT COMMENT '累积被收藏次数',
`appraise_last_1d_good_count` BIGINT COMMENT '最近1日好评数',
`appraise_last_1d_mid_count` BIGINT COMMENT '最近1日中评数',
`appraise_last_1d_bad_count` BIGINT COMMENT '最近1日差评数',
`appraise_last_1d_default_count` BIGINT COMMENT '最近1日默认评价数',
`appraise_last_7d_good_count` BIGINT COMMENT '最近7日好评数',
`appraise_last_7d_mid_count` BIGINT COMMENT '最近7日中评数',
`appraise_last_7d_bad_count` BIGINT COMMENT '最近7日差评数',
`appraise_last_7d_default_count` BIGINT COMMENT '最近7日默认评价数',
`appraise_last_30d_good_count` BIGINT COMMENT '最近30日好评数',
`appraise_last_30d_mid_count` BIGINT COMMENT '最近30日中评数',
`appraise_last_30d_bad_count` BIGINT COMMENT '最近30日差评数',
`appraise_last_30d_default_count` BIGINT COMMENT '最近30日默认评价数',
`appraise_good_count` BIGINT COMMENT '累积好评数',
`appraise_mid_count` BIGINT COMMENT '累积中评数',
`appraise_bad_count` BIGINT COMMENT '累积差评数',
`appraise_default_count` BIGINT COMMENT '累积默认评价数'

优惠券主题
dwt_coupon_topic(
`coupon_id` STRING COMMENT '优惠券ID',
`get_last_1d_count` BIGINT COMMENT '最近1日领取次数',
`get_last_7d_count` BIGINT COMMENT '最近7日领取次数',
`get_last_30d_count` BIGINT COMMENT '最近30日领取次数',
`get_count` BIGINT COMMENT '累积领取次数',
`order_last_1d_count` BIGINT COMMENT '最近1日使用某券下单次数',
`order_last_1d_reduce_amount` DECIMAL(16,2) COMMENT '最近1日使用某券下单优惠金额',
`order_last_1d_original_amount` DECIMAL(16,2) COMMENT '最近1日使用某券下单原始金额',
`order_last_1d_final_amount` DECIMAL(16,2) COMMENT '最近1日使用某券下单最终金额',
`order_last_7d_count` BIGINT COMMENT '最近7日使用某券下单次数',
`order_last_7d_reduce_amount` DECIMAL(16,2) COMMENT '最近7日使用某券下单优惠金额',
`order_last_7d_original_amount` DECIMAL(16,2) COMMENT '最近7日使用某券下单原始金额',
`order_last_7d_final_amount` DECIMAL(16,2) COMMENT '最近7日使用某券下单最终金额',
`order_last_30d_count` BIGINT COMMENT '最近30日使用某券下单次数',
`order_last_30d_reduce_amount` DECIMAL(16,2) COMMENT '最近30日使用某券下单优惠金额',
`order_last_30d_original_amount` DECIMAL(16,2) COMMENT '最近30日使用某券下单原始金额',
`order_last_30d_final_amount` DECIMAL(16,2) COMMENT '最近30日使用某券下单最终金额',
`order_count` BIGINT COMMENT '累积使用(下单)次数',
`order_reduce_amount` DECIMAL(16,2) COMMENT '使用某券累积下单优惠金额',
`order_original_amount` DECIMAL(16,2) COMMENT '使用某券累积下单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '使用某券累积下单最终金额',
`payment_last_1d_count` BIGINT COMMENT '最近1日使用某券支付次数',
`payment_last_1d_reduce_amount` DECIMAL(16,2) COMMENT '最近1日使用某券优惠金额',
`payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日使用某券支付金额',
`payment_last_7d_count` BIGINT COMMENT '最近7日使用某券支付次数',
`payment_last_7d_reduce_amount` DECIMAL(16,2) COMMENT '最近7日使用某券优惠金额',
`payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日使用某券支付金额',
`payment_last_30d_count` BIGINT COMMENT '最近30日使用某券支付次数',
`payment_last_30d_reduce_amount` DECIMAL(16,2) COMMENT '最近30日使用某券优惠金额',
`payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日使用某券支付金额',
`payment_count` BIGINT COMMENT '累积使用(支付)次数',
`payment_reduce_amount` DECIMAL(16,2) COMMENT '使用某券累积优惠金额',
`payment_amount` DECIMAL(16,2) COMMENT '使用某券累积支付金额',
`expire_last_1d_count` BIGINT COMMENT '最近1日过期次数',
`expire_last_7d_count` BIGINT COMMENT '最近7日过期次数',
`expire_last_30d_count` BIGINT COMMENT '最近30日过期次数',
`expire_count` BIGINT COMMENT '累积过期次数'

活动主题
dwt_activity_topic(
`activity_rule_id` STRING COMMENT '活动规则ID',
`activity_id` STRING  COMMENT '活动ID',
`order_last_1d_count` BIGINT COMMENT '最近1日参与某活动某规则下单次数',
`order_last_1d_reduce_amount` DECIMAL(16,2) COMMENT '最近1日参与某活动某规则下单优惠金额',
`order_last_1d_original_amount` DECIMAL(16,2) COMMENT '最近1日参与某活动某规则下单原始金额',
`order_last_1d_final_amount` DECIMAL(16,2) COMMENT '最近1日参与某活动某规则下单最终金额',
`order_count` BIGINT COMMENT '参与某活动某规则累积下单次数',
`order_reduce_amount` DECIMAL(16,2) COMMENT '参与某活动某规则累积下单优惠金额',
`order_original_amount` DECIMAL(16,2) COMMENT '参与某活动某规则累积下单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '参与某活动某规则累积下单最终金额',
`payment_last_1d_count` BIGINT COMMENT '最近1日参与某活动某规则支付次数',
`payment_last_1d_reduce_amount` DECIMAL(16,2) COMMENT '最近1日参与某活动某规则支付优惠金额',
`payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日参与某活动某规则支付金额',
`payment_count` BIGINT COMMENT '参与某活动某规则累积支付次数',
`payment_reduce_amount` DECIMAL(16,2) COMMENT '参与某活动某规则累积支付优惠金额',
`payment_amount` DECIMAL(16,2) COMMENT '参与某活动某规则累积支付金额'

地区主题
dwt_area_topic(
`province_id` STRING COMMENT '编号',
`visit_last_1d_count` BIGINT COMMENT '最近1日访客访问次数',
`login_last_1d_count` BIGINT COMMENT '最近1日用户访问次数',
`visit_last_7d_count` BIGINT COMMENT '最近7访客访问次数',
`login_last_7d_count` BIGINT COMMENT '最近7日用户访问次数',
`visit_last_30d_count` BIGINT COMMENT '最近30日访客访问次数',
`login_last_30d_count` BIGINT COMMENT '最近30日用户访问次数',
`visit_count` BIGINT COMMENT '累积访客访问次数',
`login_count` BIGINT COMMENT '累积用户访问次数',
`order_last_1d_count` BIGINT COMMENT '最近1天下单次数',
`order_last_1d_original_amount` DECIMAL(16,2) COMMENT '最近1天下单原始金额',
`order_last_1d_final_amount` DECIMAL(16,2) COMMENT '最近1天下单最终金额',
`order_last_7d_count` BIGINT COMMENT '最近7天下单次数',
`order_last_7d_original_amount` DECIMAL(16,2) COMMENT '最近7天下单原始金额',
`order_last_7d_final_amount` DECIMAL(16,2) COMMENT '最近7天下单最终金额',
`order_last_30d_count` BIGINT COMMENT '最近30天下单次数',
`order_last_30d_original_amount` DECIMAL(16,2) COMMENT '最近30天下单原始金额',
`order_last_30d_final_amount` DECIMAL(16,2) COMMENT '最近30天下单最终金额',
`order_count` BIGINT COMMENT '累积下单次数',
`order_original_amount` DECIMAL(16,2) COMMENT '累积下单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '累积下单最终金额',
`payment_last_1d_count` BIGINT COMMENT '最近1天支付次数',
`payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1天支付金额',
`payment_last_7d_count` BIGINT COMMENT '最近7天支付次数',
`payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7天支付金额',
`payment_last_30d_count` BIGINT COMMENT '最近30天支付次数',
`payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30天支付金额',
`payment_count` BIGINT COMMENT '累积支付次数',
`payment_amount` DECIMAL(16,2) COMMENT '累积支付金额',
`refund_order_last_1d_count` BIGINT COMMENT '最近1天退单次数',
`refund_order_last_1d_amount` DECIMAL(16,2) COMMENT '最近1天退单金额',
`refund_order_last_7d_count` BIGINT COMMENT '最近7天退单次数',
`refund_order_last_7d_amount` DECIMAL(16,2) COMMENT '最近7天退单金额',
`refund_order_last_30d_count` BIGINT COMMENT '最近30天退单次数',
`refund_order_last_30d_amount` DECIMAL(16,2) COMMENT '最近30天退单金额',
`refund_order_count` BIGINT COMMENT '累积退单次数',
`refund_order_amount` DECIMAL(16,2) COMMENT '累积退单金额',
`refund_payment_last_1d_count` BIGINT COMMENT '最近1天退款次数',
`refund_payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1天退款金额',
`refund_payment_last_7d_count` BIGINT COMMENT '最近7天退款次数',
`refund_payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7天退款金额',
`refund_payment_last_30d_count` BIGINT COMMENT '最近30天退款次数',
`refund_payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30天退款金额',
`refund_payment_count` BIGINT COMMENT '累积退款次数',
`refund_payment_amount` DECIMAL(16,2) COMMENT '累积退款金额'

ads层
根据具体需求定
此层不压缩和分区了，存储的是各个需求的结果，数据量少

访客主题
访客统计
ads_visit_stats (
`dt` STRING COMMENT '统计日期',
`is_new` STRING COMMENT '新老标识,1:新,0:老',
`recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
`channel` STRING COMMENT '渠道',
`uv_count` BIGINT COMMENT '日活(访问人数)',
`duration_sec` BIGINT COMMENT '页面停留总时长',
`avg_duration_sec` BIGINT COMMENT '一次会话，页面停留平均时长,单位为描述',
`page_count` BIGINT COMMENT '页面总浏览数',
`avg_page_count` BIGINT COMMENT '一次会话，页面平均浏览数',
`sv_count` BIGINT COMMENT '会话次数',
`bounce_count` BIGINT COMMENT '跳出数',
`bounce_rate` DECIMAL(16,2) COMMENT '跳出率'

路径分析
ads_page_path (
`dt` STRING COMMENT '统计日期',
`recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
`source` STRING COMMENT '跳转起始页面ID',
`target` STRING COMMENT '跳转终到页面ID',
`path_count` BIGINT COMMENT '跳转次数'

用户主题
用户统计
ads_user_total (
`dt` STRING COMMENT '统计日期',
`recent_days` BIGINT COMMENT '最近天数,0:累积值,1:最近1天,7:最近7天,30:最近30天',
`new_user_count` BIGINT COMMENT '新注册用户数',
`new_order_user_count` BIGINT COMMENT '新增下单用户数',
`order_final_amount` DECIMAL(16,2) COMMENT '下单总金额',
`order_user_count` BIGINT COMMENT '下单用户数',
`no_order_user_count` BIGINT COMMENT '未下单用户数(具体指活跃用户中未下单用户)'

用户变动统计
CREATE EXTERNAL TABLE `ads_user_change` (
  `dt` STRING COMMENT '统计日期',
  `user_churn_count` BIGINT COMMENT '流失用户数',
  `user_back_count` BIGINT COMMENT '回流用户数'
) COMMENT '用户变动统计'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/gmall/ads/ads_user_change/';

用户行为漏斗分析
CREATE EXTERNAL TABLE `ads_user_action` (
  `dt` STRING COMMENT '统计日期',
  `recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
  `home_count` BIGINT COMMENT '浏览首页人数',
  `good_detail_count` BIGINT COMMENT '浏览商品详情页人数',
  `cart_count` BIGINT COMMENT '加入购物车人数',
  `order_count` BIGINT COMMENT '下单人数',
  `payment_count` BIGINT COMMENT '支付人数'
) COMMENT '漏斗分析'
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/gmall/ads/ads_user_action/';

用户留存率
CREATE EXTERNAL TABLE ads_user_retention (
  `dt` STRING COMMENT '统计日期',
  `create_date` STRING COMMENT '用户新增日期',
  `retention_day` BIGINT COMMENT '截至当前日期留存天数',
  `retention_count` BIGINT COMMENT '留存用户数量',
  `new_user_count` BIGINT COMMENT '新增用户数量',
  `retention_rate` DECIMAL(16,2) COMMENT '留存率'
) COMMENT '用户留存率'
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/gmall/ads/ads_user_retention/';

商品主题
商品统计
ads_order_spu_stats(
`dt` STRING COMMENT '统计日期',
`recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
`spu_id` STRING COMMENT '商品ID',
`spu_name` STRING COMMENT '商品名称',
`tm_id` STRING COMMENT '品牌ID',
`tm_name` STRING COMMENT '品牌名称',
`category3_id` STRING COMMENT '三级品类ID',
`category3_name` STRING COMMENT '三级品类名称',
`category2_id` STRING COMMENT '二级品类ID',
`category2_name` STRING COMMENT '二级品类名称',
`category1_id` STRING COMMENT '一级品类ID',
`category1_name` STRING COMMENT '一级品类名称',
`order_count` BIGINT COMMENT '订单数',
`order_amount` DECIMAL(16,2) COMMENT '订单金额'

品牌复购率
CREATE EXTERNAL TABLE `ads_repeat_purchase` (
  `dt` STRING COMMENT '统计日期',
  `recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
  `tm_id` STRING COMMENT '品牌ID',
  `tm_name` STRING COMMENT '品牌名称',
  `order_repeat_rate` DECIMAL(16,2) COMMENT '复购率'
) COMMENT '品牌复购率'
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/gmall/ads/ads_repeat_purchase/';

订单主题
订单统计
ads_order_total(
`dt` STRING COMMENT '统计日期',
`recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
`order_count` BIGINT COMMENT '订单数',
`order_amount` DECIMAL(16,2) COMMENT '订单金额',
`order_user_count` BIGINT COMMENT '下单人数'

各地区订单统计
CREATE EXTERNAL TABLE `ads_order_by_province` (
  `dt` STRING COMMENT '统计日期',
  `recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
  `province_id` STRING COMMENT '省份ID',
  `province_name` STRING COMMENT '省份名称',
  `area_code` STRING COMMENT '地区编码',
  `iso_code` STRING COMMENT '国际标准地区编码',
  `iso_code_3166_2` STRING COMMENT '国际标准地区编码',
  `order_count` BIGINT COMMENT '订单数',
  `order_amount` DECIMAL(16,2) COMMENT '订单金额'
) COMMENT '各地区订单统计'
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/gmall/ads/ads_order_by_province/';

优惠券主题
优惠券统计
ads_coupon_stats (
`dt` STRING COMMENT '统计日期',
`coupon_id` STRING COMMENT '优惠券ID',
`coupon_name` STRING COMMENT '优惠券名称',
`start_date` STRING COMMENT '发布日期',
`rule_name` STRING COMMENT '优惠规则，例如满100元减10元',
`get_count`  BIGINT COMMENT '领取次数',
`order_count` BIGINT COMMENT '使用(下单)次数',
`expire_count`  BIGINT COMMENT '过期次数',
`order_original_amount` DECIMAL(16,2) COMMENT '使用优惠券订单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '使用优惠券订单最终金额',
`reduce_amount` DECIMAL(16,2) COMMENT '优惠金额',
`reduce_rate` DECIMAL(16,2) COMMENT '补贴率'

活动主题
活动统计
ads_activity_stats(
`dt` STRING COMMENT '统计日期',
`activity_id` STRING COMMENT '活动ID',
`activity_name` STRING COMMENT '活动名称',
`start_date` STRING COMMENT '活动开始日期',
`order_count` BIGINT COMMENT '参与活动订单数',
`order_original_amount` DECIMAL(16,2) COMMENT '参与活动订单原始金额',
`order_final_amount` DECIMAL(16,2) COMMENT '参与活动订单最终金额',
`reduce_amount` DECIMAL(16,2) COMMENT '优惠金额',
`reduce_rate` DECIMAL(16,2) COMMENT '补贴率'
-------------
搭建hadoop2.7.2单机
1.ssh, rsync, jdk8
2.配置hadoop环境变量
3./etc/hadoop/core-site.xml
<configuration> 
    <property> 
        <name>fs.defaultFS</name> 
        <value>hdfs://bigdata101:9000</value> 
    </property> 
    <!-- 配置 HDFS 网页登录使用的静态用户为 root -->
    <property>
            <name>hadoop.http.staticuser.user</name>
            <value>root</value>
    </property>
    <!-- 配置 该root（superUser）允许通过代理访问的主机节点 -->
    <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>*</value>
    </property>
    <!-- 配置 该root（superUser）允许通过代理用户所属组 -->
    <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
    </property>
</configuration>
4./etc/hadoop/hdfs-site.xml
<configuration> 
    <property> 
        <name>dfs.replication</name> 
        <value>1</value> 
    </property> 
</configuration>
4.ssh主机授权
5.在本地MapReduce运行作业
格式化文件系统
bin/hdfs namenode -format
启动NameNode和DataNode进程
sbin/start-dfs.sh

jps：
3080 DataNode
3273 SecondaryNameNode
2988 NameNode

进程日志输出在$HADOOP_LOG_DIR目录（默认为$HADOOP_HOME/logs）

NameNode 的 Web 界面
http://ip:50070/

6.创建hdfs文件目录
bin/hdfs dfs -mkdir /user 
bin/hdfs dfs -mkdir /user/shon

将本地文件放入hdfs中
bin/hdfs dfs -put etc/hadoop /user/shon

运行示例：
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep /user/shon/a /output 'dfs[az.]+'

将hdfs文件复制到本地
bin/hdfs dfs -get /hdfsUrl /serverUrl

直接查看hdfs上文件
bin/hdfs dfs -cat /user/shon/a

7.关闭hdfs
sbin/stop-dfs.sh

8.运行ResourceManager进程和 NodeManager进程
/etc/hadoop/mapred-site.xml
<configuration> 
    <property> 
        <name>mapreduce.framework.name</name> 
        <value>yarn</value> 
    </property> 
</configuration>

/etc/hadoop/yarn-site.xml
<configuration> 
    <property> 
        <name>yarn.nodemanager.aux-services</name> 
        <value>mapreduce_shuffle</value> 
    </property> 
    <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
	<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>

sbin/start-yarn.sh

jps：
5410 NodeManager
3080 DataNode
5304 ResourceManager
3273 SecondaryNameNode
2988 NameNode

ResourceManager 的 Web 界面
http://192.168.30.117:8088/

9.关闭yarn进程
sbin/stop-yarn.sh
-------------
scala-2.11.12安装
环境变量
#scala
export SCALA_HOME=/opt/module/scala-2.11.12
export PATH=$PATH:$SCALA_HOME/bin

验证
scala -version 
-------------
spark-2.4.3安装
环境变量
#spark
export SPARK_HOME=/opt/bigdata/spark-2.4.3
export PATH=$PATH:$SPARK_HOME/bin

验证
spark-shell

vim conf/spark-env.sh
export SPARK_HOME=/opt/bigdata/spark-2.4.3
export SCALA_HOME=/opt/bigdata/scala-2.11.12
export JAVA_HOME=/opt/java/jdk1.8.0_212
export HADOOP_HOME=/opt/bigdata/hadoop-2.7.2
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SCALA_HOME/bin
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$YARN_HOME/etc/hadoop
#export SPARK_MASTER_IP=218.199.92.227
#SPARK_LOCAL_DIRS=/home/haodop/spark-1.6.1-bin-hadoop2.6
#SPARK_DRIVER_MEMORY=1G
export SPARK_LIBARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native
-------------
修改hbase默认端口
vim conf/hbase-site.xml

<property>
    <name>hbase.master.port</name>
    <value>16000</value>
</property>
<property>
    <name>hbase.master.info.port</name>
    <value>16010</value>
</property>
<property>
    <name>hbase.regionserver.port</name>
    <value>16201</value>
</property>
<property>
    <name>hbase.regionserver.info.port</name>
    <value>16301</value>
</property>
-------------