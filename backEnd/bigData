----------
大数据数据仓库项目

大数据数据来源
1.业务数据 => mysql
2.用户行为数据 => 通过埋点，记录在日志服务器，文件形式
3.爬虫数据 => 爬取其他网站（违法）

数据仓库
为企业制定决策，提供数据支持

用户行为数据=>通过（flume框架，文件形式存储）=>hive(hdfs)
业务数据（mysql）=>sqoop=>hive(hdfs)

数据仓库分层（数据纬度建模）（atlas看数据分层的流向）
ODS数据原始备份
DWD数据清洗（比如去重、去空、脱敏）
DIM数据维度层，保存维度数据，对业务事实的描述信息，例如何时何人何地
DWS（数据汇总层，按天轻度汇总，比如一个用户一天下单次数）,DWT（数据主题层，累积型，例如一个用户从注册至今一共下了多少次单）数据聚合（合成大表，类似join池）
ADS统计报表层，存在mysql => [报表系统（图表展示），用户画像（给用户打标签），推荐系统]

数仓为什么要分层
1.复杂问题简单化
2.减少重复开发
3.隔离原始数据

用户画像：
统计类标签（hql）
规则类标签（hql）
机器学习（算法）

数据仓库分层由任务调度工具完成（Azkaban,Oozie）

数仓集群监控Zabbix,Prometheus
数仓权限管理Ranger

hdfs存储多目录
1.linux磁盘挂载
2.hdfs-site.xml
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3</value>
</property>

yarn配置
给开发测试环境增加ApplicationMaster资源比例，生产用默认值10%
默认每个资源队列上的ApplicationMaster最多可使用资源为总资源的10%，防止Map/Reduce Task无法执行，但开发环境集群资源少，一个job可能就超10%了，任务可达并行度很低，故可调大ApplicationMaster占比
vim /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml
改为80%
<property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.8</value>

可在http://bigdata101:8088/cluster/scheduler 中的
Configured Max Application Master Limit查看

关闭hive map端的小文件合并
因为hive map端的小文件合并，会把lzo文件的索引文件合并掉，导致lzo文件无法切片
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
----------
hadoop:
1.海量数据的存储，tb,pb,eb
2.海量数据的分析计算

4高，高可靠性，高扩展性，高效性，高容错性

hdfs 
NameNode（nn）元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等
DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。
Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份

yarn
ResourceManager：负责整个集群的资源管理与调度
NodeManager：管理Hadoop集群中单个计算节点

MapReduce
MapReduce 将计算过程分为两个阶段：Map 和 Reduce

Map 阶段并行处理输入数据（拆数据）
Reduce 阶段对 Map 结果进行汇总（合数据）

部署方式：
单机:
无hdfs，yarn，只能测试MapReduce
修改配置文件：
hadoop-env.sh添加上JAVA_HOME

伪分布式:
进程全，都在一台机器上
修改配置文件：
hadoop-env.sh
hdfs-site.xml
core-site.xml
mapred-site.xml	
yarn-site.xml

启动：
sbin/start-all.sh

jobhistory历史服务器默认是不启动的

分布式:
进程全，在多台机器上
修改配置文件：

核心配置文件
core-site.xml

hdfs配置文件
hdfs-site.xml

yarn配置文件，配置日志聚集功能
yarn-site.xml

MapReduce配置文件，配置历史服务器
mapred-site.xml	

workers配置文件

Hadoop三种访问方式：
1.命令行方式
2.java api 访问
3.web 浏览器
----------
atlas概念和架构
搭建atlas（元数据管理：组建大数据数据字典，查看大数据资产目录，数据质量监控，自动生成大数据表和字段的血缘关系图）

hive做元数据仓库
atlas将hive中的元数据做成数据字典，方便查询管理

atlas为组织提供开放式元数据管理和治理功能，构建资产目录，形成数据字典

数据字典：可以查到hive库的释义，表介绍，字段解释说明
表与表间血缘依赖
字段和字段间血缘依赖

atlas架构
核心：【导入/导出模块，分类系统模块，图引擎模块】

1.导入/导出模块:
可导入元数据源：【hive,sqoop,falcon,storm,hbase】

atlas能对于hive中表结构改变，而增量导入，通过kafka消息队列

atlas的 hive hook钩子，能把hive变动的元数据同步到kafka中，做消息缓冲，atlas导入导出会消费kafka	

atlas本身不存储元数据，还要导出数据，用hbase存储（metadata store元数据存储库）

2.分类系统模块：
atlas对元数据分类
库，表，字段，路径

每一张hive表都指向hdfs的一个存储路径

3.图引擎模块：
生成对应图表
表与表间血缘依赖图
字段间血缘依赖图

因为hbase底层kv结构，线性结构不能存储图结构

简单结构：线性结构
复杂：树结构
最复杂：图结构（点，线）

要存入图数据库（janusGraph），经过图数据库转变，再存入hbase里

atlas搭建数据字典，solr全文检索数据库，类似es，帮助搜索元数据

solr给所有元数据做索引（index store索引数据库）

atlas提供的api接口（http/rest协议）

上层对接
ranger tag
admin ui(查询atlas元数据库)
业务分类系统

流程总结：
1.metadata sources:atlas支持一下来源提取：hive,sqoop,falcon,storm,hbase,kafka,spark

2.messaging:与kafka对接的消息队列

3.导入/导出，类型系统分裂，图形引擎

4.hbase元数据库，solr索引数据库

5.http/rest的api

6.admin UI，ranger权限管理模块，业务分类模块

atlas2更新
支持hadoop3，hive3，hbase2，solr7.5,kafka2，janusGraph0.3.1
身份验证可信代理
指标模块收集通知
atlas增量导出元数据模式

hadoop三组件：
hdfs[nameNode,dataNode,secondaryNameNode],
yarn[resourceManager.nodeManager],
historyServer[jobHistoryServer]

安装顺序：
1.搭建前准备
2.jdk
3.hadoop
4.mysql
5.hive
6.zookeeper
7.kafka
8.hbase
9.solr
10.atlas
kafka, hbase, solr把自己的元数据（配置信息）存入zookeeper中
hive会把元数据存入mysql
----------
atlas搭建步骤
1.搭建前准备
2.jdk-8u212-linux-x64.tar.gz
3.hadoop-3.1.3.tar.gz集群

4.mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar
按照顺序安装
sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm 
如果linux最小化安装，额外装 sudo yum install -y libaio
sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm 

5.apache-hive-3.1.2-bin.tar.gz
6.apache-zookeeper-3.5.7-bin.tar.gz集群
7.kafka_2.11-2.4.1.tgz集群
8.hbase-2.0.5-bin.tar.gz集群
9.solr-7.7.3.tgz集群
10.apache-atlas-2.1.0-sources.tar.gz

1.搭建前准备
准备bigData100模板虚拟机（centos7，CentOS-7.5-x86-1804）
内存/处理器/硬盘：4 8 60

添加用户，修改用户密码
useradd shonqian
passwd shonqian

配置shonqian用户root权限
vim /etc/sudoers
大约91行，%wheel 这行下面加上
shonqian ALL=(ALL) NOPASSWD:ALL

安装必要环境
vim /etc/yum.repos.d/epel.repo
baseurl解开注释，metalink加上注释即可
yum install -y epel-release
yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git

关闭centos防火墙
systemctl stop firewalld
systemctl disable firewalld.service

在/opt目录下创建文件夹，module，software
software放安装包
module是安装目录

修改 module、software 文件夹的所有者和所属组均为 shonqian 用户（-R对子文件夹也起作用）
sudo chown -R shonqian:shonqian /opt/module
sudo chown -R shonqian:shonqian /opt/software

查看 module、software 文件夹的所有者和所属组，是否已为shonqian
cd /opt/
ll

卸载虚拟机自带的 JDK
查看是否有：
rpm -qa | grep -i java
卸载：
rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps

reboot看是否成功

配置/etc/hosts 文件，建立ip地址和主机名映射
vim /etc/hosts
后面加上
192.168.137.100 bigData100
192.168.137.101 bigData101
192.168.137.102 bigData102
192.168.137.103 bigData103
192.168.137.104 bigData104
192.168.137.105 bigData105
192.168.137.106 bigData106
192.168.137.107 bigData107
192.168.137.108 bigData108

把bigData100模板虚拟机克隆三台虚拟机
内存/处理器/硬盘
8 8 60
4 8 60
4 8 60

固定ip
cd /etc/sysconfig/network-scripts/
vim ifcfg-eno16777736

BOOTPROTO="static"
IPADDR=192.168.137.102
GATEWAY=192.168.137.2

改主机名
vim /etc/hostname
bigData102

reboot

其它两台参照配置

ping www.baidu.com 看能否连接外网

联不通可以右上角网络设置，设置下dns（223.5.5.5/223.6.6.6阿里的dns），再把飞行模式和有线连接开关下

xshell连接

配置windows hosts文件
C:\Windows\System32\drivers\etc\hosts

192.168.137.100 bigData100
192.168.137.101 bigData101
192.168.137.102 bigData102
192.168.137.103 bigData103
192.168.137.104 bigData104
192.168.137.105 bigData105
192.168.137.106 bigData106
192.168.137.107 bigData107
192.168.137.108 bigData108
192.168.137.109 bigData109

在102机器上把所有软件安装包通过xftp传到/opt/software中

配置ssh免密登录

ssh localhost

找到.ssh文件夹
find / -name .ssh -type d

cd /home/root/.ssh/

生成公钥
ssh-keygen -t rsa

将公钥复制到远程机器的authorized_keys文件中，也能有到远程机器的home, ~./ssh , 和 ~/.ssh/authorized_keys的权利
ssh-copy-id bigData102
ssh-copy-id bigData103
ssh-copy-id bigData104
返回普通用户
exit

2.jdk8安装
在102上安装java1.8
在/opt/software中
tar -zxvf jdk-xxxx.tar.gz -C /opt/module/
cd /opt/module
mv jdk1.8.0_212/ jdk1.8

cd /etc/profile.d/

sudo vim my_env.sh
添加
# JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8
export PATH=$PATH:$JAVA_HOME/bin

source /etc/profile

集群分发脚本xsync

echo $PATH 查到现有的path路径，在path路径下添加脚本，可到处执行
cd /usr/local/bin

vim xsync
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
	echo Not Enough Arguement!
	exit;
fi
#2. 遍历集群所有机器
for host in bigData102 bigData103 bigData104
do
	echo ==================== $host ====================
	#3. 遍历所有目录，挨个发送
	for file in $@
	do
		#4. 判断文件是否存在
		if [ -e $file ]
		then
			#5. 获取父目录
			pdir=$(cd -P $(dirname $file); pwd)
			#6. 获取当前文件的名称
			fname=$(basename $file)
			ssh $host "mkdir -p $pdir"
			rsync -av $pdir/$fname $host:$pdir
		else
			echo $file does not exists!
			fi
	done
done

给脚本添加执行权限
chmod +x xsync

cd ..
xsync bin	

cd /opt/module
xsync java8
//普通用户没有权限发送etc目录。而sudo xsync也不行，因为xsync是配在普通用户的PATH里，root用户的家目录里没有，所以要将xsync向root的PATH发一份
sudo cp /usr/local/bin/xsync  /usr/bin/
sudo xsync /etc/profile.d/my_env.sh

xshell-工具-发送键输入到所有会话
source /etc/profile

3.安装hadoop3.1.3集群部署
在102机器
cd /opt/software/2_hadoop
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/

给hadoop配置环境变量
sudo vim /etc/profile.d/my_env.sh

#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

source下
hadoop version

配置hadoop完全分布式集群
cd /opt/module/hadoop-3.1.3/etc/hadoop
核心配置文件
core-site.xml
vim core-site.xml
<configuration>
	<!-- 指定 NameNode 的地址 -->
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://bigData102:8020</value>
	</property>
	<!-- 指定 hadoop 数据的存储目录 -->
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/module/hadoop-3.1.3/data</value>
	</property>
	<!-- 配置 HDFS 网页登录使用的静态用户为 shonqian -->
	<property>
		<name>hadoop.http.staticuser.user</name>
		<value>shonqian</value>
	</property>
	<!-- 配置 该shonqian（superUser）允许通过代理访问的主机节点 -->
	<property>
		<name>hadoop.proxyuser.shonqian.hosts</name>
		<value>*</value>
	</property>
	<!-- 配置 该shonqian（superUser）允许通过代理用户所属组 -->
	<property>
		<name>hadoop.proxyuser.shonqian.groups</name>
		<value>*</value>
	</property>
	<!-- 配置 该shonqian（superUser）允许通过代理的用户 -->
	<property>
		<name>hadoop.proxyuser.shonqian.groups</name>
		<value>*</value>
	</property>
</configuration>

hdfs配置文件
vim hdfs-site.xml
<configuration>
	<!-- namenode web 端访问地址-->
	<property>
		<name>dfs.namenode.http-address</name>
		<value>bigData102:9870</value>
	</property>
	<!-- 2nn(secondaryNameNode) web 端访问地址-->
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>bigData104:9868</value>
	</property>
</configuration>

yarn配置文件
vim yarn-site.xml
<configuration>
    <!-- 指定 MR 走 shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定 ResourceManager 的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigData103</value>
    </property>
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name> <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
	<!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
	<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
	<!-- yarn容器允许管理的物理内存大小 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>	
	<!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
	<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>

MapReduce配置文件
vim mapred-site.xml	
<configuration>
    <!-- 指定 MapReduce 程序运行在 Yarn 上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

workers配置文件
vim workers
删去localhost
bigData102
bigData103
bigData104
表示集群有3台机器

配置历史服务器
vim mapred-site.xml
<!-- 历史服务器端地址 -->
<property>
	<name>mapreduce.jobhistory.address</name>
	<value>bigData102:10020</value>
</property>
<!-- 历史服务器web端地址 -->
<property>
	<name>mapreduce.jobhistory.webapp.address</name>
	<value>bigData102:19888</value>
</property>

配置日志聚集功能
vim yarn-site.xml
<!-- 开启日志聚集功能 -->
<property>
	<name>yarn.log-aggregation-enabled</name>
	<value>true</value>
</property>
<!-- 设置日志聚集服务器地址 -->
<property>
	<name>yarn.log.server.url</name>
	<value>http://bigData102:19888/jobhistory/logs</value>
</property>
<!-- 设置日志保留时间为7天 -->
<property>
	<name>yarn.log-aggregation.retain-seconds</name>
	<value>604800</value>
</property>

hadoop文件分发
cd /opt/module
xsync hadoop-3.1.3/
sudo xsync /etc/profile.d/my_env.sh

工具-发送键输入到所有会话
source /etc/profile
hadoop version 检测下

cd hadoop-3.1.3/

第一次启动前，102机器格式化namenode
hdfs namenode -format

启动hadoop
群起
102上
#sbin/start-dfs.sh
103
#sbin/start-yarn.sh
102开启历史服务器
#mapred --daemon start historyserver

群起脚本
myhadoop.sh

cd /usr/local/bin

vim myhadoop.sh

#!/bin/bash
if [ $# -lt 1 ]
then
  echo "No Args Input..."
  exit ;
fi
case $1 in
"start")
  echo " =================== 启动 hadoop 集群 ==================="
  echo " --------------- 启动 hdfs ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
  echo " --------------- 启动 yarn ---------------"
  ssh bigData103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
  echo " --------------- 启动 historyserver ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
  echo " =================== 关闭 hadoop 集群 ==================="
  echo " --------------- 关闭 historyserver ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
  echo " --------------- 关闭 yarn ---------------"
  ssh bigData103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
  echo " --------------- 关闭 hdfs ---------------"
  ssh bigData102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
  echo "Input Args Error..."
;;
esac

chmod +x myhadoop.sh

102上
myhadoop.sh start 开启hadoop集群
myhadoop.sh stop 关闭hadoop集群

jpsall查看3台机器java进程
cd /usr/local/bin

vim jpsall

#!/bin/bash
for host in bigData102 bigData103 bigData104
do
	echo =============== $host ===============
	ssh $host jps
done

chmod +x jpsall

如果未格式化启动了hadoop，去102，103，104删除data和logs目录

===============
正常启动jpsall显示：
bigData102
NodeManager
JobHistoryServer
DataNode
NameNode

bigData103
ResourceManager
DataNode
NodeManager

bigData104
SecondaryNameNode
NodeManager
DataNode

一个NameNode，三个DateNode
一个ResourceManager，三个NodeManager
一个Secondary	NameNode，一个JobHistoryServer

102，103，104 => 4，3，3架构（hadoop集群）
===============

浏览器访问hdfs
http://bigData102:9870/dfshealth.html#tab-overview

创建目录，上传文件
http://bigData102:9870/explorer.html#/
能上传就说明hdfs没问题

yarn
http://bigData103:8088/cluster

历史服务器
http://bigData102:19888/jobhistory

4.安装mysql
因为hive元数据是存入mysql中

102上装mysql
检查是否安装过mysql
rpm -qa | grep mariadb
如有需卸载
sudo rpm -e --nodeps mariadb-libs

cd /opt/software/3_mysql
里面有mysql安装包、连接驱动

tar -xvf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar

按照顺序安装
sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm 
sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm 
如果linux最小化安装，额外装 sudo yum install -y libaio
sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm 

检查mysql指向目录，是否干净
cd /var/lib/mysql
sudo ls
如果有东西，sudo rm -rf ./*

初始化mysql
sudo mysqld --initialize --user=mysql
查看临时生成的root用户的密码
sudo cat /var/log/mysqld.log

启动mysql服务
sudo systemctl start mysqld
查看状态
sudo systemctl status mysqld

mysql -uroot -p
输入初始密码

重置密码
set password = password("root");

修改mysql库下user表中的root用户允许任意ip连接，'%'表示允许所有机器访问
update mysql.user set host = '%' where user = 'root';
刷新mysql表权限
flush privileges;
quit;

mysql -uroot -proot

5.hive3.1.2安装部署
102机器
cd /opt/software/4_hive
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/
cd /opt/module/
mv apache-hive-3.1.2-bin/ hive

添加hive环境变量
sudo vim /etc/profile.d/my_env.sh
#HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
source /etc/profile
hive --version

hive元数据配置到mysql

给hive拷贝mysql连接驱动
cp /opt/software/3_mysql/mysql-connector-java-5.1.37.jar /opt/module/hive/lib/

配置hive配置文件
cd /opt/module/hive/conf
vim hive-site.xml

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- jdbc连接的url -->
  <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://bigData102:3306/metastore?useSSL=false</value>
  </property>
<!-- jdbc连接的Driver -->
  <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
  </property>
<!-- jdbc连接的username -->
  <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
  </property>
<!-- jdbc连接的password -->
  <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>root</value>
  </property>
<!-- Hive默认在HDFS的工作目录 -->
  <property>
      <name>hive.metastore.warehoues.dir</name>
      <value>/user/hive/warehouse</value>
  </property>
<!-- 指定hiveserver2连接的端口号 -->
  <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
  </property>
<!-- 指定hiveserver2连接的host -->
  <property>
    <name>hive.server2.thrift.bind.host</name>
    <value>bigData102</value>
  </property>

<!-- 指定存储元数据要连接的地址 -->
<!-- <property>
  <name>hive.metastore.uris</name>
  <value>thrift://bigData102:9083</value>
</property>-->

<!-- 元数据存储授权 -->
 <property>
   <name>hive.metastore.event.db.notification.api.auth</name>
   <value>false</value>
 </property> 
<!-- Hive元数据存储版本的验证 -->
 <property>
   <name>hive.metastore.schema.verification</name>
   <value>false</value>
 </property>
<!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 -->
 <property>
   <name>hive.server2.active.passive.ha.enable</name>
   <value>true</value>
 </property>
</configuration>

hive调优：
mv hive-env.sh.template hive-env.sh
vim hive-env.sh
放开hadoop堆内存大小注释：
export HADOOP_HEAPSIZE=1024
默认256MB，运行大型sql会报错

mv hive-log4j2.properties.template hive-log4j2.properties
vim hive-log4j2.properties

修改hive运行日志目录
property.hive.log.dir = /opt/module/hive/logs
默认在/tmp/user/hive/log

初始化hive元数据库
mysql -uroot -proot
新建hive元数据库
create database metastore;
quit;
初始化hive元数据库，创建对应表（存储元数据所需要的表）
schematool -initSchema -dbType mysql -verbose

启动hive客户端
前提：hadoop是正常运行的

进到hive客户端
hive
show databases; (一个default库)

show tables;(没有表)

在hive创建用户表
create TABLE test_user (
	`id` STRING COMMENT '编号',
	`name` STRING COMMENT '姓名',
	`province_id` STRING COMMENT '省份ID',
	`province_name` STRING COMMENT '省份名称'
) COMMENT '用户表'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

既create TABLE test_user (`id` STRING COMMENT '编号',`name` STRING COMMENT '姓名',`province_id` STRING COMMENT '省份ID',`province_name` STRING COMMENT '省份名称') COMMENT '用户表' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

向用户表装载数据
insert into table test_user values('1','zhangsan','001','北京');
此时需要跑mapReduce，mapReduce跑在yarn上
可打开http://bigData103:8088/cluster，看任务是否进入yarn集群
state为FINISHED，表示插入成功
select * from test_user;
元数据存在mysql
数据存在hdfs上，http://bigData102:9870/explorer.html#/user/hive/warehouse/test_user

设置元数据支持中文显示
desc test_user;
中文显示??，atlas也会乱码
用dbe打开metastore-COLUMNS_V2表
打开metastore属性，字符集默认是latin1
只能修改对应字段的字符集，把需要存中文的列改为utf8

在metastore库下运行sql
# 修改字段注释字符集
alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
# 修改表注释字符集
alter table TABLE_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;
# 修改分区参数，支持分区建用中文表示
alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;
alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(20000) character set utf8;
#修改索引名注释，支持中文表示
alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
#修改视图，支持视图中文
ALTER TABLE TBLS modify COLUMN VIEW_EXPANDED_TEXT mediumtext CHARACTER SET utf8;
ALTER TABLE TBLS modify COLUMN VIEW_ORIGINAL_TEXT mediumtext CHARACTER SET utf8;

修改hive-site.xml中Hive读取元数据的编码
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://bigData102:3306/metastore?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
</property>

cd /opt/module/hive/conf
vim hive-site.xml
第一条添加&amp;useUnicode=true&amp;characterEncoding=UTF-8
修改后支持存读中文

hive
desc test_user;(还是乱码，因为是以latin1存入，已损坏)

create TABLE test_user2 (`id` STRING COMMENT '编号',`name` STRING COMMENT '姓名',`province_id` STRING COMMENT '省份ID',`province_name` STRING COMMENT '省份名称') COMMENT '用户表' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

desc test_user2;

6.安装zookeeper3.5.7
zookeeper集群
分布式安装部署
cd /opt/software/5_zookeeper
tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
cd /opt/module/
mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
cd zookeeper-3.5.7
mkdir zkData
cd zkData
vim myid(服务器编号)
2
cd /opt/module/zookeeper-3.5.7/conf
mv zoo_sample.cfg zoo.cfg 
vim zoo.cfg 
dataDir=/opt/module/zookeeper-3.5.7/zkData
#######cluster集群模式#########
server.2=bigData102:2888:3888
server.3=bigData103:2888:3888
server.4=bigData104:2888:3888

2888 是主从服务交换信息端口号
3888 如果leader挂了，选举leader的端口号

cd /opt/module/
xsync zookeeper-3.5.7/

去103，104机器，改myid
vim /opt/module/zookeeper-3.5.7/zkData/myid
分别
3
4

启动zookeeper
cd /opt/module/zookeeper-3.5.7/
#bin/zkServer.sh start

群起脚本：
在102机器
cd /usr/local/bin
sudo vim zk.sh
#!/bin/bash
if [ $# -lt 1 ]
then
  echo "No Args Input..."
  exit ;
fi

case $1 in
"start")
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- zookeeper $i 启动 ------------
                ssh $i "source /etc/profile && /opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
        done
;;
"stop")
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- zookeeper $i 停止 ------------    
                ssh $i "source /etc/profile && /opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
        done
;;
"status")
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- zookeeper $i 状态 ------------    
                ssh $i "source /etc/profile && /opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
        done
;;
*)
	echo "Input Args Error..."
;;
esac

sudo chmod +x zk.sh

jpaall
是否有3个 QuorumPeerMain

zk.sh status 一主两从

7.安装kafka2.4.1（单机安装方式同）
集群规划
hadoop 3台
zk 3台
kafka 3台

在102
cd /opt/software/6_kafka
tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/

cd /opt/module/
mv kafka_2.11-2.4.1/ kafka
cd kafka
mkdir logs

修改配置文件
cd config/
vim server.properties
不同的broker.id，0，1，2
broker.id=0
下行加上
#删除topic功能
delete.topic.enable=true

#修改kafka运行日志存放的路径
#kafka日志存放的路径（kafka存的topic的消息，既topic数据。错误信息存logs目录）
log.dirs=/opt/module/kafka/data

# 配置连接zookeeper集群地址（将kakfa的元数据放在zookeeper根节点下面的kafka目录下）
zookeeper.connect=bigData102:2181,bigData103:2181,bigData104:2181/kafka

配置环境变量
sudo vim /etc/profile.d/my_env.sh 

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin

分发
xsync /opt/module/kafka/
sudo xsync /etc/profile.d/my_env.sh

103机器
vim /opt/module/kafka/config/server.properties
broker.id=1
104机器
vim /opt/module/kafka/config/server.properties
broker.id=2

工具-发送所有会话
source /etc/profile

启动kafka
# bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties

群起脚本
cd /usr/local/bin/
sudo cp zk.sh kf.sh
sudo vim kf.sh

#!/bin/bash

case $1 in
"start"){
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- kafka $i 启动 ------------
                ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
        done
};;
"stop"){
        for i in bigData102 bigData103 bigData104
        do
        echo ---------- kafka $i 停止 ------------    
                ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
        done
};;
esac

sudo chmod +x kf.sh

kf.sh start

kafka起来后，去zookeeper客户端看下
cd /opt/module/zookeeper-3.5.7/
bin/zkCli.sh
ls /  可以看见有kafka节点
ls /kafka  查看kafka元数据

kafka 命令行操作
cd /opt/module/zookeeper-3.5.7
查看kafka下topic
bin/kafka-topics.sh --zookeeper bigData102:2181/kafka --list
创建topic，3个副本，1个分区，名字叫first的topic。--topic 名字，--replication-factor 副本数，--partitions 分区数
kafka-topics.sh --zookeeper bigData102:2181/kafka --create --replication-factor 3 --partitions 1 --topic first

向topic发送消息，kafka-console-producer.sh kafka生产者脚本
kafka-console-producer.sh --broker-list bigData102:9092 --topic first
1
2
3
4
5

103机器上，消费者脚本，--from-beginning从头开始消费
cd /opt/module/kafka/
bin/kafka-console-consumer.sh --bootstrap-server bigData102:9092 --from-beginning --topic first

查看topic详情
kafka-topics.sh --zookeeper bigData102:2181/kafka --describe --topic first

修改分区数
kafka-topics.sh --zookeeper bigData102:2181/kafka --alter --topic first --partitions 6

8.安装Hbase2.0.5
atlas会把元数据存入hbase
前提：
1.zookeeper正常启动，因为hbase和kafka一样，要把元数据存入zookeeper中
2.hadoop正常启动，hbase本身是hdfs的高级应用

cd /opt/software/7_hbase
tar -zxvf hbase-2.0.5-bin.tar.gz -C /opt/module/
cd /opt/module/
mv hbase-2.0.5/ hbase

配置环境变量
sudo vim /etc/profile.d/my_env.sh

#HBASE_HOME
export HBASE_HOME=/opt/module/hbase
export PATH=$PATH:$HBASE_HOME/bin

修改配置文件
cd /opt/module/hbase/conf
vim hbase-env.sh
将 内部hbase管理zk 关闭，用的是外部的zk
export HBASE_MANAGES_ZK=false


vim hbase-site.xml

<configuration>
	<!--hbase在hdfs上的根路径-->
	<property>
		<name>hbase.rootdir</name>
		<value>hdfs://bigData102:8020/HBase</value>
	</property>
	<!--是否部署分布式hbase，如果为false，会去启动内置zk，如用外置zk，得设为true-->
	<property>
		<name>hbase.cluster.distributed</name>
		<value>true</value>
	</property>
	<!--指定外部zk集群，单机版hbase只需把zk集群改为一个就行-->
	<property>
		<name>hbase.zookeeper.quorum</name>
		<value>bigData102,bigData103,bigData104</value>
	</property>
</configuration>

hbase有两个角色组成：hmaster,hregionservers

vim regionservers
删去localhost
bigData102
bigData103
bigData104

分发
cd /opt/module
xsync hbase/
sudo xsync /etc/profile.d/my_env.sh 
工具-发送键输入到所有会话
source /etc/profile

启动hbase
只启动HMaster：
#bin/hbase-daemon.sh start master

起HMaster和NodeManager：
bin/start-hbase.sh

关闭hbase
#bin/stop-hbase.sh

jpsall能看到三台分别
HMaster NodeManager / NodeManager / NodeManager

hBase集群正常启动
http://bigData102:16010/master-status

9.安装solr7.7.3（单机安装方式同）
atlas的索引数据库是solr

为solr创建系统用户solr
分别102，103，104上
sudo useradd solr

#--stdin读取echo输出，为solr用户的密码
分别102，103，104上
echo solr | sudo passwd --stdin solr

102上
cd /opt/software/8_solr
tar -zxvf solr-7.7.3.tgz -C /opt/module/
cd /opt/module
mv solr-7.7.3/ solr

把solr的所属主和所属组改为solr用户
sudo chown -R solr:solr /opt/module/solr/
ll

修改solr配置文件
cd /opt/module/solr/bin
//shonqian用户没有权限
sudo vim solr.in.sh
//solr也要将元数据存入zk，配置zk地址
ZK_HOST="bigData102:2181,bigData103:2181,bigData104:2181"
sudo xsync /opt/module/solr/

启动zk集群
zk.sh start
3台机器用solr用户启动solr
sudo -i -u solr /opt/module/solr/bin/solr start
//出现Happy searching!，表示启动成功

jps查看，会多一个jar的进程
===============
//如想按照推荐值修改
//修改打开文件数限制
vim /etc/security/limits.conf
增加
soft nofile 65000
hard nofile 65000
//修改进程数数限制
vim /etc/security/limits.d/20-nproc.conf
增加
soft nproc 65000
重启solr服务生效
===============

访问solr web页面
http://bigData102:8983/solr/#/
ui界面出现cloud菜单栏，说明solr集群部署成功

关闭所有solr
sudo -i -u solr /opt/module/solr/bin/solr stop -all


10.安装atlas2.1.0
10.1 atlas安装：
cd /opt/software/9_atlas
tar -zxvf apache-atlas-2.1.0-server.tar.gz -C /opt/module/
atlas的其它hook安装包是对接其它的钩子程序

cd /opt/module/
mv apache-atlas-2.1.0/ atlas
文件构成：
bin脚本
conf配置文件
models模块
server服务器
tools工具

atlas配置其它服务的集成
atlas集成Hbase
atlas要把元数据存入hbase

vim /opt/module/atlas/conf/atlas-application.properties
# 要存入hbase，配的是zk地址，因为hbase会把元数据放入zk中
atlas.graph.storage.hostname=bigData102:2181,bigData103:2181,bigData104:2181

vim /opt/module/atlas/conf/atlas-env.sh
# 让atlas能找到hbase的conf目录
最后行加上
export HBASE_CONF_DIR=/opt/module/hbase/conf

atlas集成solr
vim /opt/module/atlas/conf/atlas-application.properties
# 因为solr的元数据也放在的zk中，能找到zk就能找到solr
atlas.graph.index.search.solr.zookeeper-url=bigData102:2181,bigData103:2181,bigData104:2181

创建solr collection，用来存储索引文件，-c指定名字，-d指定配置文件，shards 3个分片，replicationFactor两个副本
# vertex_index 存的是点
sudo -i -u solr /opt/module/solr/bin/solr create -c vertex_index -d /opt/module/atlas/conf/solr/ -shards 3 -replicationFactor 2
# vertex_index 存的是线
sudo -i -u solr /opt/module/solr/bin/solr create -c edge_index -d /opt/module/atlas/conf/solr/ -shards 3 -replicationFactor 2
# vertex_index 存的是全文检索
sudo -i -u solr /opt/module/solr/bin/solr create -c fulltext_index -d /opt/module/atlas/conf/solr/ -shards 3 -replicationFactor 2
去 http://bigData102:8983/solr/#/~cloud 看，能看到刚建的索引

这样atlas的元数据的索引信息可以存到solr的collection中了，可以做全文检索了


atlas集成kafka
atlas需要kafka做数据缓冲，所以atlas需要kafka
vim /opt/module/atlas/conf/atlas-application.properties
atlas.notification.embedded=false
# kafka数据的存储位置
atlas.kafka.data=/opt/module/kafka/data
# kafka元数据在zk的位置
atlas.kafka.zookeeper.connect=bigData102:2181,bigData103:2181,bigData104:2181/kafka
# kafka本身broker连接地址
atlas.kafka.bootstrap.servers=bigData102:9092,bigData103:9092,bigData104:9092

atlas server配置
vim /opt/module/atlas/conf/atlas-application.properties
# 设置atlas主机名、端口号
atlas.rest.address=http://bigData102:21000
# 是否每次都初始化
atlas.server.run.setup.on.start=false
# 给atlas设置zk连接地址
atlas.audit.hbase.zookeeper.quorum=bigData102:2181,bigData103:2181,bigData104:2181

开启atlas日志中的记录性能指标
vim /opt/module/atlas/conf/atlas-log4j.xml
放开下面注释
<appender name="perf_appender" class="org.apache.log4j.DailyRollingFileAppender">
	<param name="file" value="${atlas.log.dir}/atlas_perf.log" />
	<param name="datePattern" value="'.'yyyy-MM-dd" />
	<param name="append" value="true" />
	<layout class="org.apache.log4j.PatternLayout">
		<param name="ConversionPattern" value="%d|%t|%m%n" />
	</layout>
</appender>

<logger name="org.apache.atlas.perf" additivity="false">
	<level value="debug" />
	<appender-ref ref="perf_appender" />
</logger>


atlas集成hive，钩取hive元数据
vim /opt/module/atlas/conf/atlas-application.properties
在最后加上
#########  Hive Hook Configs ########
# 关闭hive钩子自动同步，改为手动同步
atlas.hook.hive.synchronous=false
# 重试次数3
atlas.hook.hive.numRetries=3
# 队列大小10000
atlas.hook.hive.queueSize=10000
# 集群名称
atlas.cluster.name=primary

修改Hive配置文件，配置Hive Hook
vim /opt/module/hive/conf/hive-site.xml
在最后一个<property>后面加上
<!-- 让hive知道启用atlas的hook程序 -->	
<property>
   <name>hive.exec.post.hooks</name>
   <value>org.apache.atlas.hive.hook.HiveHook</value>
 </property>

安装hive hook程序
cd /opt/software/9_atlas
tar -zxvf apache-atlas-2.1.0-hive-hook.tar.gz
cp -r apache-atlas-hive-hook-2.1.0/* /opt/module/atlas/
修改hive-env.sh，把atlas/hook/hive给到hive第三方jar包路径，这样hive才能使用atlas的钩子程序，让atlas实时钩取hive元数据的变化
vim /opt/module/hive/conf/hive-env.sh
export HIVE_AUX_JARS_PATH=/opt/module/atlas/hook/hive

把atlas的配置文件拷贝到hive/conf目录
让hive能使用atlas的配置文件
cp /opt/module/atlas/conf/atlas-application.properties /opt/module/hive/conf/

10.2 atlas启动
前提：
启动hadoop集群
myhadoop.sh start
启动zk集群
zk.sh start
启动kafka集群
kf.sh start
启动hbase集群
/opt/module/hbase/bin/start-hbase.sh
启动solr集群，3台机器
sudo -i -u solr /opt/module/solr/bin/solr stop -all
sudo -i -u solr /opt/module/solr/bin/solr start

jpsall
包含jps的话，进程数9 7 7

启动atlas：
/opt/module/atlas/bin/atlas_start.py

错误信息路径：
/opt/module/atlas/logs/*.out
和
application.log

停止atlas
#bin/atlas_stop.py

atlas启动后只在102机器有一个进程

web页面
http://bigData102:21000/

如果jpsall，进程数为10 7 7
服务都起了，页面还是不出来，就是要等比较久，我等了七八分钟，页面就能正常访问了

只要application.log中没有error，就能耐心等待
第一次启动很慢，第二次启动就快了

初始账户密码，admin admin

左侧界面三tab:搜索（主用，查看数据字典，元数据管理和查询，血缘依赖的查看）、查看标签、查看商业分类，右侧是展示

10.3 atlas使用：
atlas使用
同步hive的元数据，构建元数据实体之间的关联关系，对所存储的元数据建立索引，最终为用户提供数据血缘查看和元数据检索

atlas刚装好后，需手动执行一次元数据的全量导入，后面atlas会利用hive hook增量同步hive的元数据

现在hive中有两张表test_user,test_user2

hive元数据初次导入
当公司数据仓库发展到一定阶段（hive中有很多表），才会需要元数据管理这个功能（导入atlas）
/opt/module/atlas/hook-bin/import-hive.sh
输入用户名、密码：admin admin
#Hive Meta Data imported successfully!!! 表示成功

点右上角statistics，查看元数据信息（数据资产目录）
hive_column(8) 8个列
hive_db(1) 1个库
hive_storagedesc(2) hive存储位置
hive_column(2) 2个表

查看血缘关系
点进表，点lineage，后面是关系图、分类、审计日志、元数据信息（数据字典）
跑了sql才有血缘关系

hive元数据增量同步
只要hive元数据发生变化，hive hook会把元数据变动通知atlas。atlas会根据dml获取数据之间的血缘关系

生成血缘依赖
进入hive客户端
hive

创建订单表
create table dwd_order_info (
    `id` string COMMENT '订单编号',
    `final_amount` decimal(16,2) COMMENT '订单最终金额',
    `order_status` string COMMENT '订单状态',
    `user_id` string COMMENT '用户id' ,
    `payment_way` string COMMENT '支付方式',
	`delivery_address` string COMMENT '送货地址',
    `out_trade_no` string COMMENT '支付流水号',
    `create_time` string COMMENT '创建时间',	
    `operate_time` string COMMENT '操作时间',
	`expire_time` string COMMENT '过期时间',
	`tracking_no` string COMMENT '物流单编号',
	`province_id` string COMMENT '省份id',
	`activity_reduce_amount` decimal(16,2) COMMENT '活动减免金额',
	`coupou_reduce_amount` decimal(16,2) COMMENT '优惠券减免金额',
	`original_amount` decimal(16,2) COMMENT '订单原价金额',
	`feight_fee` decimal(16,2) COMMENT '运费',
	`feight_fee_reduce` decimal(16,2) COMMENT '运费减免'    
) COMMENT '订单表'
row format delimited fields terminated by '\t';

既 create table dwd_order_info ( `id` string COMMENT '订单编号',`final_amount` decimal(16,2) COMMENT '订单最终金额',`order_status` string COMMENT '订单状态',`user_id` string COMMENT '用户id' ,`payment_way` string COMMENT '支付方式',`delivery_address` string COMMENT '送货地址',`out_trade_no` string COMMENT '支付流水号',`create_time` string COMMENT '创建时间',`operate_time` string COMMENT '操作时间',`expire_time` string COMMENT '过期时间',`tracking_no` string COMMENT '物流单编号',`province_id` string COMMENT '省份id',`activity_reduce_amount` decimal(16,2) COMMENT '活动减免金额',`coupou_reduce_amount` decimal(16,2) COMMENT '优惠券减免金额',`original_amount` decimal(16,2) COMMENT '订单原价金额',`feight_fee` decimal(16,2) COMMENT '运费',`feight_fee_reduce` decimal(16,2) COMMENT '运费减免' ) COMMENT '订单表' row format delimited fields terminated by '\t';

创建地区维度表
# DROP TABLE IF EXISTS `dim_base_province`;
CREATE TABLE `dim_base_province` (
`id` string COMMENT '编号',
`name` string COMMENT '省份名称',
`region_id` string COMMENT '地区 id',
`area_code` string COMMENT '地区编码',
`iso_code` string COMMENT 'ISO-3166编码，供可视化使用',
`iso_3166_2` string COMMENT 'ISO-3166-2编码，供可视化使用'
) COMMENT '省份表'
row format delimited fields terminated by '\t';

既CREATE TABLE `dim_base_province` (`id` string COMMENT '编号',`name` string COMMENT '省份名称',`region_id` string COMMENT '地区 id',`area_code` string COMMENT '地区编码',`iso_code` string COMMENT 'ISO-3166编码，供可视化使用',`iso_3166_2` string COMMENT 'ISO-3166-2编码，供可视化使用') COMMENT '省份表' row format delimited fields terminated by '\t';

这些建表语句就是ddl，数据的定义源

在atlas页面右上角Statistics点开，hive_table(4)，由2变为4，atlas已能实时获取hive元数据变化

给两张表填充数据，将base_province.txt和order_info.txt上传到hive表对应目录下

base_province.txt：
1	北京	1	110000	CN-11	CN-BJ
2	天津	1	120000	CN-12	CN-TJ
3	山西	1	140000	CN-14	CN-SX
4	内蒙古	1	150000	CN-15	CN-NM
5	河北	1	130000	CN-13	CN-HE
6	上海	2	310000	CN-31	CN-SH
7	江苏	2	320000	CN-32	CN-JS
8	浙江	2	330000	CN-33	CN-ZJ
9	安徽	2	340000	CN-34	CN-AH
10	福建	2	350000	CN-35	CN-FJ
11	江西	2	360000	CN-36	CN-JX
12	山东	2	370000	CN-37	CN-SD
14	台湾	2	710000	CN-71	CN-TW
15	黑龙江	3	230000	CN-23	CN-HL

order_info.txt:
4934	12033.00	1004	11	\N	第14大街第6号楼5单元647门	366893968325668	2020-06-10 22:39:20.0	2020-06-10 22:39:20.0	2020-06-10 22:54:20.0	\N	8	200.00	0.00	12218.00	15.00	\N
4935	12065.00	1005	22	\N	第6大街第11号楼2单元864门	845678229874954	2020-06-10 22:39:20.0	2020-06-12 22:43:57.0	2020-06-10 22:54:20.0	\N	11	0.00	70.00	12115.00	20.00	\N
4936	8308.00	1005	24	\N	第1大街第1号楼6单元562门	886898254767124	2020-06-10 22:39:20.0	2020-06-11 22:42:49.0	2020-06-10 22:54:20.0	\N	25	0.00	0.00	8298.00	10.00	\N
4937	3442.00	1005	37	\N	第17大街第23号楼7单元518门	333789257252777	2020-06-10 22:39:20.0	2020-06-10 22:39:20.0	2020-06-10 22:54:20.0	\N	17	0.00	30.00	3453.00	19.00	\N
4938	21014.00	1005	39	\N	第1大街第39号楼3单元294门	334997784963613	2020-06-10 22:39:20.0	2020-06-10 22:39:20.0	2020-06-10 22:54:20.0	\N	8	0.00	0.00	20997.00	17.00	\N
4939	2255.00	1005	47	\N	第11大街第7号楼7单元661门	456112632711976	2020-06-10 22:39:20.0	2020-06-13 22:44:29.0	2020-06-10 22:54:20.0	\N	21	0.00	70.00	2309.00	16.00	\N
4940	24904.00	1005	57	\N	第14大街第37号楼5单元536门	232551552927594	2020-06-10 22:39:20.0	2020-06-13 22:44:29.0	2020-06-10 22:54:20.0	\N	3	500.00	0.00	25390.00	14.00	\N
4941	8401.00	1005	58	\N	第12大街第18号楼4单元334门	711941717753122	2020-06-10 22:39:20.0	2020-06-12 22:43:57.0	2020-06-10 22:54:20.0	\N	12	0.00	0.00	8385.00	16.00	\N

http://bigData102:9870/explorer.html#/user/hive/warehouse
路径/user/hive/warehouse下面，上传到对应目录

去hive看一下
hive
select * from dwd_order_info;
select * from dim_base_province;

假设需求指标：
根据订单事实表，地区维度表，求出每个省份的订单次数和订单金额

建表：
create table `ads_order_by_province` (
	`dt` string comment '统计日期',
	`province_id` string comment '省份id',
	`province_name` string comment '省份名称',
	`area_code` string comment '地区编码',
	`iso_code` string comment '国际标准地区编码',
	`iso_code_3166_2` string comment '国际标准地区编码',
	`order_count` bigint comment '订单数',
	`order_amount` decimal(16,2) comment '订单金额'
) comment '各省份订单统计' 
row format delimited fields terminated by '\t';

既create table `ads_order_by_province` (`dt` string comment '统计日期',`province_id` string comment '省份id',`province_name` string comment '省份名称',`area_code` string comment '地区编码',`iso_code` string comment '国际标准地区编码',`iso_code_3166_2` string comment '国际标准地区编码',`order_count` bigint comment '订单数',`order_amount` decimal(16,2) comment '订单金额') comment '各省份订单统计' row format delimited fields terminated by '\t';

数据装载
insert into table ads_order_by_province
select 
	'2021-08-30' dt,
	bp.id,
	bp.name,
	bp.area_code,
	bp.iso_code,
	bp.iso_3166_2,
	count(*) order_count,
	sum(oi.final_amount) order_amount
from dwd_order_info oi 
left join dim_base_province bp 
on oi.province_id=bp.id 
group by bp.id,bp.name,bp.area_code,bp.iso_code,bp.iso_3166_2;

既insert into table ads_order_by_province select '2021-08-30' dt,bp.id,bp.name,bp.area_code,bp.iso_code,bp.iso_3166_2,count(*) order_count,sum(oi.final_amount) order_amount from dwd_order_info oi left join dim_base_province bp on oi.province_id=bp.id group by bp.id,bp.name,bp.area_code,bp.iso_code,bp.iso_3166_2;

hive里执行sql，atlas中可以实时获取到血缘关系

select * from ads_order_by_province;

表的血缘关系，数据来自哪些表
http://bigData102:21000/index.html#!/detailPage/c41f2548-39f6-4033-aaef-b17bfad8f544?tabActive=lineage

数据质量监控
字段的血缘关系
这一字段分别来源于哪些字段，源头字段来自哪张表
http://bigData102:21000/index.html#!/detailPage/87939055-4108-4230-a8af-682f13ad146a?tabActive=lineage

atlas主要功能：
1.右上角查看数据资产目录（数据字典）
2.左边搜索框（数据字典）
3.表或字段的血缘关系图

atlas源码编译（将atlas源码包编译成安装包，官网只有源码包）
将maven3.6.1下到/opt/software/下
解压到/opt/module/中
改名为maven
添加环境变量到/etc/profile
#MAVEN_HOME
export MAVEN_HOME=/opt/module/maven
export PATH=$PATH:$MAVEN_HOME/bin
刷新环境变量
修改maven setting.xml
vim /opt/module/maven/conf/settings.xml
<!--阿里云镜像-->
<mirror>
 <id>nexus-aliyun</id>
 <mirrorOf>central</mirrorOf>
 <name>Nexus aliyun</name>
<url>http://maven.aliyun.com/nexus/content/groups/public</url>
</mirror>
<mirror>
 <id>UK</id>
 <name>UK Central</name>
 <url>http://uk.maven.org/maven2</url>
 <mirrorOf>central</mirrorOf>
</mirror>
<mirror>
 <id>repo1</id>
 <mirrorOf>central</mirrorOf>
 <name>Human Readable Name for this Mirror.</name>
 <url>http://repo1.maven.org/maven2/</url>
</mirror>
<mirror>
 <id>repo2</id>
 <mirrorOf>central</mirrorOf>
 <name>Human Readable Name for this Mirror.</name>
 <url>http://repo2.maven.org/maven2/</url>
</mirror>

把 apache-atlas-2.1.0-sources.tar.gz 上传到 bigData102 的/opt/software 目录下
解压 apache-atlas-2.1.0-sources.tar.gz 到/opt/module/目录下面
下载 Atlas 依赖
export MAVEN_OPTS="-Xms2g -Xmx2g"
cd /opt/module/apache-atlas-sources2.1.0/
mvn clean -DskipTests install
mvn clean -DskipTests package -Pdis
#一定要在${atlas_home}执行
cd distro/target/
mv apache-atlas-2.1.0-server.tar.gz /opt/software/
mv apache-atlas-2.1.0-hive-hook.tar.gz /opt/software/

提示：执行过程比较长，会下载很多依赖，大约需要半个小时，期间如果报错很有可能是因为 TimeOut 造成的网络中断，重试即可。

atlas内存配置

atlas存储数万个元数据对象，建议调整参数值获得最佳的 JVM GC（垃圾回收器） 性能。以下是常见的服务器端选项
1）修改配置文件/opt/module/atlas/conf/atlas-env.sh

#设置 Atlas 内存
export ATLAS_SERVER_OPTS="-server -XX:SoftRefLRUPolicyMSPerMB=0
-XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC -
XX:+CMSParallelRemarkEnabled -XX:+PrintTenuringDistribution -
XX:+HeapDumpOnOutOfMemoryError -
XX:HeapDumpPath=dumps/atlas_server.hprof -Xloggc:logs/gcworker.log -verbose:gc -XX:+UseGCLogFileRotation -
XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1m -
XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps"

#建议 JDK1.7 使用以下配置
export ATLAS_SERVER_HEAP="-Xms15360m -Xmx15360m -
XX:MaxNewSize=3072m -XX:PermSize=100M -XX:MaxPermSize=512m"
#建议 JDK1.8 使用以下配置
export ATLAS_SERVER_HEAP="-Xms15360m -Xmx15360m -
XX:MaxNewSize=5120m -XX:MetaspaceSize=100M -
XX:MaxMetaspaceSize=512m"
#如果是 Mac OS 用户需要配置
export ATLAS_SERVER_OPTS="-Djava.awt.headless=true -
Djava.security.krb5.realm= -Djava.security.krb5.kdc="

参数说明：-XX:SoftRefLRUPolicyMSPerMB 此参数对管理具有许多并发用户的查询繁重工作负载的 GC 性能特别有用。

4.3 配置新用户名密码

#Atlas 支持以下身份验证方法：File、Kerberos 协议、LDAP 协议
#通过修改配置文件 atlas-application.properties 文件开启或关闭三种验证方法
#atlas.authentication.method.kerberos=true|false
#atlas.authentication.method.ldap=true|false
#atlas.authentication.method.file=true|false
#如果两个或多个身份证验证方法设置为 true，如果较早的方法失败，则身份验证将回退到后一种方法。
#例如，如果 Kerberos 身份验证设置为 true 并且 ldap 身份验证也设置为 true，那么，如果对于没有 kerberos principal 和 keytab 的请求，LDAP #身份验证将作为后备方案。

本文主要讲解采用文件方式修改用户名和密码设置。其他方式可以参见官网配置即可。
1）打开/opt/module/atlas/conf/users-credentials.properties 文件
vim users-credentials.properties

admin=ADMIN::8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918
#用户名=组名::密码（sha256加密后）
linux中得到sha256密文方式：echo -n "shonqian"|sha256sum

重启atlas生效

修改用户名和密码
vim users-credentials.properties
#username=group::sha256-password
shonqian=ADMIN::2628be627712c3555d65e0e5f9101dbdd403626e6646b72fdf728a20c5261dc2

rangertagsync=RANGER_TAG_SYNC::e3f67240f5117d1753c940dae9eea772d36ed5fe9bd9c94a300e40413f1afb9d
--------------
大数据项目理解：
1.数据采集：
数据不像业务系统那样一条条产生，通过文件或者数据库形式导入
1.1建立hive数据仓库
在hive中建立表结构，把数据导入hdfs对应路径

2.数据处理
2.1根据需求，通过hive语句把现有数据进行联合查询、合并到一张新表
2.2atlas通过hive hook，可以自动获取到hive的表结构和数据，包括后续更新
--------------
hive

内部表
create table
内部表数据由Hive自身管理
存储的位置
hive.metastore.warehouse.dir（默认：/user/hive/warehouse）
删除内部表会直接删除元数据（metadata）及存储数据；
分建表和数据载入

外部表
create external table
外部表数据由HDFS管理
存储位置由自己制定
删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除；
同时完成创建和数据载入

hiveserver2提供了一个新的命令行工具beeline，增加了权限控制
要使用beeline需要先启动hiverserver2，再使用beeline连接

启动hiveserver2服务
hiveserver2 &
或
$HIVE_HOME/bin/hiveserver2 &
或
nohup hive --service hiveserver2 &

启动beeline，连接hiveserver2
jdbc方式连接hive
beeline
!connect jdbc:hive2://hadoop101:10000
或
beeline -u jdbc:hive2://bigData101:10000 -n shonqian

hive UI
http://192.168.30.99:10002/

向表中装载数据（Load）
hive> load data [local] inpath '/opt/module/datas/student.txt' overwrite | into table student [partition (partcol1=val1,…)];
（1）load data:表示加载数据
（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表
（3）inpath:表示加载数据的路径
（4）overwrite:表示覆盖表中已有数据，否则表示追加
（5）into table:表示加载到哪张表
（6）student:表示具体的表
（7）partition:表示上传到指定分区

创建一张表
hive (default)> create table student(id string, name string) row format delimited fields terminated by '\t';
加载HDFS文件到hive中
上传文件到HDFS
hive (default)> dfs -put /opt/module/datas/student.txt /user/atguigu/hive;
加载HDFS上数据
hive (default)> load data inpath '/user/atguigu/hive/student.txt' into table default.student;

创建表时通过Location指定加载数据路径
1．创建表，并指定在hdfs上的位置
hive (default)> create table if not exists student5(
              id int, name string
              )
              row format delimited fields terminated by '\t'
              location '/user/hive/warehouse/student5';
2．上传数据到hdfs上
hive (default)> dfs -put /opt/module/datas/student.txt
/user/hive/warehouse/student5;
3．查询数据
hive (default)> select * from student5;
--------------
把hive的mapreduce替换成spark：

hive on spark生态好，语法hql
spark on hive速度快，语法spark sql

hive3.1.2与spark3.0.0兼容问题
官网hive3.1.2支持的spark是2.4.5的，所以我们要手动修改下
官网下载hive3.1.2源码，修改pom文件引用的spark改为3.0.0，重新编译jar包

检测lib下spark是否为3.0.0
1.进入hive lib目录
cd /opt/module/hive/lib
2.查询spark的相关依赖
ls -al | grep spark

部署spark
cd /opt/software/spark
将
spark-3.0.0-bin-hadoop3.2.tgz
spark-3.0.0-bin-without-hadoop.tgz
传入
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/

cd /opt/module/
mv spark-3.0.0-bin-hadoop3.2/ spark

root账户
vim /etc/profile.d/my_env.sh
#spark
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin
source /etc/profile

在hive中创建spark配置文件
vim /opt/module/hive/conf/spark-defaults.conf
spark.master    yarn #spark计算模式
spark.eventLog.enabled  true #开启spark的事件日志
spark.eventLog.dir  hdfs://bigData101:8020/spark-history #指定日志路径
spark.executor.memory   1g #配置executor和driver内存大小
spark.driver.memory 1g

hadoop fs -mkdir /spark-history

将spark纯净版jar包上传到hdfs，这样每次提交任务就不用再重复上传了，纯净版避免冲突
hadoop fs -mkdir /spark-jars
cd /opt/software/spark/
tar -zxvf spark-3.0.0-bin-without-hadoop.tgz
cd spark-3.0.0-bin-without-hadoop/jars/
hadoop fs -put ./* /spark-jars

修改hive-site.xml文件
vim /opt/module/hive/conf/hive-site.xml
<!--Spark依赖位置（端口号8020必须和namenode端口号一致）-->
<property>
    <name>spark.yarn.jars</name>
    <value>hdfs://bigData101:8020/spark-jars/*</value>
</property>
<!--hive执行引擎-->
<property>
    <name>hive.execution.engine</name>
    <value>spark</value>
</property>

测试spark引擎是否成功
hive
create table temp_student(id int,name string);
insert into table temp_student values(1,'abc');

出现
STAGES: 02/02    [==========================>>] 100%  ELAPSED TIME: 3.05 s
代表成功

如果报错：
FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session 7fc2665b-5b7e-496f-a8bf-4a87430d8a00
或者：
yarn http://bigdata101:8088/cluster
上任务报错，Timed out waiting for RPC server connection

重试下命令
或者
中止RunJar进程？
或者
重启hadoop相关服务？
或者
set hive.spark.client.server.connect.timeout=300000;？
或者vim /opt/module/hive/conf/hive-site.xml中增加
<property>
    <name>hive.spark.client.connect.timeout</name>
    <value>100000ms</value>
</property>？
或者
cd /opt/module/spark/conf
mv spark-env.sh.template spark-env.sh
最后增加export SPARK_DIST_CLASSPATH=${hadoop classpath}？
--------------
Azkaban（轻）与oozie（重）对比
市面上最流行的两种工作流调度系统

简单调度：
Linux的Crontab

一个完整的数据分析系统通常都是由大量任务单元组成：
Shell脚本程序，Java程序，MapReduce程序、Hive脚本等

大数据工作流调度流程：
某业务系统每天产生20G原始数据，我们每天对其进行处理
1.先将原始数据同步到HDFS上
HDFS
2.对原始数据进行计算
MapReduce
3.生成的数据以分区表形式存储到多张Hive表中
[Hive表，Hive表，Hive表]
4.对Hive中多个表的数据进行JOIN处理，得到一张明细数据Hive大表
明细数据Hive大表
5.将明细数据进行复杂的统计分析，得到结果报表信息
结果报表
6.将统计分析得到的结果数据同步到业务系统中
业务系统

Azkaban集群部署
web-server,exec-server,mysql
将
azkaban-db-3.84.4.tar.gz
azkaban-exec-server-3.84.4.tar.gz
azkaban-web-server-3.84.4.tar.gz
上传至/opt/software/azkaban/
解压至/opt/module/azkaban/下

mv azkaban-exec-server-3.84.4 azkaban-exec
mv azkaban-web-server-3.84.4 azkaban-web

mysql -uroot -proot
create database azkaban;
use azkaban;

source /opt/module/azkaban/azkaban-db-3.84.4/create-all-sql-3.84.4.sql;

show tables;

quit;

修改mysql允许包大小，防止azkaban连接mysql阻塞
sudo vim /etc/my.cnf
在[mysqld]下面加上
max_allowed_packet=1024M

重启mysql
sudo systemctl restart mysqld
完成数据库初始化

配置ExecutorServer
cd /opt/module/azkaban/azkaban-exec/conf
vim azkaban.properties
default.timezone.id=Asia/Shanghai
azkaban.webserver.url=http://bigData101:8081
mysql.host=bigData101
mysql.user=root
mysql.password=root
最后添加executor.port=12321//不指定的话，每次都会是随机值，不方便管理

启动executor
cd /opt/module/azkaban/azkaban-exec
bin/start-exec.sh
// 停止命令bin/shutdown-exec.sh
// 把azkaban.properties里的相对路径改为绝对路径后，就能不用进目录再执行了，如conf/...

ll 查看当前目录下是否有executor.port文件，有说明启动成功，没有就查看日志

激活executor
向服务发送一个get请求即可
curl -G "bigData101:12321/executor?action=activate" && echo

配置Web Server
cd /opt/module/azkaban/azkaban-web/
vim conf/azkaban.properties
default.timezone.id=Asia/Shanghai
mysql.host=bigData101
mysql.user=root
mysql.password=root
azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus // 多executor时设置，az选择器挑选机制：StaticRemainingFlowSize目前排队任务数最少，MinimumFreeMemory最小内存限制6G(会导致任务无法执行)，CpuStatus cpu使用率最低
vim conf/azkaban-users.xml用户、权限管理文件
添加
<user password="root" roles="admin" username="shonqian"/>
启动
bin/start-web.sh
// 停止 bin/shutdown-web.sh

访问
http://bigdata101:8081/
登录
shonqian
root

在windows上
新建first.project
azkaban-flow-version: 2.0

新建first.flow
nodes:
  - name: jobA
    type: command
    config:
      command: echo "Hello World"

两个打包在一起，zip格式

右上角，Create Project

upload上传那个zip压缩包

点Execute Flow

左边Schedule定时调度
右边Execute立即执行

job每一个工作单元
flow所有工作单元组成的工作流

日志排查在Job List里的Log里

first.project文件内容固定，确定flow版本
first.flow文件（可properties和yaml两种格式）

yaml：
1.大小写敏感
2.缩进表示层级
3.不能用tab，用空格
4.缩进空格数不重要，相同层级元素要左侧对齐
5.#表示注释

yaml语法：
对象：{name:zhangsan,age:18}
name: zhangsan
age: 18
数组：[a,b,c]
- a
- b
- c
对象数组：[{name:zhangsan,age:18},{name:zhangsan,age:18},{name:zhangsan,age:18}]
- name: zhangsan
  age: 18
- name: zhangsan
  age: 18
- name: zhangsan
  age: 18
--------------
sqoop数据传递工具
(mysql,oracle) <=> (hdfs,hive,hbase)互导工具

sqoop版本
sqoop1（1.9之前）
sqoop2（1.9之后）
1与2不兼容

sqoop的导入/导出就是对mapreduce的inputformat和outputformat进行定制

安装：
0.安装前提，已有java和hadoop环境
1.下载安装包，一般使用sqoop1.4.6
2.解压
3.配置环境变量
4.修改配置文件
sqoop/conf/下
mv sqoop-env-template.sh sqoop-env.sh
配置入hadoop和hive等需要的地址
export HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3
export HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3
export HIVE_HOME=/opt/module/hive
5.拷贝jdbc驱动到sqoop/lib/下
6.查看sqoop是否配置正确
sqoop help

用法：
1.查出所有库
sqoop list-databases --connect jdbc:mysql://192.168.30.95:3306/ --username root --password root
#list-tables查出所有表

2.导入import（一般指关系型数据库 => 大数据仓库hdfs,hive,hbase）

2.1单表到hdfs
sqoop import \
--connect jdbc:mysql://192.168.30.95:3306/jtdb \
--username root \
--password root \
--table section \
--target-dir /user/sqoop \ #--target-dir指定目录，已有会报错
--fields-terminated-by "\t" #每字段结束符是tab键

2.2全库到hdfs
sqoop import-all-tables --connect "jdbc:mysql://192.168.30.95:3306/user01" --username root --password root --warehouse-dir "/user/hive/warehouse"  -m 1 --fields-terminated-by "\t"
#--warehouse-dir指定父目录
#-m 指定mapreduce进程数

2.3单表到hive
sqoop-import --connect 'jdbc:mysql://192.168.30.95:3306/dababase_a' --username root --password root --table table_a --hive-import --hive-overwrite  --hive-table default.table_a --null-string '\\N' --null-non-string '\\N' -m 1
#--table指定关系型数据库的表名
#--hive-table指定hive的表名
#--hive-import 数据导入hdfs后，再载入hive目录

2.4全库到hive
sqoop import-all-tables \
-Dorg.apache.sqoop.splitter.allow_text_splitter=true \
--connect jdbc:mysql://192.168.30.95:3306/user01 \
--username root \
--password root \
--hive-import \
-m 1

3.其它用法

3.1查询
--query 'select name,sex from staff where id <=1 and $CONDITIONS;'
# $CONDITIONS为固定用法

3.2导入指定列
--columns id,sex

3.3常用参数、命令 => 百度

增量导入
1.按数值类型增长（比如id）
append追加模式
append --check-column id --last-value 1201
更新数据追加到新文件，会造成重复数据

2.按时间（比如xxxTime）
2.1Lastmodified上次更新模式
时间+1秒，会取>=的记录
更新数据追加到新文件，会造成重复数据

2.2merge-key形式
把增量数据合并到原文件中
导入增量数据和更新变化的数据，且数据不会重复，
底层相当于做了一次完整的mapreduce

sqoop当中job作业
用其它调度软件工作调度这个job

同步策略
1.全量同步（每日全量表=>hive表中的一个分区），数据量小/码表
2.增量同步，数据量大+不变化
3.新增及变化同步，数据量大+变化
4.特殊情况（只需要存储一次，比如地区表、省份表）

业务表同步
第一次全量同步
sqoop import \
--connect jdbc:mysql://192.168.30.95:3306/gmall \
--username root \
--password root \
--query 'select * from order_info where $CONDITIONS' \
--target-dir /order_info/2020-06-14 \ #--target-dir指定目录，已有会报错
--delete-target-dir \ #已存在会删除
--fields-terminated-by "\t" #每字段结束符是tab键
--num-mappers 2 \ #指定mapreduce数
--split-by id #分割基准字段
后续新增及变化同步
sqoop import \
--connect jdbc:mysql://192.168.30.95:3306/gmall \
--username root \
--password root \
--query 'select * from order_info where (date_format(create_time,'%Y-%m-%d')='2020-06-15' or date_format(update_time,'%Y-%m-%d')='2020-06-15') and $CONDITIONS' \
--target-dir /order_info/2020-06-15 \ #--target-dir指定目录，已有会报错
--delete-target-dir \ #已存在会删除
--fields-terminated-by "\t" #每字段结束符是tab键
--num-mappers 2 \ #指定mapreduce数
--split-by id #分割基准字段

编写首日同步shell脚本，日期用变量传入
mysql_to_hdfs_init.sh
#!/bin/bash

APP=gmall#数据库库名
sqoop=/opt/module/sqoop/bin/sqoop#sqoop绝对路径

if [ -n "$2" ] ;then#-n非空判断，返回布尔
   do_date=$2
else 
   echo "请传入日期参数"
   exit
fi 

import_data(){#公用函数
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \#$1值此函数的第一个参数
--delete-target-dir \
--query "$2 where \$CONDITIONS" \#$2值此函数的第二个参数
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \#输出文件进行压缩
--compression-codec lzop \#压缩格式lzop
--null-string '\\N' \#将mysql中null值转换，hive中null值为\N
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}#为上传的lzop文件创建索引
import_order_info(){
  import_data order_info #调用import_data函数，order_info为$1既传入参数
  "select#为import_data函数的$2参数
        id, 
        total_amount, 
        order_status, 
        user_id, 
        payment_way,
        delivery_address,
        out_trade_no, 
        create_time, 
        operate_time,
        expire_time,
        tracking_no,
        province_id,
        activity_reduce_amount,
        coupon_reduce_amount,                            
        original_total_amount,
        feight_fee,
        feight_fee_reduce      
    from order_info"
}

import_coupon_use(){
  import_data coupon_use 
  "select
        id,
        coupon_id,
        user_id,
        order_id,
        coupon_status,
        get_time,
        using_time,
        used_time,
        expire_time
    from coupon_use"
}

case $1 in #脚本的第一个参数
  "order_info")
     import_order_info #调import_order_info方法，同步order_info数据
;;
  "coupon_use")
      import_coupon_use
;;
  "all") #传all就同步所有表数据
   import_order_info
   import_coupon_use
;;
esac

编写每日同步shell脚本
mysql_to_hdfs.sh
#!/bin/bash

APP=gmall#数据库库名
sqoop=/opt/module/sqoop/bin/sqoop#sqoop绝对路径

if [ -n "$2" ] ;then#-n非空判断，返回布尔
   do_date=$2
else 
   do_date=`date -d '-1 day'+%F` #为空就取前一天
fi 

import_data(){#公用函数
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \#$1值此函数的第一个参数
--delete-target-dir \
--query "$2 where \$CONDITIONS" \#$2值此函数的第二个参数
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \#输出文件进行压缩
--compression-codec lzop \#压缩格式lzop
--null-string '\\N' \#将mysql中null值转换，hive中null值为\N
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}#为上传的lzop文件创建索引
import_order_info(){
  import_data order_info #调用import_data函数，order_info为$1既传入参数
  "select#为import_data函数的$2参数
        id, 
        total_amount, 
        order_status, 
        user_id, 
        payment_way,
        delivery_address,
        out_trade_no, 
        create_time, 
        operate_time,
        expire_time,
        tracking_no,
        province_id,
        activity_reduce_amount,
        coupon_reduce_amount,                            
        original_total_amount,
        feight_fee,
        feight_fee_reduce      
    from order_info
    #新增及变化同步
    where (date_format(create_time,'%Y-%m-%d')='$do_date' 
    or date_format(update_time,'%Y-%m-%d')='$do_date')"
}

import_comment_info(){
  import_data comment_info 
  "select
        id,
        user_id,
        sku_id,
        spu_id,
        order_id,
        appraise,
        create_time
    from comment_info
    #新增同步
    where (date_format(create_time,'%Y-%m-%d')='$do_date')"
}
#商品表
import_sku_info(){
  import_data sku_info 
  "select
        id,
        spu_id,
        price,
        sku_name,
        sku_desc,
        weight,
        tm_id,
        category3_id,
        is_sale,
        create_time
    from sku_info
    #全量同步
    where (1=1)"
}

case $1 in #脚本的第一个参数
  "order_info")
     import_order_info #调import_order_info方法，同步order_info数据
;;
  "comment_info")
      import_comment_info
;;
  "sku_info")
      import_sku_info
;;
  "all") #同步每一张表数据
   import_order_info
   import_comment_info
   import_sku_info
;;
esac

第一个参数传表名或者all，第二个参数传日期
-------------
java操作sqoop1，将mysql数据导入hdfs
1.pom
<!-- sqoop1同步hdfs所需依赖 -->
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
</dependency>
<!-- https://repo.maven.apache.org/maven2/org/apache/sqoop/sqoop/1.4.6/ -->
<dependency>
    <groupId>org.apache.sqoop</groupId>
    <artifactId>sqoop</artifactId>
    <version>1.4.6</version>
    <classifier>hadoop200</classifier>
</dependency>
<!-- hadoop -->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-core</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-common</artifactId>
    <version>2.8.4</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
    <version>2.8.4</version>
    <scope>test</scope>
</dependency>
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.8.1</version>
</dependency>
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-mapred</artifactId>
    <version>1.8.1</version>
</dependency>

2.service
public void mysql2hdfs() {
    try {
        String[] args = new String[]{
                "import",
                "--connect", "jdbc:mysql://192.168.40.113:3306/pra?serverTimezone=Asia/Shanghai",
                "--driver", "com.mysql.cj.jdbc.Driver",
                "-username", "root",
                "-password", "root",
                "--table", "emp",
                "--null-string", "na",
                "--null-non-string", "na",
                "-m", String.valueOf(1),
                "--outdir", "/temp/xx/",
                "--delete-target-dir",
                "--target-dir", /test/5,
                "--fields-terminated-by","\t",
                "--hadoop-mapred-home", "/opt/module/hadoop-3.1.3"
//                    "--hadoop-mapred-home", "$HADOOP_MAPRED_HOME"
        };
        String[] expandArguments = OptionsFileUtil.expandArguments(args);
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS", "hdfs://bigData101:8020");
        Sqoop.runTool(expandArguments, conf);
    } catch (Exception e) {
        e.printStackTrace();
    }
}

3.问题
windows下需要hadoop运行所需文件和环境变量：
安装winutils，看看对应版本有没有hadoop.dll和winutils.exe这两个文件
将winutils下对应hadoop版本添加到系统变量
HADOOP_HOME C:\qxn\myPro\winutils\hadoop-3.0.0
添加Path变量，%HADOOP_HOME%\bin
将hadoop.dll复制到C:\Window\System32下
重启idea
还不行的话，试试重启电脑

打成jar包后运行：
在sqoop运行过程中，会在/tmp/下生成一个mysql表名.java文件，文件头部有
import org.apache.hadoop.io.BytesWritable;
import com.cloudera.sqoop.lib.JdbcWritableBridge;等

org.apache.hadoop和com.cloudera.sqoop两个包会找不到，报错，
sqoop参数设定了"--hadoop-mapred-home", "/opt/module/hadoop-3.1.3"后，就能找到hadoop包了
将sqoop-1.4.6 jar包复制改名为hadoop-common-sqoop-1.4.6.jar，
并放入linux中hadoop目录/share/hadoop/common/下，就能找到sqoop包了
至此jar包能够正常运行
-------------
hive-jdbc和hadoop jar包有冲突
得把hive-jdbc去掉一些jar包，服务才能起起来
<!-- hive -->
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-jdbc</artifactId>
    <version>2.3.0</version>
    <exclusions>
        <exclusion>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-slf4j-impl</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.eclipse.jetty.orbit</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.eclipse.jetty.aggregate</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
            <groupId>tomcat</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>*</artifactId>
        </exclusion>
    </exclusions>
<!--     未生效       <exclusions>-->
<!--                <exclusion>-->
<!--                    <groupId>*</groupId>-->
<!--                    <artifactId>*</artifactId>-->
<!--                </exclusion>-->
<!--            </exclusions>-->
</dependency>
-------------
